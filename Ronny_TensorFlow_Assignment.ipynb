{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn6lPM0rRHKUIEYT6v1kJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_TensorFlow_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Looking back on scratch**\n",
        "\n",
        "List of core components needed to implement a deep learning model:\n",
        "1. Initialization of Weights and Biases: You can't start without them. These are the parameters that the model will learn. You need to create tensors to hold these values and initialize them, often with small random numbers or zeros. This is a manual process without tf.keras.layers.\n",
        "\n",
        "2. Forward Propagation: This is the core of the model. You must write the code to compute the output of the network given an input. This involves matrix multiplications (weights\n",
        "times input) and adding biases, followed by the application of an activation function for each layer.\n",
        "\n",
        "3. Loss Function: To train the model, you need a way to measure how wrong its predictions are. You must define a function that takes the model's output and the true labels and returns a single loss value. For binary classification, this would typically be binary cross-entropy.\n",
        "\n",
        "3. Optimizer: This is the engine of the learning process. It's the algorithm that uses the loss to update the weights and biases. You have to implement an optimization algorithm, like Gradient Descent or Adam, which calculates the gradients of the loss with respect to the weights and then updates the weights in the opposite direction of the gradient.\n",
        "\n",
        "4. Epoch and Batch Loops: Training is an iterative process. You need a loop that runs for a number of epochs. Inside this, you'll have another loop that processes the data in smaller chunks called batches. This is known as mini-batch gradient descent, which is more computationally efficient than processing the entire dataset at once.\n",
        "\n",
        "5. Backpropagation: This is the process of calculating the gradients. In TensorFlow's low-level API, this is often handled by a tf.GradientTape. It's a key feature that automatically records operations to compute gradients, which are essential for the optimizer.\n",
        "\n",
        "6. Accuracy Metric: During and after training, you need to evaluate the model's performance. You'll have to write a function to compare the predicted class with the true class and calculate a metric like accuracy.\n",
        "\n",
        "7. Data Preprocessing and Splitting: Even at a low level, data must be prepared. This involves loading the data, splitting it into features and labels, and then splitting it into training and testing sets. You also need to handle normalization and one-hot encoding for the labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "PJYj1tdNNqhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Filtering for the two specified species: 'Iris-versicolor' and 'Iris-virginica'\n",
        "df = df[(df['Species'] == 'Iris-versicolor') | (df['Species'] == 'Iris-virginica')]\n",
        "\n",
        "# Mapping the species names to numerical labels (0 and 1)\n",
        "df['Species'].replace({'Iris-versicolor': 0, 'Iris-virginica': 1}, inplace=True)\n",
        "\n",
        "# Separating features (X) and labels (y)\n",
        "X = df.iloc[:, 1:5].values\n",
        "y = df['Species'].values\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converting to TensorFlow tensors\n",
        "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
        "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
        "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
        "y_test_tensor = tf.constant(y_test, dtype=tf.float32)\n",
        "\n",
        "print(\"Training features shape:\", X_train_tensor.shape)\n",
        "print(\"Training labels shape:\", y_train_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adFXtUSRtJ6",
        "outputId": "d201166d-d9b1-4f55-97ec-85b797f7112c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (80, 4)\n",
            "Training labels shape: (80,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-956150092.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Species'].replace({'Iris-versicolor': 0, 'Iris-virginica': 1}, inplace=True)\n",
            "/tmp/ipython-input-956150092.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Species'].replace({'Iris-versicolor': 0, 'Iris-virginica': 1}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Consider the correspondence between scratch and TensorFlow**\n",
        "\n",
        "The provided sample code uses TensorFlow 1.x, which operates on a static computation graph. This is a very low-level approach, similar to what you would build from scratch, and it's a great way to understand the underlying mechanics. Let's break down how the scratch components you listed map to the TensorFlow 1.x code.\n",
        "\n",
        "Initialization of Weights and Biases:\n",
        "\n",
        "Scratch: You would manually create arrays or matrices to store weights and biases, and fill them with random numbers.\n",
        "\n",
        "TensorFlow: This is handled by tf.Variable. The code tf.Variable(tf.random_normal([n_input, n_hidden1])) creates a TensorFlow variable and initializes it with random values. These variables are part of the computation graph and their values can be modified during training. tf.global_variables_initializer() is a separate operation that runs the initializers for all tf.Variable objects.\n",
        "\n",
        "Forward Propagation:\n",
        "\n",
        "Scratch: You would write a series of matrix multiplications and additions using a library like NumPy, followed by applying activation functions.\n",
        "\n",
        "TensorFlow: The entire model is defined as a function (example_net). The operations like tf.matmul (matrix multiplication), tf.add, and tf.nn.relu (ReLU activation) define the static computation graph. This graph is not executed until a session is run.\n",
        "\n",
        "Loss Function:\n",
        "\n",
        "Scratch: You'd implement the loss calculation (e.g., binary cross-entropy) as a custom function.\n",
        "\n",
        "TensorFlow: This is done with tf.nn.sigmoid_cross_entropy_with_logits and tf.reduce_mean. The tf.nn module contains many pre-built loss functions, which are more efficient and stable than a manual implementation. The tf.reduce_mean operation calculates the average loss over the batch.\n",
        "\n",
        "Optimizer:\n",
        "\n",
        "Scratch: You would manually calculate gradients and update weights using a small learning rate.\n",
        "\n",
        "TensorFlow: This is abstracted by tf.train.AdamOptimizer. You create an optimizer instance and then call its minimize method, which automatically computes the gradients (tf.gradients under the hood) and updates the variables to reduce the loss. This is a significant abstraction from the scratch implementation.\n",
        "\n",
        "Epoch and Batch Loops:\n",
        "\n",
        "Scratch: You would use standard Python for loops to iterate through epochs and mini-batches.\n",
        "\n",
        "TensorFlow: The code still uses a standard Python for loop for epochs. However, a custom GetMiniBatch iterator class is written to manage the batching process, shuffling the data, and providing mini-batches. This part is a manual, \"scratch-like\" implementation because it's a data-handling task, not a core TensorFlow operation.\n",
        "\n",
        "Backpropagation:\n",
        "\n",
        "Scratch: This is the most complex part to implement manually, involving the chain rule to compute gradients.\n",
        "\n",
        "TensorFlow: It's fully automated by the optimizer. When you call optimizer.minimize(loss_op), TensorFlow automatically calculates the gradients of the loss_op with respect to all the variables (tf.Variable) in the graph. The use of a static graph makes this process highly efficient.\n",
        "\n",
        "Placeholders:\n",
        "\n",
        "Scratch: You would just use Python variables to hold your input data.\n",
        "\n",
        "TensorFlow: tf.placeholder is used to create a \"hole\" in the computation graph where you can feed data. The data is passed into the graph at runtime using the feed_dict parameter of sess.run().\n",
        "\n",
        "The core difference is that in the low-level TensorFlow 1.x approach, you first build a static graph of all the computations and then use a tf.Session to execute it, feeding in data as needed. This is different from the eager execution of TensorFlow 2.x and a scratch-like approach, where operations are performed immediately.\n",
        "\n",
        "---\n",
        "Application to Other Datasets\n",
        "\n",
        "*** Adapting code for two datasets\n",
        "\n",
        "This requires changing the data loading, preprocessing, and model architecture (especially the output layer) to suit the new problems.\n",
        "\n",
        "A. Iris Dataset (All Three Classes)\n",
        "\n",
        "This is a multi-class classification problem. The key changes are:\n",
        "\n",
        "The labels must be one-hot encoded.\n",
        "\n",
        "The final layer of the neural network must have 3 neurons (one for each class).\n",
        "\n",
        "The loss function must be softmax_cross_entropy_with_logits.\n",
        "\n",
        "The correct_pred logic must be changed for multi-class.\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "df=pd.read_csv(\"Iris.csv\")\n",
        "y=df[\"Species\"]\n",
        "X=df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X=np.array(X)\n",
        "y=np.array(y).reshape(-1, 1) # Reshape for one-hot encoding\n",
        "\n",
        "# One-hot encode labels\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "y = ohe.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Hyperparameters and placeholders\n",
        "n_classes = 3 # Now 3 classes\n",
        "n_input = X_train.shape[1]\n",
        "X_tf = tf.placeholder(\"float\", [None, n_input])\n",
        "Y_tf = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# (The rest of the model definition and training loop is similar, with these changes)\n",
        "# Weights and biases now have n_classes\n",
        "# The final layer: tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "# The loss function: tf.nn.softmax_cross_entropy_with_logits(labels=Y_tf, logits=logits)\n",
        "# The accuracy metric: tf.equal(tf.argmax(logits, 1), tf.argmax(Y_tf, 1))\n",
        "```\n",
        "B. House Prices Dataset\n",
        "\n",
        "This is a regression problem, not classification. We are predicting a continuous value (price) instead of a discrete class. The core changes are:\n",
        "\n",
        "The output layer has only one neuron.\n",
        "\n",
        "There is no activation function on the final layer.\n",
        "\n",
        "The loss function is a regression loss, like Mean Squared Error (MSE).\n",
        "\n",
        "The labels should not be one-hot encoded.\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"House_Prices.csv\") # Assuming a House_Prices.csv with features and a \"Price\" column\n",
        "y = df[\"Price\"]\n",
        "X = df.drop(\"Price\", axis=1)\n",
        "X = np.array(X)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "# Scale the data for regression\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Hyperparameters and placeholders\n",
        "n_classes = 1 # Only one output for price\n",
        "n_input = X_train.shape[1]\n",
        "X_tf = tf.placeholder(\"float\", [None, n_input])\n",
        "Y_tf = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# (Model definition and training loop changes)\n",
        "# The final layer: tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "# NO activation function on the final layer\n",
        "# The loss function: tf.losses.mean_squared_error(labels=Y_tf, predictions=logits)\n",
        "# No accuracy metric is used. Instead, we would report MSE or RMSE.\n",
        "```"
      ],
      "metadata": {
        "id": "q3nqlsIIXEsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using a neural network implemented in TensorFlow 2.x.\n",
        "This model is configured to classify only 'Iris-versicolor' and 'Iris-virginica'.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Data Preparation\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "# Filter for the two specified species\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X, dtype=np.float32) # Ensure X is float32\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert labels to numbers (0 and 1)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.float32)[:, np.newaxis]\n",
        "\n",
        "# Split into train, val, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Define a simple mini-batch iterator\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# 2. Hyperparameter and Model Setup\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = 1\n",
        "\n",
        "# Define weights and biases as tf.Variable for TensorFlow 2.x\n",
        "tf.random.set_seed(0)\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random.normal([n_input, n_hidden1], dtype=tf.float32)),\n",
        "    'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2], dtype=tf.float32)),\n",
        "    'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes], dtype=tf.float32))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden1], dtype=tf.float32)),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden2], dtype=tf.float32)),\n",
        "    'b3': tf.Variable(tf.random.normal([n_classes], dtype=tf.float32))\n",
        "}\n",
        "\n",
        "# Define the Adam optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# --- 3. Model Architecture (Forward Pass) ---\n",
        "def example_net(x):\n",
        "    \"\"\"Simple 3-layer neural network for binary classification.\"\"\"\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 4. Training and Evaluation Loop\n",
        "# This loop now uses Eager Execution and tf.GradientTape for backpropagation.\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        # Use tf.GradientTape to record operations for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = example_net(mini_batch_x)\n",
        "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=mini_batch_y, logits=logits))\n",
        "\n",
        "        # Calculate gradients and apply them\n",
        "        gradients = tape.gradient(loss, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                         biases['b1'], biases['b2'], biases['b3']])\n",
        "        optimizer.apply_gradients(zip(gradients, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                                  biases['b1'], biases['b2'], biases['b3']]))\n",
        "\n",
        "        # Calculate accuracy for the mini-batch\n",
        "        correct_pred = tf.equal(tf.sign(mini_batch_y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "        acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "        total_acc += acc.numpy()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    total_batch = len(get_mini_batch_train)\n",
        "    total_loss /= total_batch\n",
        "    total_acc /= total_batch\n",
        "\n",
        "    # Validation\n",
        "    val_logits = example_net(X_val)\n",
        "    val_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_val, logits=val_logits))\n",
        "    val_correct_pred = tf.equal(tf.sign(y_val - 0.5), tf.sign(tf.sigmoid(val_logits) - 0.5))\n",
        "    val_acc = tf.reduce_mean(tf.cast(val_correct_pred, tf.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, Loss: {total_loss:.4f}, Val Loss: {val_loss.numpy():.4f}, Accuracy: {total_acc:.3f}, Val Acc: {val_acc.numpy():.3f}\")\n",
        "\n",
        "# Test\n",
        "test_logits = example_net(X_test)\n",
        "test_correct_pred = tf.equal(tf.sign(y_test - 0.5), tf.sign(tf.sigmoid(test_logits) - 0.5))\n",
        "test_acc = tf.reduce_mean(tf.cast(test_correct_pred, tf.float32))\n",
        "print(f\"\\nTest Accuracy: {test_acc.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5gVIzttXmGV",
        "outputId": "39b4d270-89a3-4612-c07e-1dd0764bfc9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 41.3720, Val Loss: 36.8386, Accuracy: 0.550, Val Acc: 0.375\n",
            "Epoch 002, Loss: 21.8496, Val Loss: 11.9242, Accuracy: 0.550, Val Acc: 0.438\n",
            "Epoch 003, Loss: 7.5692, Val Loss: 3.9683, Accuracy: 0.486, Val Acc: 0.625\n",
            "Epoch 004, Loss: 7.0586, Val Loss: 2.4231, Accuracy: 0.521, Val Acc: 0.750\n",
            "Epoch 005, Loss: 3.1090, Val Loss: 1.1669, Accuracy: 0.621, Val Acc: 0.750\n",
            "Epoch 006, Loss: 2.6151, Val Loss: 0.5328, Accuracy: 0.736, Val Acc: 0.750\n",
            "Epoch 007, Loss: 1.2863, Val Loss: 0.4620, Accuracy: 0.764, Val Acc: 0.938\n",
            "Epoch 008, Loss: 1.4483, Val Loss: 0.4094, Accuracy: 0.814, Val Acc: 0.938\n",
            "Epoch 009, Loss: 1.3068, Val Loss: 0.0442, Accuracy: 0.821, Val Acc: 1.000\n",
            "Epoch 010, Loss: 1.1049, Val Loss: 0.0091, Accuracy: 0.821, Val Acc: 1.000\n",
            "Epoch 011, Loss: 0.9076, Val Loss: 0.0232, Accuracy: 0.886, Val Acc: 1.000\n",
            "Epoch 012, Loss: 0.8890, Val Loss: 0.0103, Accuracy: 0.871, Val Acc: 1.000\n",
            "Epoch 013, Loss: 0.8337, Val Loss: 0.0017, Accuracy: 0.871, Val Acc: 1.000\n",
            "Epoch 014, Loss: 0.7542, Val Loss: 0.0010, Accuracy: 0.886, Val Acc: 1.000\n",
            "Epoch 015, Loss: 0.6893, Val Loss: 0.0010, Accuracy: 0.886, Val Acc: 1.000\n",
            "Epoch 016, Loss: 0.6474, Val Loss: 0.0005, Accuracy: 0.886, Val Acc: 1.000\n",
            "Epoch 017, Loss: 0.5960, Val Loss: 0.0003, Accuracy: 0.900, Val Acc: 1.000\n",
            "Epoch 018, Loss: 0.5425, Val Loss: 0.0004, Accuracy: 0.900, Val Acc: 1.000\n",
            "Epoch 019, Loss: 0.4991, Val Loss: 0.0004, Accuracy: 0.914, Val Acc: 1.000\n",
            "Epoch 020, Loss: 0.4593, Val Loss: 0.0004, Accuracy: 0.929, Val Acc: 1.000\n",
            "Epoch 021, Loss: 0.4227, Val Loss: 0.0004, Accuracy: 0.929, Val Acc: 1.000\n",
            "Epoch 022, Loss: 0.3917, Val Loss: 0.0005, Accuracy: 0.929, Val Acc: 1.000\n",
            "Epoch 023, Loss: 0.3637, Val Loss: 0.0005, Accuracy: 0.929, Val Acc: 1.000\n",
            "Epoch 024, Loss: 0.3370, Val Loss: 0.0005, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 025, Loss: 0.3113, Val Loss: 0.0005, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 026, Loss: 0.2887, Val Loss: 0.0005, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 027, Loss: 0.2651, Val Loss: 0.0006, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 028, Loss: 0.2435, Val Loss: 0.0006, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 029, Loss: 0.2242, Val Loss: 0.0007, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 030, Loss: 0.2086, Val Loss: 0.0007, Accuracy: 0.943, Val Acc: 1.000\n",
            "Epoch 031, Loss: 0.1954, Val Loss: 0.0008, Accuracy: 0.957, Val Acc: 1.000\n",
            "Epoch 032, Loss: 0.1853, Val Loss: 0.0009, Accuracy: 0.957, Val Acc: 1.000\n",
            "Epoch 033, Loss: 0.1767, Val Loss: 0.0010, Accuracy: 0.957, Val Acc: 1.000\n",
            "Epoch 034, Loss: 0.1684, Val Loss: 0.0011, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 035, Loss: 0.1581, Val Loss: 0.0012, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 036, Loss: 0.1465, Val Loss: 0.0013, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 037, Loss: 0.1351, Val Loss: 0.0016, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 038, Loss: 0.1236, Val Loss: 0.0021, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 039, Loss: 0.1152, Val Loss: 0.0029, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 040, Loss: 0.1075, Val Loss: 0.0037, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 041, Loss: 0.0999, Val Loss: 0.0049, Accuracy: 0.971, Val Acc: 1.000\n",
            "Epoch 042, Loss: 0.0940, Val Loss: 0.0071, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 043, Loss: 0.0865, Val Loss: 0.0099, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 044, Loss: 0.0805, Val Loss: 0.0134, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 045, Loss: 0.0741, Val Loss: 0.0158, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 046, Loss: 0.0675, Val Loss: 0.0172, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 047, Loss: 0.0625, Val Loss: 0.0185, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 048, Loss: 0.0565, Val Loss: 0.0187, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 049, Loss: 0.0514, Val Loss: 0.0188, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 050, Loss: 0.0456, Val Loss: 0.0200, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 051, Loss: 0.0403, Val Loss: 0.0208, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 052, Loss: 0.0356, Val Loss: 0.0205, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 053, Loss: 0.0323, Val Loss: 0.0227, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 054, Loss: 0.0308, Val Loss: 0.0234, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 055, Loss: 0.0297, Val Loss: 0.0233, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 056, Loss: 0.0282, Val Loss: 0.0252, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 057, Loss: 0.0276, Val Loss: 0.0273, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 058, Loss: 0.0267, Val Loss: 0.0269, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 059, Loss: 0.0254, Val Loss: 0.0268, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 060, Loss: 0.0248, Val Loss: 0.0288, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 061, Loss: 0.0237, Val Loss: 0.0288, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 062, Loss: 0.0236, Val Loss: 0.0302, Accuracy: 0.986, Val Acc: 1.000\n",
            "Epoch 063, Loss: 0.0227, Val Loss: 0.0303, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 064, Loss: 0.0221, Val Loss: 0.0298, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 065, Loss: 0.0213, Val Loss: 0.0314, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 066, Loss: 0.0210, Val Loss: 0.0331, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 067, Loss: 0.0205, Val Loss: 0.0323, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 068, Loss: 0.0197, Val Loss: 0.0322, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 069, Loss: 0.0194, Val Loss: 0.0338, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 070, Loss: 0.0188, Val Loss: 0.0335, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 071, Loss: 0.0184, Val Loss: 0.0340, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 072, Loss: 0.0180, Val Loss: 0.0346, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 073, Loss: 0.0176, Val Loss: 0.0351, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 074, Loss: 0.0172, Val Loss: 0.0352, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 075, Loss: 0.0169, Val Loss: 0.0360, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 076, Loss: 0.0165, Val Loss: 0.0360, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 077, Loss: 0.0163, Val Loss: 0.0367, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 078, Loss: 0.0159, Val Loss: 0.0366, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 079, Loss: 0.0156, Val Loss: 0.0369, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 080, Loss: 0.0152, Val Loss: 0.0378, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 081, Loss: 0.0148, Val Loss: 0.0384, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 082, Loss: 0.0145, Val Loss: 0.0392, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 083, Loss: 0.0141, Val Loss: 0.0396, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 084, Loss: 0.0138, Val Loss: 0.0403, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 085, Loss: 0.0134, Val Loss: 0.0412, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 086, Loss: 0.0133, Val Loss: 0.0424, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 087, Loss: 0.0129, Val Loss: 0.0418, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 088, Loss: 0.0127, Val Loss: 0.0423, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 089, Loss: 0.0125, Val Loss: 0.0436, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 090, Loss: 0.0123, Val Loss: 0.0430, Accuracy: 1.000, Val Acc: 1.000\n",
            "Epoch 091, Loss: 0.0121, Val Loss: 0.0435, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 092, Loss: 0.0118, Val Loss: 0.0444, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 093, Loss: 0.0117, Val Loss: 0.0446, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 094, Loss: 0.0115, Val Loss: 0.0444, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 095, Loss: 0.0114, Val Loss: 0.0448, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 096, Loss: 0.0112, Val Loss: 0.0452, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 097, Loss: 0.0110, Val Loss: 0.0453, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 098, Loss: 0.0109, Val Loss: 0.0454, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 099, Loss: 0.0108, Val Loss: 0.0457, Accuracy: 1.000, Val Acc: 0.938\n",
            "Epoch 100, Loss: 0.0106, Val Loss: 0.0458, Accuracy: 1.000, Val Acc: 0.938\n",
            "\n",
            "Test Accuracy: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create a model of Iris using all three types of objective variables**"
      ],
      "metadata": {
        "id": "JKrOO9xSZyuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multi-class classification of Iris dataset using a neural network implemented in TensorFlow 2.x.\n",
        "This model is configured to classify all three species of Iris.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 1. Data Preparation\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X, dtype=np.float32) # Ensure X is float32\n",
        "y = np.array(y).reshape(-1, 1) # Reshape for one-hot encoding\n",
        "\n",
        "# One-hot encode the labels for multi-class classification\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "y = ohe.fit_transform(y)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Split into train, val, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Define a simple mini-batch iterator\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# 2. Hyperparameter and Model Setup\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = 3 # NOW 3 CLASSES FOR THE 3 SPECIES\n",
        "\n",
        "# Define weights and biases as tf.Variable for TensorFlow 2.x\n",
        "tf.random.set_seed(0)\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random.normal([n_input, n_hidden1], dtype=tf.float32)),\n",
        "    'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2], dtype=tf.float32)),\n",
        "    'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes], dtype=tf.float32))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden1], dtype=tf.float32)),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden2], dtype=tf.float32)),\n",
        "    'b3': tf.Variable(tf.random.normal([n_classes], dtype=tf.float32))\n",
        "}\n",
        "\n",
        "# Define the Adam optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# 3. Model Architecture (Forward Pass)\n",
        "def example_net(x):\n",
        "    \"\"\"Simple 3-layer neural network for multi-class classification.\"\"\"\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 4. Training and Evaluation Loop\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        # Use tf.GradientTape to record operations for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = example_net(mini_batch_x)\n",
        "            # Use softmax cross-entropy for multi-class classification\n",
        "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=mini_batch_y, logits=logits))\n",
        "\n",
        "        # Calculate gradients and apply them\n",
        "        gradients = tape.gradient(loss, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                         biases['b1'], biases['b2'], biases['b3']])\n",
        "        optimizer.apply_gradients(zip(gradients, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                                  biases['b1'], biases['b2'], biases['b3']]))\n",
        "\n",
        "        # Calculate accuracy for the mini-batch\n",
        "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(mini_batch_y, 1))\n",
        "        acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "        total_acc += acc.numpy()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    total_batch = len(get_mini_batch_train)\n",
        "    total_loss /= total_batch\n",
        "    total_acc /= total_batch\n",
        "\n",
        "    # Validation\n",
        "    val_logits = example_net(X_val)\n",
        "    val_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_val, logits=val_logits))\n",
        "    val_correct_pred = tf.equal(tf.argmax(val_logits, 1), tf.argmax(y_val, 1))\n",
        "    val_acc = tf.reduce_mean(tf.cast(val_correct_pred, tf.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, Loss: {total_loss:.4f}, Val Loss: {val_loss.numpy():.4f}, Accuracy: {total_acc:.3f}, Val Acc: {val_acc.numpy():.3f}\")\n",
        "\n",
        "# Test\n",
        "test_logits = example_net(X_test)\n",
        "test_correct_pred = tf.equal(tf.argmax(test_logits, 1), tf.argmax(y_test, 1))\n",
        "test_acc = tf.reduce_mean(tf.cast(test_correct_pred, tf.float32))\n",
        "print(f\"\\nTest Accuracy: {test_acc.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUoRjCnSa1kj",
        "outputId": "47929395-e2ed-4914-e391-403085ab10ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 76.5162, Val Loss: 54.4866, Accuracy: 0.000, Val Acc: 0.000\n",
            "Epoch 002, Loss: 33.5534, Val Loss: 29.0514, Accuracy: 0.133, Val Acc: 0.333\n",
            "Epoch 003, Loss: 15.0872, Val Loss: 19.6846, Accuracy: 0.497, Val Acc: 0.375\n",
            "Epoch 004, Loss: 4.0110, Val Loss: 10.8383, Accuracy: 0.687, Val Acc: 0.750\n",
            "Epoch 005, Loss: 1.1285, Val Loss: 5.0824, Accuracy: 0.890, Val Acc: 0.750\n",
            "Epoch 006, Loss: 1.5111, Val Loss: 4.7168, Accuracy: 0.930, Val Acc: 0.750\n",
            "Epoch 007, Loss: 1.2311, Val Loss: 6.0103, Accuracy: 0.950, Val Acc: 0.792\n",
            "Epoch 008, Loss: 1.0240, Val Loss: 7.0371, Accuracy: 0.930, Val Acc: 0.792\n",
            "Epoch 009, Loss: 0.8997, Val Loss: 5.9861, Accuracy: 0.940, Val Acc: 0.792\n",
            "Epoch 010, Loss: 0.8538, Val Loss: 5.7975, Accuracy: 0.950, Val Acc: 0.833\n",
            "Epoch 011, Loss: 0.7821, Val Loss: 5.9163, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 012, Loss: 0.7058, Val Loss: 5.5962, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 013, Loss: 0.6415, Val Loss: 5.3032, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 014, Loss: 0.5830, Val Loss: 5.2504, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 015, Loss: 0.5357, Val Loss: 5.2061, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 016, Loss: 0.4914, Val Loss: 4.9491, Accuracy: 0.960, Val Acc: 0.833\n",
            "Epoch 017, Loss: 0.4638, Val Loss: 4.7927, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 018, Loss: 0.4487, Val Loss: 4.6504, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 019, Loss: 0.4362, Val Loss: 4.5569, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 020, Loss: 0.4258, Val Loss: 4.4803, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 021, Loss: 0.4154, Val Loss: 4.3860, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 022, Loss: 0.4052, Val Loss: 4.3096, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 023, Loss: 0.3952, Val Loss: 4.2378, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 024, Loss: 0.3853, Val Loss: 4.1627, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 025, Loss: 0.3753, Val Loss: 4.0893, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 026, Loss: 0.3652, Val Loss: 4.0192, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 027, Loss: 0.3552, Val Loss: 3.9493, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 028, Loss: 0.3451, Val Loss: 3.8772, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 029, Loss: 0.3350, Val Loss: 3.8016, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 030, Loss: 0.3248, Val Loss: 3.7219, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 031, Loss: 0.3147, Val Loss: 3.6338, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 032, Loss: 0.3046, Val Loss: 3.5341, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 033, Loss: 0.2945, Val Loss: 3.4205, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 034, Loss: 0.2845, Val Loss: 3.2941, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 035, Loss: 0.2745, Val Loss: 3.1583, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 036, Loss: 0.2645, Val Loss: 3.0173, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 037, Loss: 0.2545, Val Loss: 2.8735, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 038, Loss: 0.2444, Val Loss: 2.7285, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 039, Loss: 0.2343, Val Loss: 2.5844, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 040, Loss: 0.2240, Val Loss: 2.4447, Accuracy: 0.980, Val Acc: 0.833\n",
            "Epoch 041, Loss: 0.2137, Val Loss: 2.3136, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 042, Loss: 0.2033, Val Loss: 2.1845, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 043, Loss: 0.1929, Val Loss: 2.0706, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 044, Loss: 0.1825, Val Loss: 1.9639, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 045, Loss: 0.1719, Val Loss: 1.8629, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 046, Loss: 0.1614, Val Loss: 1.7703, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 047, Loss: 0.1510, Val Loss: 1.6918, Accuracy: 0.980, Val Acc: 0.917\n",
            "Epoch 048, Loss: 0.1409, Val Loss: 1.6302, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 049, Loss: 0.1313, Val Loss: 1.5829, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 050, Loss: 0.1225, Val Loss: 1.5352, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 051, Loss: 0.1140, Val Loss: 1.4947, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 052, Loss: 0.1063, Val Loss: 1.4479, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 053, Loss: 0.1005, Val Loss: 1.4159, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 054, Loss: 0.0924, Val Loss: 1.3674, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 055, Loss: 0.0855, Val Loss: 1.3201, Accuracy: 0.980, Val Acc: 0.875\n",
            "Epoch 056, Loss: 0.0769, Val Loss: 1.2805, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 057, Loss: 0.0727, Val Loss: 1.2459, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 058, Loss: 0.0601, Val Loss: 1.1851, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 059, Loss: 0.0510, Val Loss: 1.1399, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 060, Loss: 0.0424, Val Loss: 1.1024, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 061, Loss: 0.0351, Val Loss: 1.0696, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 062, Loss: 0.0268, Val Loss: 1.0406, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 063, Loss: 0.0210, Val Loss: 1.0227, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 064, Loss: 0.0153, Val Loss: 1.0116, Accuracy: 0.990, Val Acc: 0.875\n",
            "Epoch 065, Loss: 0.0109, Val Loss: 0.9970, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 066, Loss: 0.0085, Val Loss: 0.9897, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 067, Loss: 0.0072, Val Loss: 0.9836, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 068, Loss: 0.0059, Val Loss: 0.9784, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 069, Loss: 0.0055, Val Loss: 0.9740, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 070, Loss: 0.0045, Val Loss: 0.9675, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 071, Loss: 0.0044, Val Loss: 0.9694, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 072, Loss: 0.0037, Val Loss: 0.9618, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 073, Loss: 0.0037, Val Loss: 0.9646, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 074, Loss: 0.0032, Val Loss: 0.9600, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 075, Loss: 0.0031, Val Loss: 0.9593, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 076, Loss: 0.0029, Val Loss: 0.9572, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 077, Loss: 0.0027, Val Loss: 0.9560, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 078, Loss: 0.0026, Val Loss: 0.9546, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 079, Loss: 0.0025, Val Loss: 0.9537, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 080, Loss: 0.0024, Val Loss: 0.9524, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 081, Loss: 0.0023, Val Loss: 0.9512, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 082, Loss: 0.0022, Val Loss: 0.9504, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 083, Loss: 0.0021, Val Loss: 0.9489, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 084, Loss: 0.0020, Val Loss: 0.9493, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 085, Loss: 0.0020, Val Loss: 0.9478, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 086, Loss: 0.0019, Val Loss: 0.9458, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 087, Loss: 0.0018, Val Loss: 0.9460, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 088, Loss: 0.0018, Val Loss: 0.9460, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 089, Loss: 0.0017, Val Loss: 0.9444, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 090, Loss: 0.0017, Val Loss: 0.9429, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 091, Loss: 0.0016, Val Loss: 0.9432, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 092, Loss: 0.0016, Val Loss: 0.9425, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 093, Loss: 0.0016, Val Loss: 0.9414, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 094, Loss: 0.0015, Val Loss: 0.9412, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 095, Loss: 0.0015, Val Loss: 0.9407, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 096, Loss: 0.0015, Val Loss: 0.9402, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 097, Loss: 0.0014, Val Loss: 0.9388, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 098, Loss: 0.0014, Val Loss: 0.9387, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 099, Loss: 0.0014, Val Loss: 0.9392, Accuracy: 1.000, Val Acc: 0.875\n",
            "Epoch 100, Loss: 0.0014, Val Loss: 0.9382, Accuracy: 1.000, Val Acc: 0.875\n",
            "\n",
            "Test Accuracy: 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Create a model of House Prices**"
      ],
      "metadata": {
        "id": "6KjiXXcKbupu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Regression model for House Prices dataset using a neural network implemented in TensorFlow 2.x.\n",
        "This model predicts the 'SalePrice' based on 'GrLivArea' and 'YearBuilt'.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Data Preparation\n",
        "# Load the dataset (assuming 'House_Prices.csv' is available)\n",
        "df = pd.read_csv(\"House_Prices.csv\")\n",
        "\n",
        "# Select features (X) and label (y)\n",
        "X = df[['GrLivArea', 'YearBuilt']]\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Convert to NumPy arrays for easier processing\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.float32).reshape(-1, 1)\n",
        "\n",
        "# Standardize the data. This is crucial for regression models.\n",
        "scaler_X = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "scaler_y = StandardScaler()\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# Split into train, val, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Define a simple mini-batch iterator\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# 2. Hyperparameter and Model Setup\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_output = 1  # For regression, the output is a single continuous value\n",
        "\n",
        "# Define weights and biases as tf.Variable for TensorFlow 2.x\n",
        "tf.random.set_seed(0)\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random.normal([n_input, n_hidden1], dtype=tf.float32)),\n",
        "    'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2], dtype=tf.float32)),\n",
        "    'w3': tf.Variable(tf.random.normal([n_hidden2, n_output], dtype=tf.float32))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden1], dtype=tf.float32)),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden2], dtype=tf.float32)),\n",
        "    'b3': tf.Variable(tf.random.normal([n_output], dtype=tf.float32))\n",
        "}\n",
        "\n",
        "# Define the Adam optimizer and Mean Squared Error loss\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# 3. Model Architecture (Forward Pass)\n",
        "def example_net(x):\n",
        "    \"\"\"Simple 3-layer neural network for regression.\"\"\"\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # The final layer produces a single output without an activation function\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 4. Training and Evaluation Loop\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        # Use tf.GradientTape to record operations for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = example_net(mini_batch_x)\n",
        "            loss = mse_loss(mini_batch_y, y_pred)\n",
        "\n",
        "        # Calculate gradients and apply them\n",
        "        gradients = tape.gradient(loss, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                         biases['b1'], biases['b2'], biases['b3']])\n",
        "        optimizer.apply_gradients(zip(gradients, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                                  biases['b1'], biases['b2'], biases['b3']]))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    total_batch = len(get_mini_batch_train)\n",
        "    total_loss /= total_batch\n",
        "\n",
        "    # Validation\n",
        "    val_pred = example_net(X_val)\n",
        "    val_loss = mse_loss(y_val, val_pred)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, Loss: {total_loss:.4f}, Val Loss: {val_loss.numpy():.4f}\")\n",
        "\n",
        "# Test\n",
        "test_pred = example_net(X_test)\n",
        "test_loss = mse_loss(y_test, test_pred)\n",
        "print(f\"\\nTest MSE: {test_loss.numpy():.4f}\")"
      ],
      "metadata": {
        "id": "eJlf56z3k75b",
        "outputId": "f5dcedb1-ad0d-4534-c049-40e0dd4499fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 404.3761, Val Loss: 65.7382\n",
            "Epoch 002, Loss: 48.3589, Val Loss: 32.1602\n",
            "Epoch 003, Loss: 26.6619, Val Loss: 19.8194\n",
            "Epoch 004, Loss: 18.4097, Val Loss: 13.3238\n",
            "Epoch 005, Loss: 13.2609, Val Loss: 10.0146\n",
            "Epoch 006, Loss: 9.9662, Val Loss: 7.9801\n",
            "Epoch 007, Loss: 7.7832, Val Loss: 6.5586\n",
            "Epoch 008, Loss: 6.2552, Val Loss: 5.5688\n",
            "Epoch 009, Loss: 5.1503, Val Loss: 4.8929\n",
            "Epoch 010, Loss: 4.3165, Val Loss: 4.4184\n",
            "Epoch 011, Loss: 3.6568, Val Loss: 4.0572\n",
            "Epoch 012, Loss: 3.1292, Val Loss: 3.7596\n",
            "Epoch 013, Loss: 2.6997, Val Loss: 3.4877\n",
            "Epoch 014, Loss: 2.3475, Val Loss: 3.2716\n",
            "Epoch 015, Loss: 2.0751, Val Loss: 3.0659\n",
            "Epoch 016, Loss: 1.8461, Val Loss: 2.8675\n",
            "Epoch 017, Loss: 1.6545, Val Loss: 2.6809\n",
            "Epoch 018, Loss: 1.4981, Val Loss: 2.5095\n",
            "Epoch 019, Loss: 1.3736, Val Loss: 2.3540\n",
            "Epoch 020, Loss: 1.2655, Val Loss: 2.2241\n",
            "Epoch 021, Loss: 1.1730, Val Loss: 2.1083\n",
            "Epoch 022, Loss: 1.0922, Val Loss: 2.0069\n",
            "Epoch 023, Loss: 1.0214, Val Loss: 1.9107\n",
            "Epoch 024, Loss: 0.9617, Val Loss: 1.8158\n",
            "Epoch 025, Loss: 0.9104, Val Loss: 1.7315\n",
            "Epoch 026, Loss: 0.8663, Val Loss: 1.6619\n",
            "Epoch 027, Loss: 0.8310, Val Loss: 1.6014\n",
            "Epoch 028, Loss: 0.8029, Val Loss: 1.5578\n",
            "Epoch 029, Loss: 0.7816, Val Loss: 1.5190\n",
            "Epoch 030, Loss: 0.7655, Val Loss: 1.5014\n",
            "Epoch 031, Loss: 0.7500, Val Loss: 1.4771\n",
            "Epoch 032, Loss: 0.7381, Val Loss: 1.4864\n",
            "Epoch 033, Loss: 0.7295, Val Loss: 1.4666\n",
            "Epoch 034, Loss: 0.7221, Val Loss: 1.4708\n",
            "Epoch 035, Loss: 0.7185, Val Loss: 1.4912\n",
            "Epoch 036, Loss: 0.7138, Val Loss: 1.4772\n",
            "Epoch 037, Loss: 0.7106, Val Loss: 1.4921\n",
            "Epoch 038, Loss: 0.7053, Val Loss: 1.4828\n",
            "Epoch 039, Loss: 0.7033, Val Loss: 1.4811\n",
            "Epoch 040, Loss: 0.7034, Val Loss: 1.5029\n",
            "Epoch 041, Loss: 0.7011, Val Loss: 1.4846\n",
            "Epoch 042, Loss: 0.7017, Val Loss: 1.4814\n",
            "Epoch 043, Loss: 0.7002, Val Loss: 1.4901\n",
            "Epoch 044, Loss: 0.6923, Val Loss: 1.4746\n",
            "Epoch 045, Loss: 0.6798, Val Loss: 1.4536\n",
            "Epoch 046, Loss: 0.6709, Val Loss: 1.4257\n",
            "Epoch 047, Loss: 0.6547, Val Loss: 1.3935\n",
            "Epoch 048, Loss: 0.6464, Val Loss: 1.3507\n",
            "Epoch 049, Loss: 0.6274, Val Loss: 1.2970\n",
            "Epoch 050, Loss: 0.6199, Val Loss: 1.2670\n",
            "Epoch 051, Loss: 0.6130, Val Loss: 1.2042\n",
            "Epoch 052, Loss: 0.6073, Val Loss: 1.1921\n",
            "Epoch 053, Loss: 0.6014, Val Loss: 1.1532\n",
            "Epoch 054, Loss: 0.6009, Val Loss: 1.1044\n",
            "Epoch 055, Loss: 0.5946, Val Loss: 1.0881\n",
            "Epoch 056, Loss: 0.5941, Val Loss: 1.0618\n",
            "Epoch 057, Loss: 0.5758, Val Loss: 1.0380\n",
            "Epoch 058, Loss: 0.5892, Val Loss: 1.0303\n",
            "Epoch 059, Loss: 0.5784, Val Loss: 1.0228\n",
            "Epoch 060, Loss: 0.5735, Val Loss: 1.0271\n",
            "Epoch 061, Loss: 0.5869, Val Loss: 1.0480\n",
            "Epoch 062, Loss: 0.5967, Val Loss: 1.0519\n",
            "Epoch 063, Loss: 0.5902, Val Loss: 1.0580\n",
            "Epoch 064, Loss: 0.6044, Val Loss: 1.0512\n",
            "Epoch 065, Loss: 0.6033, Val Loss: 1.0486\n",
            "Epoch 066, Loss: 0.5880, Val Loss: 1.0544\n",
            "Epoch 067, Loss: 0.5898, Val Loss: 1.0400\n",
            "Epoch 068, Loss: 0.5750, Val Loss: 1.0257\n",
            "Epoch 069, Loss: 0.6003, Val Loss: 1.0137\n",
            "Epoch 070, Loss: 0.5793, Val Loss: 0.9913\n",
            "Epoch 071, Loss: 0.5911, Val Loss: 0.9903\n",
            "Epoch 072, Loss: 0.5815, Val Loss: 0.9776\n",
            "Epoch 073, Loss: 0.5742, Val Loss: 0.9637\n",
            "Epoch 074, Loss: 0.6149, Val Loss: 0.9612\n",
            "Epoch 075, Loss: 0.5962, Val Loss: 0.9502\n",
            "Epoch 076, Loss: 0.5953, Val Loss: 0.9345\n",
            "Epoch 077, Loss: 0.5853, Val Loss: 0.9450\n",
            "Epoch 078, Loss: 0.6125, Val Loss: 0.9173\n",
            "Epoch 079, Loss: 0.5676, Val Loss: 0.9071\n",
            "Epoch 080, Loss: 0.5712, Val Loss: 0.9111\n",
            "Epoch 081, Loss: 0.5822, Val Loss: 0.9011\n",
            "Epoch 082, Loss: 0.5587, Val Loss: 0.8792\n",
            "Epoch 083, Loss: 0.5744, Val Loss: 0.8821\n",
            "Epoch 084, Loss: 0.5604, Val Loss: 0.8668\n",
            "Epoch 085, Loss: 0.5819, Val Loss: 0.8622\n",
            "Epoch 086, Loss: 0.5648, Val Loss: 0.8589\n",
            "Epoch 087, Loss: 0.5650, Val Loss: 0.8619\n",
            "Epoch 088, Loss: 0.6022, Val Loss: 0.8869\n",
            "Epoch 089, Loss: 0.6319, Val Loss: 0.8974\n",
            "Epoch 090, Loss: 0.6637, Val Loss: 0.9157\n",
            "Epoch 091, Loss: 0.6919, Val Loss: 0.8887\n",
            "Epoch 092, Loss: 0.6651, Val Loss: 0.8497\n",
            "Epoch 093, Loss: 0.6531, Val Loss: 0.8010\n",
            "Epoch 094, Loss: 0.6593, Val Loss: 0.7996\n",
            "Epoch 095, Loss: 0.6658, Val Loss: 0.7871\n",
            "Epoch 096, Loss: 0.6554, Val Loss: 0.7685\n",
            "Epoch 097, Loss: 0.6678, Val Loss: 0.7808\n",
            "Epoch 098, Loss: 0.6602, Val Loss: 0.7676\n",
            "Epoch 099, Loss: 0.6505, Val Loss: 0.7640\n",
            "Epoch 100, Loss: 0.6642, Val Loss: 0.7567\n",
            "\n",
            "Test MSE: 0.8153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adapting the Model for Regression**\n",
        "\n",
        "Switching from a classification problem (like the Iris dataset) to a regression problem (like predicting house prices) requires fundamental changes to the neural network's architecture and training process. The goal is no longer to assign a category but to predict a continuous, numerical value.\n",
        "\n",
        "**Here are the key differences I've implemented in the updated code:**\n",
        "\n",
        "**Data Preparation:** I've modified the data loading to use GrLivArea and YearBuilt as explanatory variables and SalePrice as the objective variable. To ensure the model trains efficiently, all data (both features and the target SalePrice) is now standardized using a StandardScaler.\n",
        "\n",
        "**Output Layer: **The final layer of the neural network has been changed from 3 neurons to 1 neuron. A regression model only needs a single output neuron to predict the single continuous value.\n",
        "\n",
        "**Loss Function:** The multi-class softmax_cross_entropy_with_logits has been replaced with mean_squared_error (tf.keras.losses.MeanSquaredError). This is the most common loss function for regression, as it measures the average squared difference between the predicted and actual values.\n",
        "\n",
        "**Evaluation:** The accuracy metric has been removed, as it is only relevant for classification. Instead, the training and validation progress are now tracked using the Mean Squared Error (MSE) loss itself.\n",
        "\n"
      ],
      "metadata": {
        "id": "fowlClr9nHhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Create a model of MNIST**"
      ],
      "metadata": {
        "id": "RZTbasAtmogb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multi-class classification of the MNIST dataset using a simple neural network implemented in TensorFlow 2.x.\n",
        "This model is configured to classify all 10 digits (0-9).\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 0. GPU Configuration Check\n",
        "# Check for GPU device availability.\n",
        "# tf.config.list_physical_devices() is the modern TensorFlow 2.x method.\n",
        "# tf.test.gpu_device_name() is an older TensorFlow 1.x method,\n",
        "# and it may not work as expected in a TensorFlow 2.x environment.\n",
        "# The following code is for checking GPU availability in TensorFlow 2.x.\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "    print(\"Note: '!pip install tensorflow-gpu==1.14.0' is for TensorFlow 1.x.\")\n",
        "    print(\"This code is written for TensorFlow 2.x, and the GPU is automatically utilized if available.\")\n",
        "\n",
        "# 1. Data Preparation\n",
        "# Load the MNIST dataset directly from TensorFlow\n",
        "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "X_train = X_train_raw.astype(np.float32) / 255.0\n",
        "X_test = X_test_raw.astype(np.float32) / 255.0\n",
        "\n",
        "# Flatten the 28x28 images into a 784-dimensional vector\n",
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# One-hot encode the labels. For example, the label '5' becomes a vector [0,0,0,0,0,1,0,0,0,0]\n",
        "y_train = tf.keras.utils.to_categorical(y_train_raw, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_raw, 10)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Define a simple mini-batch iterator\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# 2. Hyperparameter and Model Setup\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "n_hidden1 = 128\n",
        "n_hidden2 = 64\n",
        "n_input = 28 * 28 # Flattened image size\n",
        "n_classes = 10    # 10 digits from 0 to 9\n",
        "\n",
        "# Define weights and biases as tf.Variable for TensorFlow 2.x\n",
        "tf.random.set_seed(0)\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random.normal([n_input, n_hidden1], dtype=tf.float32)),\n",
        "    'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2], dtype=tf.float32)),\n",
        "    'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes], dtype=tf.float32))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden1], dtype=tf.float32)),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden2], dtype=tf.float32)),\n",
        "    'b3': tf.Variable(tf.random.normal([n_classes], dtype=tf.float32))\n",
        "}\n",
        "\n",
        "# Define the Adam optimizer and Categorical Crossentropy loss\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 3. Model Architecture (Forward Pass)\n",
        "def mnist_net(x):\n",
        "    \"\"\"Simple 3-layer neural network for MNIST classification.\"\"\"\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 4. Training and Evaluation Loop\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        # Use tf.GradientTape to record operations for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = mnist_net(mini_batch_x)\n",
        "            loss = loss_fn(mini_batch_y, logits)\n",
        "\n",
        "        # Calculate gradients and apply them\n",
        "        gradients = tape.gradient(loss, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                         biases['b1'], biases['b2'], biases['b3']])\n",
        "        optimizer.apply_gradients(zip(gradients, [weights['w1'], weights['w2'], weights['w3'],\n",
        "                                                  biases['b1'], biases['b2'], biases['b3']]))\n",
        "\n",
        "        # Calculate accuracy for the mini-batch\n",
        "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(mini_batch_y, 1))\n",
        "        acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "        total_acc += acc.numpy()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    total_batch = len(get_mini_batch_train)\n",
        "    total_loss /= total_batch\n",
        "    total_acc /= total_batch\n",
        "\n",
        "    # Validation\n",
        "    val_logits = mnist_net(X_val)\n",
        "    val_loss = loss_fn(y_val, val_logits)\n",
        "    val_correct_pred = tf.equal(tf.argmax(val_logits, 1), tf.argmax(y_val, 1))\n",
        "    val_acc = tf.reduce_mean(tf.cast(val_correct_pred, tf.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, Loss: {total_loss:.4f}, Val Loss: {val_loss.numpy():.4f}, Accuracy: {total_acc:.3f}, Val Acc: {val_acc.numpy():.3f}\")\n",
        "\n",
        "# Test\n",
        "test_logits = mnist_net(X_test)\n",
        "test_loss = loss_fn(y_test, test_logits)\n",
        "test_correct_pred = tf.equal(tf.argmax(test_logits, 1), tf.argmax(y_test, 1))\n",
        "test_acc = tf.reduce_mean(tf.cast(test_correct_pred, tf.float32))\n",
        "print(f\"\\nTest Loss: {test_loss.numpy():.4f}, Test Accuracy: {test_acc.numpy():.3f}\")"
      ],
      "metadata": {
        "id": "l5WcLX8lniws",
        "outputId": "f020e14f-5b20-4598-bf4b-bbdfaf12a0b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "Please install GPU version of TF\n",
            "Note: '!pip install tensorflow-gpu==1.14.0' is for TensorFlow 1.x.\n",
            "This code is written for TensorFlow 2.x, and the GPU is automatically utilized if available.\n",
            "Epoch 001, Loss: 112.0244, Val Loss: 33.4330, Accuracy: 0.533, Val Acc: 0.743\n",
            "Epoch 002, Loss: 25.1438, Val Loss: 19.7584, Accuracy: 0.790, Val Acc: 0.817\n",
            "Epoch 003, Loss: 16.2872, Val Loss: 14.3949, Accuracy: 0.838, Val Acc: 0.848\n",
            "Epoch 004, Loss: 11.9886, Val Loss: 11.4416, Accuracy: 0.865, Val Acc: 0.864\n",
            "Epoch 005, Loss: 9.2795, Val Loss: 9.4317, Accuracy: 0.881, Val Acc: 0.877\n",
            "Epoch 006, Loss: 7.4390, Val Loss: 8.1914, Accuracy: 0.893, Val Acc: 0.884\n",
            "Epoch 007, Loss: 6.1069, Val Loss: 7.2564, Accuracy: 0.904, Val Acc: 0.889\n",
            "Epoch 008, Loss: 5.0883, Val Loss: 6.5296, Accuracy: 0.912, Val Acc: 0.895\n",
            "Epoch 009, Loss: 4.2969, Val Loss: 5.9366, Accuracy: 0.920, Val Acc: 0.900\n",
            "Epoch 010, Loss: 3.6512, Val Loss: 5.4842, Accuracy: 0.926, Val Acc: 0.904\n",
            "Epoch 011, Loss: 3.1175, Val Loss: 5.1107, Accuracy: 0.931, Val Acc: 0.905\n",
            "Epoch 012, Loss: 2.6655, Val Loss: 4.8251, Accuracy: 0.936, Val Acc: 0.906\n",
            "Epoch 013, Loss: 2.2951, Val Loss: 4.5668, Accuracy: 0.940, Val Acc: 0.908\n",
            "Epoch 014, Loss: 1.9675, Val Loss: 4.3626, Accuracy: 0.945, Val Acc: 0.909\n",
            "Epoch 015, Loss: 1.6922, Val Loss: 4.1539, Accuracy: 0.948, Val Acc: 0.911\n",
            "Epoch 016, Loss: 1.4547, Val Loss: 4.0258, Accuracy: 0.953, Val Acc: 0.913\n",
            "Epoch 017, Loss: 1.2442, Val Loss: 3.9134, Accuracy: 0.957, Val Acc: 0.914\n",
            "Epoch 018, Loss: 1.0722, Val Loss: 3.8069, Accuracy: 0.959, Val Acc: 0.913\n",
            "Epoch 019, Loss: 0.9300, Val Loss: 3.7214, Accuracy: 0.962, Val Acc: 0.915\n",
            "Epoch 020, Loss: 0.7921, Val Loss: 3.6005, Accuracy: 0.966, Val Acc: 0.916\n",
            "\n",
            "Test Loss: 3.6957, Test Accuracy: 0.915\n"
          ]
        }
      ]
    }
  ]
}