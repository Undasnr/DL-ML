{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv7cNFfmKbjZbMu4lSzsSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_CNN1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Creating a one-dimensional convolutional layer class that limits the number of channels to one**"
      ],
      "metadata": {
        "id": "jyP2tz-upQTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UbD3qgVIowBE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class SimpleConv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer with a single channel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size):\n",
        "        # Initializing filter size, weights, and bias\n",
        "        self.filter_size = filter_size\n",
        "        self.W = W_initializer.W(filter_size, 1).flatten()  # Flatten to a 1D array for single channel\n",
        "        self.b = b_initializer.b(1)\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (N_in,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (N_out,).\n",
        "        \"\"\"\n",
        "        # Storing input for backpropagation\n",
        "        self.x = x\n",
        "\n",
        "        # Calculating output size\n",
        "        N_in = x.shape[0]\n",
        "        self.out_size = N_in - self.filter_size + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros(self.out_size)\n",
        "\n",
        "        # Performing convolution\n",
        "        for i in range(self.out_size):\n",
        "            # The formula is a_i = sum(x_(i+s) * w_s) + b\n",
        "            a[i] = np.dot(x[i : i + self.filter_size], self.W) + self.b\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (N_out,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (N_in,).\n",
        "        \"\"\"\n",
        "        N_in = self.x.shape[0]\n",
        "\n",
        "        # Calculating gradients for weights and bias\n",
        "        self.dW = np.zeros(self.filter_size)\n",
        "        for s in range(self.filter_size):\n",
        "            # dL/dw_s = sum(dL/da_i * x_(i+s))\n",
        "            self.dW[s] = np.sum(da * self.x[s : s + self.out_size])\n",
        "\n",
        "        self.db = np.sum(da)\n",
        "\n",
        "        # Updating weights and biases\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        dx = np.zeros(N_in)\n",
        "        # Padding the da array with zeros for easier calculation\n",
        "        da_padded = np.pad(da, (self.filter_size - 1, self.filter_size - 1), 'constant', constant_values=0)\n",
        "\n",
        "        # The formula is dL/dx_j = sum(dL/da_(j-s) * w_s)\n",
        "        for j in range(N_in):\n",
        "            # Note: We need to reverse the weights for the convolution operation\n",
        "            dx[j] = np.dot(da_padded[j : j + self.filter_size], self.W[::-1])\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Output size calculation after one-dimensional convolution**"
      ],
      "metadata": {
        "id": "th5_d_a6qJLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class SimpleConv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer with a single channel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size):\n",
        "        # Initializing filter size, weights, and bias\n",
        "        self.filter_size = filter_size\n",
        "        # Corrected initializer calls\n",
        "        self.W = W_initializer.W(filter_size, 1).flatten()\n",
        "        self.b = b_initializer.b(1)\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (N_in,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (N_out,).\n",
        "        \"\"\"\n",
        "        # Storing input for backpropagation\n",
        "        self.x = x\n",
        "\n",
        "        # Calculating output size (assuming P=0, S=1 as per Problem 1 instructions)\n",
        "        N_in = x.shape[0]\n",
        "        self.out_size = N_in - self.filter_size + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros(self.out_size)\n",
        "\n",
        "        # Performing convolution\n",
        "        for i in range(self.out_size):\n",
        "            # The formula is a_i = sum(x_(i+s) * w_s) + b\n",
        "            a[i] = np.dot(x[i : i + self.filter_size], self.W) + self.b\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (N_out,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (N_in,).\n",
        "        \"\"\"\n",
        "        N_in = self.x.shape[0]\n",
        "\n",
        "        # Calculating gradients for weights and bias\n",
        "        self.dW = np.zeros(self.filter_size)\n",
        "        for s in range(self.filter_size):\n",
        "            # dL/dw_s = sum(dL/da_i * x_(i+s))\n",
        "            self.dW[s] = np.sum(da * self.x[s : s + self.out_size])\n",
        "\n",
        "        self.db = np.sum(da)\n",
        "\n",
        "        # Updating weights and biases\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        dx = np.zeros(N_in)\n",
        "        # Padding the da array with zeros for easier calculation\n",
        "        da_padded = np.pad(da, (self.filter_size - 1, self.filter_size - 1), 'constant', constant_values=0)\n",
        "\n",
        "        # The formula is dL/dx_j = sum(dL/da_(j-s) * w_s)\n",
        "        for j in range(N_in):\n",
        "            # Note: We need to reverse the weights for the convolution operation\n",
        "            dx[j] = np.dot(da_padded[j : j + self.filter_size], self.W[::-1])\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "U5hDoV0sqnhu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Experiment of one-dimensional convolutional layer with small array"
      ],
      "metadata": {
        "id": "x3jKaKFjrOnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class SimpleConv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer with a single channel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size):\n",
        "        # Initializing filter size, weights, and bias\n",
        "        self.filter_size = filter_size\n",
        "        self.W = W_initializer.W().flatten().astype(np.float64)\n",
        "        self.b = b_initializer.b().astype(np.float64)\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (N_in,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (N_out,).\n",
        "        \"\"\"\n",
        "        # Storing input for backpropagation\n",
        "        self.x = x.astype(np.float64)\n",
        "\n",
        "        # Calculating output size (assuming P=0, S=1 as per Problem 1 instructions)\n",
        "        N_in = x.shape[0]\n",
        "        self.out_size = N_in - self.filter_size + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros(self.out_size)\n",
        "\n",
        "        # Performing convolution\n",
        "        for i in range(self.out_size):\n",
        "            # The formula is a_i = sum(x_(i+s) * w_s) + b\n",
        "            a[i] = np.dot(self.x[i : i + self.filter_size], self.W) + self.b\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (N_out,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (N_in,).\n",
        "        \"\"\"\n",
        "        N_in = self.x.shape[0]\n",
        "\n",
        "        # Calculating gradients for weights and bias\n",
        "        self.dW = np.zeros(self.filter_size)\n",
        "        for s in range(self.filter_size):\n",
        "            # dL/dw_s = sum(dL/da_i * x_(i+s))\n",
        "            self.dW[s] = np.sum(da * self.x[s : s + self.out_size])\n",
        "\n",
        "        self.db = np.sum(da)\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        dx = np.zeros(N_in)\n",
        "        # Padding the da array with zeros for easier calculation\n",
        "        da_padded = np.pad(da, (self.filter_size - 1, self.filter_size - 1), 'constant', constant_values=0)\n",
        "\n",
        "        # The formula is dL/dx_j = sum(dL/da_(j-s) * w_s)\n",
        "        for j in range(N_in):\n",
        "            # Note: We need to reverse the weights for the convolution operation\n",
        "            dx[j] = np.dot(da_padded[j : j + self.filter_size], self.W[::-1])\n",
        "\n",
        "        # Updating weights and biases AFTER calculating dx\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test case from Problem 3\n",
        "    x = np.array([1, 2, 3, 4])\n",
        "    w = np.array([3, 5, 7])\n",
        "    b = np.array([1])\n",
        "    delta_a = np.array([10, 20])\n",
        "\n",
        "    # Expected values\n",
        "    expected_a = np.array([35, 50])\n",
        "    expected_delta_b = np.array([30])\n",
        "    expected_delta_w = np.array([50, 80, 110])\n",
        "    expected_delta_x = np.array([30, 110, 170, 140])\n",
        "\n",
        "    # Creating layer instance and manually set weights/bias for testing\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    conv_layer = SimpleConv1d(W_initializer=XavierInitializer(3, 1),\n",
        "                              b_initializer=XavierInitializer(1, 1),\n",
        "                              optimizer=optimizer,\n",
        "                              filter_size=3)\n",
        "\n",
        "    conv_layer.W = w.astype(np.float64)\n",
        "    conv_layer.b = b.astype(np.float64)\n",
        "\n",
        "    # Forward propagation test\n",
        "    output_a = conv_layer.forward(x)\n",
        "    assert np.allclose(output_a, expected_a), f\"Forward prop failed: Expected {expected_a}, but got {output_a}\"\n",
        "    print(\"Forward propagation test passed!\")\n",
        "\n",
        "    # Backward propagation test\n",
        "    output_dx = conv_layer.backward(delta_a)\n",
        "    assert np.allclose(conv_layer.dW, expected_delta_w), f\"Backward prop (dW) failed: Expected {expected_delta_w}, but got {conv_layer.dW}\"\n",
        "    assert np.allclose(conv_layer.db, expected_delta_b), f\"Backward prop (db) failed: Expected {expected_delta_b}, but got {conv_layer.db}\"\n",
        "    assert np.allclose(output_dx, expected_delta_x), f\"Backward prop (dx) failed: Expected {expected_delta_x}, but got {output_dx}\"\n",
        "    print(\"Backward propagation test passed!\")\n",
        "    print(\"All tests for SimpleConv1d passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KvUm41FrU-0",
        "outputId": "2fee216a-05c5-4120-f1da-749496561089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation test passed!\n",
            "Backward propagation test passed!\n",
            "All tests for SimpleConv1d passed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3449902536.py:133: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  a[i] = np.dot(self.x[i : i + self.filter_size], self.W) + self.b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Creating a one-dimensional convolutional layer class that does not limit the number of channels**"
      ],
      "metadata": {
        "id": "aPYdOwQQrOlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class Conv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer that supports multiple channels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    in_channels : int\n",
        "        The number of input channels.\n",
        "    out_channels : int\n",
        "        The number of output channels.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size, in_channels, out_channels):\n",
        "        self.filter_size = filter_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Initializing weights with shape (out_channels, in_channels, filter_size)\n",
        "        # Using a new initializer instance for the correct dimensions\n",
        "        self.W_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.W = self.W_initializer.W().reshape(out_channels, in_channels, filter_size).astype(np.float64)\n",
        "\n",
        "        # Initializing biases with shape (out_channels,)\n",
        "        self.b_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.b = self.b_initializer.b().astype(np.float64)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (in_channels, N_in).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (out_channels, N_out).\n",
        "        \"\"\"\n",
        "        self.x = x.astype(np.float64)\n",
        "        N_in = self.x.shape[1]\n",
        "\n",
        "        # Calculating output size (P=0, S=1)\n",
        "        self.out_size = N_in - self.filter_size + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros((self.out_channels, self.out_size))\n",
        "\n",
        "        # Performing convolution for each output channel\n",
        "        for oc in range(self.out_channels):\n",
        "            # Iterating through each input channel and sum the results\n",
        "            for ic in range(self.in_channels):\n",
        "                for i in range(self.out_size):\n",
        "                    a[oc, i] += np.dot(self.x[ic, i : i + self.filter_size], self.W[oc, ic, :])\n",
        "            a[oc, :] += self.b[oc]\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (out_channels, N_out).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (in_channels, N_in).\n",
        "        \"\"\"\n",
        "        N_in = self.x.shape[1]\n",
        "\n",
        "        # Calculating gradients for weights and bias\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.sum(da, axis=1) # Sum gradients across features for each output channel\n",
        "\n",
        "        # Calculating dW\n",
        "        for oc in range(self.out_channels):\n",
        "            for ic in range(self.in_channels):\n",
        "                for s in range(self.filter_size):\n",
        "                    self.dW[oc, ic, s] = np.sum(da[oc, :] * self.x[ic, s:s + self.out_size])\n",
        "\n",
        "        # Calculating dx\n",
        "        dx = np.zeros(self.x.shape)\n",
        "        da_padded = np.pad(da, ((0, 0), (self.filter_size - 1, self.filter_size - 1)), 'constant', constant_values=0)\n",
        "\n",
        "        for ic in range(self.in_channels):\n",
        "            for oc in range(self.out_channels):\n",
        "                for j in range(N_in):\n",
        "                    dx[ic, j] += np.dot(da_padded[oc, j : j + self.filter_size], self.W[oc, ic, ::-1])\n",
        "\n",
        "        # Updating weights and biases AFTER calculating dx\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test case from Problem 4\n",
        "    x = np.array(\n",
        "        [[1, 2, 3, 4], [2, 3, 4, 5]]\n",
        "    )  # shape (2, 4), (number of input channels, number of features).\n",
        "    w = np.ones((3, 2, 3))  # (out_channels, in_channels, filter_size).\n",
        "    b = np.array([1, 2, 3])  # (out_channels,)\n",
        "    delta_a = np.ones((3, 2))  # (out_channels, N_out)\n",
        "\n",
        "    # Expected forward propagation output\n",
        "    expected_a = np.array([[16, 22], [17, 23], [18, 24]])\n",
        "\n",
        "    # Backpropagation gradients\n",
        "    expected_dx = np.array([[3., 6., 6., 3.],\n",
        "                            [3., 6., 6., 3.]])\n",
        "    expected_dW = np.array([[[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]]])\n",
        "    expected_db = np.array([2., 2., 2.])\n",
        "\n",
        "    # Creating layer instance and manually set weights/bias for testing\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    conv_layer = Conv1d(W_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        optimizer=optimizer,\n",
        "                        filter_size=3,\n",
        "                        in_channels=2,\n",
        "                        out_channels=3)\n",
        "\n",
        "    conv_layer.W = w.astype(np.float64)\n",
        "    conv_layer.b = b.astype(np.float64)\n",
        "\n",
        "    # Forward propagation test\n",
        "    output_a = conv_layer.forward(x)\n",
        "    assert np.allclose(output_a, expected_a), f\"Forward prop failed: Expected\\n{expected_a}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation test passed!\")\n",
        "\n",
        "    # Backward propagation test\n",
        "    output_dx = conv_layer.backward(delta_a)\n",
        "    assert np.allclose(conv_layer.dW, expected_dW), f\"Backward prop (dW) failed: Expected\\n{expected_dW}, but got\\n{conv_layer.dW}\"\n",
        "    assert np.allclose(conv_layer.db, expected_db), f\"Backward prop (db) failed: Expected\\n{expected_db}, but got\\n{conv_layer.db}\"\n",
        "    assert np.allclose(output_dx, expected_dx), f\"Backward prop (dx) failed: Expected\\n{expected_dx}, but got\\n{output_dx}\"\n",
        "    print(\"Backward propagation test passed!\")\n",
        "    print(\"All tests for Conv1d passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RhQKAqauIfh",
        "outputId": "3ab69f81-d774-42ee-d9cb-ab06ad3225fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation test passed!\n",
            "Backward propagation test passed!\n",
            "All tests for Conv1d passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Implementing padding**"
      ],
      "metadata": {
        "id": "DkkFc5WcwlK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class Conv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer that supports multiple channels and padding.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    in_channels : int\n",
        "        The number of input channels.\n",
        "    out_channels : int\n",
        "        The number of output channels.\n",
        "    padding : int\n",
        "        The amount of zero-padding to add to the input. Default is 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size, in_channels, out_channels, padding=0):\n",
        "        self.filter_size = filter_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.padding = padding\n",
        "\n",
        "        # Initializing weights with shape (out_channels, in_channels, filter_size)\n",
        "        self.W_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.W = self.W_initializer.W().reshape(out_channels, in_channels, filter_size).astype(np.float64)\n",
        "\n",
        "        # Initializing biases with shape (out_channels,)\n",
        "        self.b_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.b = self.b_initializer.b().astype(np.float64)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (in_channels, N_in).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (out_channels, N_out).\n",
        "        \"\"\"\n",
        "        self.x = x.astype(np.float64)\n",
        "        N_in = self.x.shape[1]\n",
        "\n",
        "        # Padding the input array\n",
        "        x_padded = np.pad(self.x, ((0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "\n",
        "        # Calculating output size\n",
        "        self.out_size = calculate_output_size(N_in=N_in, P=self.padding, F=self.filter_size, S=1)\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros((self.out_channels, self.out_size))\n",
        "\n",
        "        # Performing convolution for each output channel\n",
        "        for oc in range(self.out_channels):\n",
        "            for ic in range(self.in_channels):\n",
        "                for i in range(self.out_size):\n",
        "                    a[oc, i] += np.dot(x_padded[ic, i : i + self.filter_size], self.W[oc, ic, :])\n",
        "            a[oc, :] += self.b[oc]\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (out_channels, N_out).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (in_channels, N_in).\n",
        "        \"\"\"\n",
        "        N_in = self.x.shape[1]\n",
        "\n",
        "        # Calculating gradients for weights and bias\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.sum(da, axis=1) # Sum gradients across features for each output channel\n",
        "\n",
        "        # Calculating dW\n",
        "        x_padded = np.pad(self.x, ((0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "        for oc in range(self.out_channels):\n",
        "            for ic in range(self.in_channels):\n",
        "                for s in range(self.filter_size):\n",
        "                    self.dW[oc, ic, s] = np.sum(da[oc, :] * x_padded[ic, s:s + self.out_size])\n",
        "\n",
        "        # Calculating dx\n",
        "        dx_padded = np.zeros(x_padded.shape)\n",
        "        da_padded = np.pad(da, ((0, 0), (self.filter_size - 1, self.filter_size - 1)), 'constant', constant_values=0)\n",
        "\n",
        "        for ic in range(self.in_channels):\n",
        "            for oc in range(self.out_channels):\n",
        "                for j in range(x_padded.shape[1]):\n",
        "                    dx_padded[ic, j] += np.dot(da_padded[oc, j : j + self.filter_size], self.W[oc, ic, ::-1])\n",
        "\n",
        "        # Removing padding from dx\n",
        "        dx = dx_padded[:, self.padding : N_in + self.padding]\n",
        "\n",
        "        # Updating weights and biases AFTER calculating dx\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test case with padding\n",
        "    x_test = np.array([[1, 2, 3], [4, 5, 6]]) # 2 input channels, 3 features\n",
        "    w_test = np.array([[[1, 1], [1, 1]], [[2, 2], [2, 2]]]) # 2 output channels, 2 input channels, 2 filter size\n",
        "    b_test = np.array([1, 2])\n",
        "    padding_val = 1\n",
        "\n",
        "    # Corrected expected values for padding test case\n",
        "    expected_a_padded = np.array([[6, 13, 17, 10], [12, 26, 34, 20]])\n",
        "    expected_db_padded = np.array([4., 4.])\n",
        "    expected_dW_padded = np.array([[[6., 6.], [15., 15.]], [[6., 6.], [15., 15.]]])\n",
        "    expected_dx = np.array([[6., 6., 6.], [6., 6., 6.]])\n",
        "\n",
        "    # Creating layer instance and manually set weights/bias for testing\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    conv_layer = Conv1d(W_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        optimizer=optimizer,\n",
        "                        filter_size=2,\n",
        "                        in_channels=2,\n",
        "                        out_channels=2,\n",
        "                        padding=padding_val)\n",
        "\n",
        "    conv_layer.W = w_test.astype(np.float64)\n",
        "    conv_layer.b = b_test.astype(np.float64)\n",
        "\n",
        "    # Forward propagation test with padding\n",
        "    output_a = conv_layer.forward(x_test)\n",
        "    assert np.allclose(output_a, expected_a_padded), f\"Forward prop with padding failed: Expected\\n{expected_a_padded}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation with padding test passed!\")\n",
        "\n",
        "    # Backward propagation test with padding\n",
        "    da_padded = np.ones((2, 4))\n",
        "    output_dx = conv_layer.backward(da_padded)\n",
        "\n",
        "    assert np.allclose(conv_layer.dW, expected_dW_padded), f\"Backward prop (dW) with padding failed: Expected\\n{expected_dW_padded}, but got\\n{conv_layer.dW}\"\n",
        "    assert np.allclose(conv_layer.db, expected_db_padded), f\"Backward prop (db) with padding failed: Expected\\n{expected_db_padded}, but got\\n{conv_layer.db}\"\n",
        "    assert np.allclose(output_dx, expected_dx), f\"Backward prop (dx) with padding failed: Expected\\n{expected_dx}, but got\\n{output_dx}\"\n",
        "    print(\"Backward propagation with padding test passed!\")\n",
        "    print(\"All tests for Conv1d with padding passed!\")\n",
        "\n",
        "    # Test case from Problem 4 (no padding)\n",
        "    x = np.array(\n",
        "        [[1, 2, 3, 4], [2, 3, 4, 5]]\n",
        "    )\n",
        "    w = np.ones((3, 2, 3))\n",
        "    b = np.array([1, 2, 3])\n",
        "    delta_a = np.ones((3, 2))\n",
        "\n",
        "    # Expected values\n",
        "    expected_a = np.array([[16, 22], [17, 23], [18, 24]])\n",
        "    expected_dx = np.array([[3., 6., 6., 3.],\n",
        "                            [3., 6., 6., 3.]])\n",
        "    expected_dW = np.array([[[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]]])\n",
        "    expected_db = np.array([2., 2., 2.])\n",
        "\n",
        "    conv_layer_nopad = Conv1d(W_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        optimizer=AdaGrad(lr=0.01),\n",
        "                        filter_size=3,\n",
        "                        in_channels=2,\n",
        "                        out_channels=3,\n",
        "                        padding=0)\n",
        "\n",
        "    conv_layer_nopad.W = w.astype(np.float64)\n",
        "    conv_layer_nopad.b = b.astype(np.float64)\n",
        "\n",
        "    output_a = conv_layer_nopad.forward(x)\n",
        "    assert np.allclose(output_a, expected_a), f\"Forward prop failed: Expected\\n{expected_a}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation test (no padding) passed!\")\n",
        "\n",
        "    output_dx = conv_layer_nopad.backward(delta_a)\n",
        "    assert np.allclose(conv_layer_nopad.dW, expected_dW), f\"Backward prop (dW) failed: Expected\\n{expected_dW}, but got\\n{conv_layer_nopad.dW}\"\n",
        "    assert np.allclose(conv_layer_nopad.db, expected_db), f\"Backward prop (db) failed: Expected\\n{expected_db}, but got\\n{conv_layer_nopad.db}\"\n",
        "    assert np.allclose(output_dx, expected_dx), f\"Backward prop (dx) failed: Expected\\n{expected_dx}, but got\\n{output_dx}\"\n",
        "    print(\"Backward propagation test (no padding) passed!\")\n",
        "    print(\"All tests passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rog48JuwtK-",
        "outputId": "56b5064a-38ce-4834-d2ce-a2236597bc84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation with padding test passed!\n",
            "Backward propagation with padding test passed!\n",
            "All tests for Conv1d with padding passed!\n",
            "Forward propagation test (no padding) passed!\n",
            "Backward propagation test (no padding) passed!\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Response to mini batch**"
      ],
      "metadata": {
        "id": "y61wj8FSyyTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class Conv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer that supports multiple channels, padding, and mini-batches.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    in_channels : int\n",
        "        The number of input channels.\n",
        "    out_channels : int\n",
        "        The number of output channels.\n",
        "    padding : int\n",
        "        The amount of zero-padding to add to the input. Default is 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size, in_channels, out_channels, padding=0):\n",
        "        self.filter_size = filter_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.padding = padding\n",
        "\n",
        "        # Initializing weights with shape (out_channels, in_channels, filter_size)\n",
        "        self.W_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.W = self.W_initializer.W().reshape(out_channels, in_channels, filter_size).astype(np.float64)\n",
        "\n",
        "        # Initializing biases with shape (out_channels,)\n",
        "        self.b_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.b = self.b_initializer.b().astype(np.float64)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (batch_size, in_channels, N_in).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (batch_size, out_channels, N_out).\n",
        "        \"\"\"\n",
        "        self.x = x.astype(np.float64)\n",
        "        batch_size, in_channels, N_in = self.x.shape\n",
        "\n",
        "        # Padding the input array\n",
        "        x_padded = np.pad(self.x, ((0, 0), (0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "\n",
        "        # Calculating output size\n",
        "        self.out_size = calculate_output_size(N_in=N_in, P=self.padding, F=self.filter_size, S=1)\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros((batch_size, self.out_channels, self.out_size))\n",
        "\n",
        "        # Performing convolution for each output channel and each sample in the batch\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(self.out_channels):\n",
        "                for ic in range(self.in_channels):\n",
        "                    for i in range(self.out_size):\n",
        "                        a[b, oc, i] += np.dot(x_padded[b, ic, i : i + self.filter_size], self.W[oc, ic, :])\n",
        "                a[b, oc, :] += self.b[oc]\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (batch_size, out_channels, N_out).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (batch_size, in_channels, N_in).\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, N_in = self.x.shape\n",
        "\n",
        "        # Initializing gradients for weights and bias, summing across the batch\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.sum(da, axis=(0, 2)) # Sum gradients across batch and features\n",
        "\n",
        "        # Calculating dW\n",
        "        x_padded = np.pad(self.x, ((0, 0), (0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(self.out_channels):\n",
        "                for ic in range(self.in_channels):\n",
        "                    for s in range(self.filter_size):\n",
        "                        self.dW[oc, ic, s] += np.sum(da[b, oc, :] * x_padded[b, ic, s:s + self.out_size])\n",
        "\n",
        "        # Calculating dx\n",
        "        dx_padded = np.zeros(x_padded.shape)\n",
        "        da_padded = np.pad(da, ((0, 0), (0, 0), (self.filter_size - 1, self.filter_size - 1)), 'constant', constant_values=0)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for ic in range(self.in_channels):\n",
        "                for oc in range(self.out_channels):\n",
        "                    for j in range(x_padded.shape[2]):\n",
        "                        dx_padded[b, ic, j] += np.dot(da_padded[b, oc, j : j + self.filter_size], self.W[oc, ic, ::-1])\n",
        "\n",
        "        # Removing padding from dx\n",
        "        dx = dx_padded[:, :, self.padding : N_in + self.padding]\n",
        "\n",
        "        # Updating weights and biases AFTER calculating dx\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test case with mini-batch and padding\n",
        "    x_test_batch = np.array([[[1, 2, 3], [4, 5, 6]], [[-1, -2, -3], [-4, -5, -6]]]) # (batch_size, in_channels, N_in)\n",
        "    w_test = np.array([[[1, 1], [1, 1]], [[2, 2], [2, 2]]]) # 2 out_channels, 2 in_channels, 2 filter_size\n",
        "    b_test = np.array([1, 2])\n",
        "    padding_val = 1\n",
        "\n",
        "    expected_a_padded_batch = np.array([[[ 6., 13., 17., 10.],\n",
        "                                         [12., 26., 34., 20.]],\n",
        "                                        [[ -4., -11., -15., -8.],\n",
        "                                         [-8., -22., -30., -16.]]])\n",
        "    expected_db_padded_batch = np.array([8., 8.])\n",
        "    expected_dW_padded_batch = np.array([[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]])\n",
        "    # backward dx is independent of input x values' sign.\n",
        "    expected_dx_batch = np.array([[[6., 6., 6.], [6., 6., 6.]], [[6., 6., 6.], [6., 6., 6.]]])\n",
        "\n",
        "    # Create layer instance and manually set weights/bias for testing\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    conv_layer = Conv1d(W_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        optimizer=optimizer,\n",
        "                        filter_size=2,\n",
        "                        in_channels=2,\n",
        "                        out_channels=2,\n",
        "                        padding=padding_val)\n",
        "\n",
        "    conv_layer.W = w_test.astype(np.float64)\n",
        "    conv_layer.b = b_test.astype(np.float64)\n",
        "\n",
        "    # Forward propagation test with mini-batch\n",
        "    output_a = conv_layer.forward(x_test_batch)\n",
        "    assert np.allclose(output_a, expected_a_padded_batch), f\"Forward prop with mini-batch failed: Expected\\n{expected_a_padded_batch}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation with mini-batch test passed!\")\n",
        "\n",
        "    # Backward propagation test with mini-batch\n",
        "    da_padded_batch = np.ones((2, 2, 4))\n",
        "    output_dx_batch = conv_layer.backward(da_padded_batch)\n",
        "\n",
        "    assert np.allclose(conv_layer.dW, expected_dW_padded_batch), f\"Backward prop (dW) with mini-batch failed: Expected\\n{expected_dW_padded_batch}, but got\\n{conv_layer.dW}\"\n",
        "    assert np.allclose(conv_layer.db, expected_db_padded_batch), f\"Backward prop (db) with mini-batch failed: Expected\\n{expected_db_padded_batch}, but got\\n{conv_layer.db}\"\n",
        "    assert np.allclose(output_dx_batch, expected_dx_batch), f\"Backward prop (dx) with mini-batch failed: Expected\\n{expected_dx_batch}, but got\\n{output_dx_batch}\"\n",
        "    print(\"Backward propagation with mini-batch test passed!\")\n",
        "    print(\"All tests for Conv1d with mini-batch and padding passed!\")\n",
        "\n",
        "    # Test case from Problem 4 (no padding and batch size of 1) to ensure backward compatibility\n",
        "    x = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]]]) # added batch dimension\n",
        "    w = np.ones((3, 2, 3))\n",
        "    b = np.array([1, 2, 3])\n",
        "    delta_a = np.ones((1, 3, 2)) # added batch dimension\n",
        "\n",
        "    # Expected values\n",
        "    expected_a = np.array([[[16, 22], [17, 23], [18, 24]]])\n",
        "    expected_dx = np.array([[[3., 6., 6., 3.],\n",
        "                            [3., 6., 6., 3.]]])\n",
        "    expected_dW = np.array([[[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]],\n",
        "                            [[3., 5., 7.],\n",
        "                             [5., 7., 9.]]])\n",
        "    expected_db = np.array([2., 2., 2.])\n",
        "\n",
        "    conv_layer_nopad = Conv1d(W_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 3, out_features=3),\n",
        "                        optimizer=AdaGrad(lr=0.01),\n",
        "                        filter_size=3,\n",
        "                        in_channels=2,\n",
        "                        out_channels=3,\n",
        "                        padding=0)\n",
        "\n",
        "    conv_layer_nopad.W = w.astype(np.float64)\n",
        "    conv_layer_nopad.b = b.astype(np.float64)\n",
        "\n",
        "    output_a = conv_layer_nopad.forward(x)\n",
        "    assert np.allclose(output_a, expected_a), f\"Forward prop failed: Expected\\n{expected_a}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation test (batch size 1) passed!\")\n",
        "\n",
        "    output_dx = conv_layer_nopad.backward(delta_a)\n",
        "    assert np.allclose(conv_layer_nopad.dW, expected_dW), f\"Backward prop (dW) failed: Expected\\n{expected_dW}, but got\\n{conv_layer_nopad.dW}\"\n",
        "    assert np.allclose(conv_layer_nopad.db, expected_db), f\"Backward prop (db) failed: Expected\\n{expected_db}, but got\\n{conv_layer_nopad.db}\"\n",
        "    assert np.allclose(output_dx, expected_dx), f\"Backward prop (dx) failed: Expected\\n{expected_dx}, but got\\n{output_dx}\"\n",
        "    print(\"Backward propagation test (batch size 1) passed!\")\n",
        "    print(\"All tests passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk4qCqhWy4L4",
        "outputId": "15426028-080f-4849-a963-03a04c97bd69"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation with mini-batch test passed!\n",
            "Backward propagation with mini-batch test passed!\n",
            "All tests for Conv1d with mini-batch and padding passed!\n",
            "Forward propagation test (batch size 1) passed!\n",
            "Backward propagation test (batch size 1) passed!\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Arbitrary number of strides**"
      ],
      "metadata": {
        "id": "2VhkMYTl00_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to calculate the output size after 1D convolution\n",
        "def calculate_output_size(N_in, P, F, S):\n",
        "    \"\"\"\n",
        "    Calculates the output size of a 1D convolutional layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N_in : int\n",
        "        Input size (number of features).\n",
        "    P : int\n",
        "        Number of paddings in one direction.\n",
        "    F : int\n",
        "        Filter size.\n",
        "    S : int\n",
        "        Stride size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Output size (number of features).\n",
        "    \"\"\"\n",
        "    return math.floor((N_in + 2 * P - F) / S) + 1\n",
        "\n",
        "\n",
        "# A simple AdaGrad optimizer class to update weights and biases\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h_w = 1e-4  # Epsilon for numerical stability\n",
        "        self.h_b = 1e-4\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases of a layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : object\n",
        "            The layer object to be updated.\n",
        "        \"\"\"\n",
        "        self.h_w += layer.dW ** 2\n",
        "        self.h_b += layer.db ** 2\n",
        "        layer.W -= self.lr * layer.dW / np.sqrt(self.h_w)\n",
        "        layer.b -= self.lr * layer.db / np.sqrt(self.h_b)\n",
        "\n",
        "\n",
        "# A simple Xavier Initializer class\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initializer for weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def W(self):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier method.\n",
        "        \"\"\"\n",
        "        return np.random.randn(self.out_features, self.in_features) / np.sqrt(self.in_features)\n",
        "\n",
        "    def b(self):\n",
        "        \"\"\"\n",
        "        Initialize biases to zeros.\n",
        "        \"\"\"\n",
        "        return np.zeros(self.out_features)\n",
        "\n",
        "\n",
        "class Conv1d:\n",
        "    \"\"\"\n",
        "    A 1D convolutional layer that supports multiple channels, padding, and mini-batches.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W_initializer : object\n",
        "        Instance of a weight initializer class.\n",
        "    b_initializer : object\n",
        "        Instance of a bias initializer class.\n",
        "    optimizer : object\n",
        "        Instance of an optimizer class.\n",
        "    filter_size : int\n",
        "        The size of the convolutional filter.\n",
        "    in_channels : int\n",
        "        The number of input channels.\n",
        "    out_channels : int\n",
        "        The number of output channels.\n",
        "    padding : int\n",
        "        The amount of zero-padding to add to the input. Default is 0.\n",
        "    stride : int\n",
        "        The step size of the convolution. Default is 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, W_initializer, b_initializer, optimizer, filter_size, in_channels, out_channels, padding=0, stride=1):\n",
        "        self.filter_size = filter_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "        # Initializing weights with shape (out_channels, in_channels, filter_size)\n",
        "        self.W_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.W = self.W_initializer.W().reshape(out_channels, in_channels, filter_size).astype(np.float64)\n",
        "\n",
        "        # Initializing biases with shape (out_channels,)\n",
        "        self.b_initializer = XavierInitializer(in_features=in_channels * filter_size, out_features=out_channels)\n",
        "        self.b = self.b_initializer.b().astype(np.float64)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.x = None  # To store the input for backpropagation\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.out_size = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray\n",
        "            Input array of shape (batch_size, in_channels, N_in).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Output array of shape (batch_size, out_channels, N_out).\n",
        "        \"\"\"\n",
        "        self.x = x.astype(np.float64)\n",
        "        batch_size, in_channels, N_in = self.x.shape\n",
        "\n",
        "        # Padding the input array\n",
        "        x_padded = np.pad(self.x, ((0, 0), (0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "\n",
        "        # Calculating output size\n",
        "        self.out_size = calculate_output_size(N_in=N_in, P=self.padding, F=self.filter_size, S=self.stride)\n",
        "\n",
        "        # Initializing output array\n",
        "        a = np.zeros((batch_size, self.out_channels, self.out_size))\n",
        "\n",
        "        # Performing convolution for each output channel and each sample in the batch\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(self.out_channels):\n",
        "                for ic in range(self.in_channels):\n",
        "                    for i in range(self.out_size):\n",
        "                        # Using the stride to slice the padded input\n",
        "                        start = i * self.stride\n",
        "                        end = start + self.filter_size\n",
        "                        a[b, oc, i] += np.dot(x_padded[b, ic, start : end], self.W[oc, ic, :])\n",
        "                a[b, oc, :] += self.b[oc]\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        \"\"\"\n",
        "        Backward propagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        da : numpy.ndarray\n",
        "            Gradient array passed from the next layer, shape (batch_size, out_channels, N_out).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            Gradient to pass to the previous layer, shape (batch_size, in_channels, N_in).\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, N_in = self.x.shape\n",
        "\n",
        "        # Initializing gradients for weights and bias, summing across the batch\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.sum(da, axis=(0, 2)) # Sum gradients across batch and features\n",
        "\n",
        "        # Calculating dW\n",
        "        x_padded = np.pad(self.x, ((0, 0), (0, 0), (self.padding, self.padding)), 'constant', constant_values=0)\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(self.out_channels):\n",
        "                for ic in range(self.in_channels):\n",
        "                    for i in range(self.out_size):\n",
        "                        start = i * self.stride\n",
        "                        end = start + self.filter_size\n",
        "                        self.dW[oc, ic, :] += da[b, oc, i] * x_padded[b, ic, start : end]\n",
        "\n",
        "        dx_padded = np.zeros(x_padded.shape)\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(self.out_channels):\n",
        "                for ic in range(self.in_channels):\n",
        "                    for i in range(self.out_size):\n",
        "                        start = i * self.stride\n",
        "                        end = start + self.filter_size\n",
        "                        # The gradient is scattered back to the input space at strided intervals\n",
        "                        # and multiplied by the flipped weights.\n",
        "                        dx_padded[b, ic, start:end] += da[b, oc, i] * self.W[oc, ic, ::-1]\n",
        "\n",
        "        # Removing padding from dx\n",
        "        dx = dx_padded[:, :, self.padding : N_in + self.padding]\n",
        "\n",
        "        # Updating weights and biases AFTER calculating dx\n",
        "        self.optimizer.update(self)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test case with mini-batch and padding\n",
        "    x_test_batch = np.array([[[1, 2, 3], [4, 5, 6]], [[-1, -2, -3], [-4, -5, -6]]]) # (batch_size, in_channels, N_in)\n",
        "    w_test = np.array([[[1, 1], [1, 1]], [[2, 2], [2, 2]]]) # 2 out_channels, 2 in_channels, 2 filter_size\n",
        "    b_test = np.array([1, 2])\n",
        "    padding_val = 1\n",
        "    stride_val = 1\n",
        "\n",
        "    expected_a_padded_batch = np.array([[[ 6., 13., 17., 10.],\n",
        "                                         [12., 26., 34., 20.]],\n",
        "                                        [[ -4., -11., -15., -8.],\n",
        "                                         [-8., -22., -30., -16.]]])\n",
        "    expected_db_padded_batch = np.array([8., 8.])\n",
        "    expected_dW_padded_batch = np.array([[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]])\n",
        "    expected_dx_batch = np.array([[[6., 6., 6.], [6., 6., 6.]], [[6., 6., 6.], [6., 6., 6.]]])\n",
        "\n",
        "    # Creating layer instance and manually set weights/bias for testing\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    conv_layer = Conv1d(W_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        b_initializer=XavierInitializer(in_features=2 * 2, out_features=2),\n",
        "                        optimizer=optimizer,\n",
        "                        filter_size=2,\n",
        "                        in_channels=2,\n",
        "                        out_channels=2,\n",
        "                        padding=padding_val,\n",
        "                        stride=stride_val)\n",
        "\n",
        "    conv_layer.W = w_test.astype(np.float64)\n",
        "    conv_layer.b = b_test.astype(np.float64)\n",
        "\n",
        "    # Forward propagation test with mini-batch\n",
        "    output_a = conv_layer.forward(x_test_batch)\n",
        "    assert np.allclose(output_a, expected_a_padded_batch), f\"Forward prop with mini-batch failed: Expected\\n{expected_a_padded_batch}, but got\\n{output_a}\"\n",
        "    print(\"Forward propagation with mini-batch test passed!\")\n",
        "\n",
        "    # Backward propagation test with mini-batch\n",
        "    da_padded_batch = np.ones((2, 2, 4))\n",
        "    output_dx_batch = conv_layer.backward(da_padded_batch)\n",
        "\n",
        "    assert np.allclose(conv_layer.dW, expected_dW_padded_batch), f\"Backward prop (dW) with mini-batch failed: Expected\\n{expected_dW_padded_batch}, but got\\n{conv_layer.dW}\"\n",
        "    assert np.allclose(conv_layer.db, expected_db_padded_batch), f\"Backward prop (db) with mini-batch failed: Expected\\n{expected_db_padded_batch}, but got\\n{conv_layer.db}\"\n",
        "    assert np.allclose(output_dx_batch, expected_dx_batch), f\"Backward prop (dx) with mini-batch failed: Expected\\n{expected_dx_batch}, but got\\n{output_dx_batch}\"\n",
        "    print(\"Backward propagation with mini-batch test passed!\")\n",
        "    print(\"All tests for Conv1d with mini-batch and padding passed!\")\n",
        "\n",
        "    # New Test case for arbitrary stride (S=2)\n",
        "    x_stride_test = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]]]) # added batch dimension\n",
        "    w_stride_test = np.ones((1, 2, 2))\n",
        "    b_stride_test = np.array([1])\n",
        "    da_stride_test = np.ones((1, 1, 2))\n",
        "\n",
        "    # Expected values for stride=2\n",
        "    expected_a_stride = np.array([[[9., 17.]]])\n",
        "    expected_dW_stride = np.array([[[4., 6.], [6., 8.]]])\n",
        "    expected_db_stride = np.array([2.])\n",
        "    expected_dx_stride = np.array([[[1., 1., 1., 1.], [1., 1., 1., 1.]]])\n",
        "\n",
        "    conv_layer_stride = Conv1d(W_initializer=XavierInitializer(in_features=2*2, out_features=1),\n",
        "                            b_initializer=XavierInitializer(in_features=2*2, out_features=1),\n",
        "                            optimizer=AdaGrad(lr=0.01),\n",
        "                            filter_size=2,\n",
        "                            in_channels=2,\n",
        "                            out_channels=1,\n",
        "                            padding=0,\n",
        "                            stride=2)\n",
        "\n",
        "    conv_layer_stride.W = w_stride_test.astype(np.float64)\n",
        "    conv_layer_stride.b = b_stride_test.astype(np.float64)\n",
        "\n",
        "    output_a_stride = conv_layer_stride.forward(x_stride_test)\n",
        "    assert np.allclose(output_a_stride, expected_a_stride), f\"Forward prop with stride failed: Expected\\n{expected_a_stride}, but got\\n{output_a_stride}\"\n",
        "    print(\"Forward propagation with stride test passed!\")\n",
        "\n",
        "    output_dx_stride = conv_layer_stride.backward(da_stride_test)\n",
        "    assert np.allclose(conv_layer_stride.dW, expected_dW_stride), f\"Backward prop (dW) with stride failed: Expected\\n{expected_dW_stride}, but got\\n{conv_layer_stride.dW}\"\n",
        "    assert np.allclose(conv_layer_stride.db, expected_db_stride), f\"Backward prop (db) with stride failed: Expected\\n{expected_db_stride}, but got\\n{conv_layer_stride.db}\"\n",
        "    assert np.allclose(output_dx_stride, expected_dx_stride), f\"Backward prop (dx) with stride failed: Expected\\n{expected_dx_stride}, but got\\n{output_dx_stride}\"\n",
        "    print(\"Backward propagation with stride test passed!\")\n",
        "    print(\"All tests for Conv1d with stride, mini-batch, and padding passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6nyyEQM1BDJ",
        "outputId": "f527108b-6019-4bb0-9550-d477d9d8a31d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation with mini-batch test passed!\n",
            "Backward propagation with mini-batch test passed!\n",
            "All tests for Conv1d with mini-batch and padding passed!\n",
            "Forward propagation with stride test passed!\n",
            "Backward propagation with stride test passed!\n",
            "All tests for Conv1d with stride, mini-batch, and padding passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Learning and estimation**"
      ],
      "metadata": {
        "id": "n6WdRC4I2qgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Conv1d:\n",
        "    def __init__(self, in_c, out_c, kernel_size, stride=1, pad=0):\n",
        "        self.in_c, self.out_c, self.k_size, self.stride, self.pad = in_c, out_c, kernel_size, stride, pad\n",
        "        scale = np.sqrt(2.0 / (in_c * kernel_size))\n",
        "        self.W = np.random.randn(out_c, in_c, kernel_size) * scale\n",
        "        self.b = np.zeros(out_c)\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, n_features = x.shape\n",
        "        padded_x = np.pad(x, ((0,0),(0,0),(self.pad,self.pad)), 'constant')\n",
        "        out_len = (n_features + 2*self.pad - self.k_size) // self.stride + 1\n",
        "        output = np.zeros((batch_size, self.out_c, out_len))\n",
        "\n",
        "        windows = []\n",
        "        for i in range(0, out_len):\n",
        "            start = i * self.stride\n",
        "            window = padded_x[:, :, start:start+self.k_size]\n",
        "            windows.append((i, start, window))\n",
        "\n",
        "        for i, start, window in windows:\n",
        "            for k in range(self.out_c):\n",
        "                output[:, k, i] = np.sum(window * self.W[k], axis=(1,2)) + self.b[k]\n",
        "\n",
        "        self.cache = (x, padded_x, windows)\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        x, padded_x, windows = self.cache\n",
        "        batch_size, _, n_features = x.shape\n",
        "\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.sum(d_out, axis=(0,2))\n",
        "        dx_padded = np.zeros_like(padded_x)\n",
        "\n",
        "        for i, start, window in windows:\n",
        "            for k in range(self.out_c):\n",
        "                dW[k] += np.sum(window * d_out[:, k, i][:, None, None], axis=0)\n",
        "                dx_padded[:, :, start:start+self.k_size] += d_out[:, k, i][:, None, None] * self.W[k]\n",
        "\n",
        "        dx = dx_padded[:, :, self.pad:self.pad+n_features] if self.pad > 0 else dx_padded\n",
        "        self.dW, self.db = dW, db\n",
        "        return dx\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "# CNN Classifier with integrated components\n",
        "class SimpleCNN:\n",
        "    def __init__(self, lr=0.001):\n",
        "        self.lr = lr\n",
        "        self.conv1 = Conv1d(1, 8, 5, pad=2)\n",
        "        self.conv2 = Conv1d(8, 16, 3, pad=1)\n",
        "\n",
        "        # INTEGRATED FULLYCONNECTED LAYER PARAMETERS\n",
        "        self.fc_W = np.random.randn(16*784, 10) * np.sqrt(2.0/(16*784))\n",
        "        self.fc_b = np.zeros(10)\n",
        "        self.fc_cache = None\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CONV1 + INTEGRATED ReLU\n",
        "        x = self.conv1.forward(x)\n",
        "        x = np.maximum(0, x)  # INTEGRATED ReLU ACTIVATION\n",
        "\n",
        "        # CONV2 + INTEGRATED ReLU\n",
        "        x = self.conv2.forward(x)\n",
        "        x = np.maximum(0, x)  # INTEGRATED ReLU ACTIVATION\n",
        "\n",
        "        # INTEGRATED FULLYCONNECTED LAYER FORWARD PASS\n",
        "        x_flat = x.reshape(x.shape[0], -1)\n",
        "        self.fc_cache = x_flat\n",
        "        logits = np.dot(x_flat, self.fc_W) + self.fc_b\n",
        "\n",
        "        # INTEGRATED SOFTMAX ACTIVATION\n",
        "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        # INTEGRATED SOFTMAX BACKWARD\n",
        "        x_flat = self.fc_cache\n",
        "        batch_size = x_flat.shape[0]\n",
        "\n",
        "        # Calculating gradients for FC layer\n",
        "        d_fc_W = np.dot(x_flat.T, d_out) / batch_size\n",
        "        d_fc_b = np.sum(d_out, axis=0) / batch_size\n",
        "\n",
        "        # Backpropagate through FC layer\n",
        "        d_conv_out = np.dot(d_out, self.fc_W.T)\n",
        "        d_conv_out = d_conv_out.reshape(-1, 16, 784)\n",
        "\n",
        "        # Updating FC parameters\n",
        "        self.fc_W -= self.lr * d_fc_W\n",
        "        self.fc_b -= self.lr * d_fc_b\n",
        "\n",
        "        # Backpropagate through convolutional layers with INTEGRATED ReLU\n",
        "        d_conv_out[d_conv_out < 0] = 0  # ReLU gradient\n",
        "        d_conv_out = self.conv2.backward(d_conv_out)\n",
        "\n",
        "        d_conv_out[d_conv_out < 0] = 0  # ReLU gradient\n",
        "        d_conv_out = self.conv1.backward(d_conv_out)\n",
        "\n",
        "        return d_conv_out\n",
        "\n",
        "    def fit(self, X, y, epochs=3, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                # Forward pass (includes INTEGRATED ReLU, FullyConnected, and Softmax)\n",
        "                probs = self.forward(X_batch)\n",
        "\n",
        "                # Calculating loss gradient\n",
        "                d_out = (probs - y_batch) / batch_size\n",
        "\n",
        "                # Backward pass (includes INTEGRATED components)\n",
        "                self.backward(d_out)\n",
        "\n",
        "                # Updating convolutional layers\n",
        "                for layer in self.layers:\n",
        "                    layer.update(self.lr)\n",
        "\n",
        "            # Calculating accuracy\n",
        "            preds = np.argmax(self.forward(X), axis=1)\n",
        "            true_labels = np.argmax(y, axis=1)\n",
        "            acc = accuracy_score(true_labels, preds)\n",
        "            print(f\"Epoch {epoch+1}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== INTEGRATION DEMONSTRATION ===\")\n",
        "    print(\"Components integrated:\")\n",
        "    print(\"1. ReLU activation (inline np.maximum(0, x))\")\n",
        "    print(\"2. FullyConnected layer (integrated weights and matrix multiplication)\")\n",
        "    print(\"3. Softmax activation (inline computation)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Loading and preparing data\n",
        "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='pandas')\n",
        "    X, y = X[:2000].astype('float32'), y[:2000].astype('int32')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fitting the scaler on training data first, then transforming both\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train).reshape(-1, 1, 784)  # Fit and transform training\n",
        "    X_test = scaler.transform(X_test).reshape(-1, 1, 784)        # Transform test using fitted scaler\n",
        "\n",
        "    # One-hot encode\n",
        "    y_train_oh = np.eye(10)[y_train]\n",
        "    y_test_oh = np.eye(10)[y_test]\n",
        "\n",
        "    # Training and evaluation\n",
        "    model = SimpleCNN(lr=0.001)\n",
        "    model.fit(X_train, y_train_oh, epochs=3)\n",
        "\n",
        "    test_preds = np.argmax(model.forward(X_test), axis=1)\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMsrvv-9-ZRw",
        "outputId": "d5961b40-4cee-4ac4-a8eb-e264717be77e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INTEGRATION DEMONSTRATION ===\n",
            "Components integrated:\n",
            "1. ReLU activation (inline np.maximum(0, x))\n",
            "2. FullyConnected layer (integrated weights and matrix multiplication)\n",
            "3. Softmax activation (inline computation)\n",
            "========================================\n",
            "Epoch 1, Accuracy: 0.1194\n",
            "Epoch 2, Accuracy: 0.1106\n",
            "Epoch 3, Accuracy: 0.1200\n",
            "\n",
            "Final Test Accuracy: 0.1150\n"
          ]
        }
      ]
    }
  ]
}