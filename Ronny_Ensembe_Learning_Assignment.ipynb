{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFVokXMfavBbFI8zyNEG9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_Ensembe_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Blending Scratch Mounting**"
      ],
      "metadata": {
        "id": "fqrKdlYw1rra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teHJE4EGvBrB",
        "outputId": "13d90cf1-a472-4740-a72b-2830271ff02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully loaded and split into training and validation sets.\n",
            "Training data shape: (1168, 2)\n",
            "Validation data shape: (292, 2)\n",
            "\n",
            "Single Model MSEs (for comparison):\n",
            "Linear Regression MSE: 2495554898.67\n",
            "Support Vector Regressor MSE: 2682656031.23\n",
            "Decision Tree Regressor MSE: 2184045784.67\n",
            "\n",
            "--- Blending Results ---\n",
            "Blended Model MSE: 1991209925.52\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Selecting the features and target\n",
        "X = df[['GrLivArea', 'YearBuilt']]\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Splitting the data into 80% training and 20% validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data successfully loaded and split into training and validation sets.\")\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "# Training and Evaluating single models\n",
        "# Initializing and training the single models\n",
        "# Model 1: Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Model 2: Support Vector Regressor (SVR)\n",
        "# We use a linear kernel and a reasonable C value for this example\n",
        "svr_model = SVR(kernel='linear', C=100)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Model 3: Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the validation data\n",
        "lr_preds = lr_model.predict(X_val)\n",
        "svr_preds = svr_model.predict(X_val)\n",
        "dt_preds = dt_model.predict(X_val)\n",
        "\n",
        "# Calculating and printing the MSE for each single model\n",
        "lr_mse = mean_squared_error(y_val, lr_preds)\n",
        "svr_mse = mean_squared_error(y_val, svr_preds)\n",
        "dt_mse = mean_squared_error(y_val, dt_preds)\n",
        "\n",
        "print(\"\\nSingle Model MSEs (for comparison):\")\n",
        "print(f\"Linear Regression MSE: {lr_mse:.2f}\")\n",
        "print(f\"Support Vector Regressor MSE: {svr_mse:.2f}\")\n",
        "print(f\"Decision Tree Regressor MSE: {dt_mse:.2f}\")\n",
        "\n",
        "# Blending Scratch Implementation\n",
        "# Blending scratch implementation: average the predictions of the three models\n",
        "blended_preds = (lr_preds + svr_preds + dt_preds) / 3\n",
        "\n",
        "# Calculating the MSE for the blended model\n",
        "blended_mse = mean_squared_error(y_val, blended_preds)\n",
        "\n",
        "print(\"\\n--- Blending Results ---\")\n",
        "print(f\"Blended Model MSE: {blended_mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Scratch mounting of bagging**"
      ],
      "metadata": {
        "id": "oxFLoySy5p7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Selecting the features and target\n",
        "X = df[['GrLivArea', 'YearBuilt']]\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Splitting the data into 80% training and 20% validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data successfully loaded and split.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Training a single Decision Tree Regressor\n",
        "dt_model_single = DecisionTreeRegressor(random_state=42)\n",
        "dt_model_single.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions and calculating MSE on the validation set\n",
        "dt_preds_single = dt_model_single.predict(X_val)\n",
        "dt_mse_single = mean_squared_error(y_val, dt_preds_single)\n",
        "\n",
        "print(\"Single Model (Decision Tree) Performance:\")\n",
        "print(f\"MSE: {dt_mse_single:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Bagging implementation\n",
        "n_models = 10\n",
        "bagged_predictions = []\n",
        "\n",
        "# Getting the number of samples in the training set\n",
        "n_samples_train = X_train.shape[0]\n",
        "\n",
        "# Converting dataframes to numpy arrays for easier indexing\n",
        "X_train_np = X_train.to_numpy()\n",
        "y_train_np = y_train.to_numpy()\n",
        "\n",
        "for i in range(n_models):\n",
        "    # Creating a bootstrap sample by random selection with replacement\n",
        "    bootstrap_indices = np.random.choice(n_samples_train, size=n_samples_train, replace=True)\n",
        "    X_bootstrap = X_train_np[bootstrap_indices]\n",
        "    y_bootstrap = y_train_np[bootstrap_indices]\n",
        "\n",
        "    # Training a new Decision Tree model on the bootstrap sample\n",
        "    bagged_model = DecisionTreeRegressor(random_state=i) # Use a different random_state for diversity\n",
        "    bagged_model.fit(X_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Making predictions on the original validation set\n",
        "    bagged_preds_i = bagged_model.predict(X_val)\n",
        "    bagged_predictions.append(bagged_preds_i)\n",
        "\n",
        "# Converting the list of predictions to a NumPy array for easy averaging\n",
        "bagged_predictions = np.array(bagged_predictions)\n",
        "\n",
        "# Average the predictions across all models\n",
        "final_bagged_preds = np.mean(bagged_predictions, axis=0)\n",
        "\n",
        "# Calculating the MSE for the bagged model\n",
        "bagged_mse = mean_squared_error(y_val, final_bagged_preds)\n",
        "\n",
        "print(\"Bagged Model Performance:\")\n",
        "print(f\"Bagged Model MSE (from {n_models} trees): {bagged_mse:.2f}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKVon_It5wcP",
        "outputId": "d74776ce-014c-42ef-a6d1-02ec8cdb7c01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully loaded and split.\n",
            "------------------------------\n",
            "Single Model (Decision Tree) Performance:\n",
            "MSE: 2184045784.67\n",
            "------------------------------\n",
            "Bagged Model Performance:\n",
            "Bagged Model MSE (from 10 trees): 1841521921.24\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Stacking scratch mounting**"
      ],
      "metadata": {
        "id": "V3ToxV0r-wmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def train_stacked_ensemble(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Scratch implementation of a stacking ensemble for regression.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The training target.\n",
        "        X_val (pd.DataFrame): The validation features.\n",
        "        y_val (pd.Series): The validation target.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the final stacked predictions and the stacked MSE.\n",
        "    \"\"\"\n",
        "    # Stage 0: Training base models and generating blended data (meta-features)\n",
        "    # Using 3-fold cross-validation (K0=3)\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    # Preparing arrays to store the meta-features and the target\n",
        "    blended_data = np.zeros((X_train.shape[0], 2))\n",
        "\n",
        "    # Creating a list to hold the trained base models for later use on validation data\n",
        "    base_models = []\n",
        "\n",
        "    print(\"Starting Stage 0: Training base models on K-Folds...\")\n",
        "    for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "        print(f\"  Training on Fold {i+1}...\")\n",
        "\n",
        "        # Splitting training data into a sub-training set and a hold-out validation set\n",
        "        X_sub_train, X_hold_out = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "        y_sub_train, y_hold_out = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "        # Base Model 1: Linear Regression\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(X_sub_train, y_sub_train)\n",
        "        lr_preds = lr_model.predict(X_hold_out)\n",
        "\n",
        "        # Base Model 2: Decision Tree Regressor\n",
        "        # The same model type is used in each fold to maintain consistency\n",
        "        dt_model = DecisionTreeRegressor(random_state=42)\n",
        "        dt_model.fit(X_sub_train, y_sub_train)\n",
        "        dt_preds = dt_model.predict(X_hold_out)\n",
        "\n",
        "        # Storing predictions in the blended data array\n",
        "        blended_data[val_index, 0] = lr_preds\n",
        "        blended_data[val_index, 1] = dt_preds\n",
        "\n",
        "        # Saving the trained models for later use on the validation data\n",
        "        base_models.append((lr_model, dt_model))\n",
        "        print(f\"  Fold {i+1} complete.\")\n",
        "\n",
        "    print(\"Stage 0 complete. Blended data (meta-features) created.\")\n",
        "    print(f\"Blended data shape: {blended_data.shape}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Stage 1: Training the meta-model on the blended data\n",
        "    # Using a simple Linear Regression as our meta-model\n",
        "    print(\"Starting Stage 1: Training meta-model...\")\n",
        "    meta_model = LinearRegression()\n",
        "    meta_model.fit(blended_data, y_train)\n",
        "    print(\"Stage 1 complete. Meta-model trained.\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Making predictions on the original validation set (X_val)\n",
        "    # Generating \"blend test\" data for the validation set\n",
        "    blend_test_data = np.zeros((X_val.shape[0], 2))\n",
        "\n",
        "    print(\"Generating predictions on validation set...\")\n",
        "    # For each base model from each fold, make a prediction on the entire X_val set\n",
        "    for lr_model, dt_model in base_models:\n",
        "        blend_test_data[:, 0] += lr_model.predict(X_val)\n",
        "        blend_test_data[:, 1] += dt_model.predict(X_val)\n",
        "\n",
        "    # Average the predictions to get the final blend test data\n",
        "    blend_test_data /= kf.get_n_splits()\n",
        "\n",
        "    # Using the meta-model to make the final prediction\n",
        "    final_stacked_preds = meta_model.predict(blend_test_data)\n",
        "\n",
        "    # Calculating the MSE for the stacked model\n",
        "    stacked_mse = mean_squared_error(y_val, final_stacked_preds)\n",
        "\n",
        "    return final_stacked_preds, stacked_mse\n",
        "\n",
        "# Main Script\n",
        "try:\n",
        "    # Loading the dataset\n",
        "    df = pd.read_csv('train.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Initial number of rows: {len(df)}\")\n",
        "\n",
        "    # Explicitly ensure columns are numeric and handle missing values\n",
        "    df['GrLivArea'] = pd.to_numeric(df['GrLivArea'], errors='coerce')\n",
        "    df['YearBuilt'] = pd.to_numeric(df['YearBuilt'], errors='coerce')\n",
        "    df['SalePrice'] = pd.to_numeric(df['SalePrice'], errors='coerce')\n",
        "\n",
        "    # Dropping any rows with missing values\n",
        "    df.dropna(subset=['GrLivArea', 'YearBuilt', 'SalePrice'], inplace=True)\n",
        "\n",
        "    print(f\"Number of rows after cleaning: {len(df)}\")\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(\"\\nError: All rows were removed during the cleaning process.\")\n",
        "        print(\"This indicates that the specified columns have missing or invalid data in every row.\")\n",
        "        # Raise an error to stop execution\n",
        "        raise ValueError(\"Empty DataFrame after cleaning.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Selecting the features and target\n",
        "X = df[['GrLivArea', 'YearBuilt']]\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Splitting the data into 80% training and 20% validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData successfully split for stacking.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Getting baseline MSE for a single model (Decision Tree for comparison)\n",
        "dt_model_single = DecisionTreeRegressor(random_state=42)\n",
        "dt_model_single.fit(X_train, y_train)\n",
        "dt_mse_single = mean_squared_error(y_val, dt_model_single.predict(X_val))\n",
        "\n",
        "# Training and evaluating the stacked ensemble\n",
        "final_stacked_preds, stacked_mse = train_stacked_ensemble(X_train, y_train, X_val, y_val)\n",
        "\n",
        "print(\"Stacking Performance:\")\n",
        "print(f\"Single Model (Decision Tree) MSE: {dt_mse_single:.2f}\")\n",
        "print(f\"Stacked Model MSE: {stacked_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBb4Sj3L-3mF",
        "outputId": "1fc7d4fc-8a88-43c3-c259-0c6a108dd3bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Initial number of rows: 1460\n",
            "Number of rows after cleaning: 1460\n",
            "\n",
            "Data successfully split for stacking.\n",
            "------------------------------\n",
            "Starting Stage 0: Training base models on K-Folds...\n",
            "  Training on Fold 1...\n",
            "  Fold 1 complete.\n",
            "  Training on Fold 2...\n",
            "  Fold 2 complete.\n",
            "  Training on Fold 3...\n",
            "  Fold 3 complete.\n",
            "Stage 0 complete. Blended data (meta-features) created.\n",
            "Blended data shape: (1168, 2)\n",
            "------------------------------\n",
            "Starting Stage 1: Training meta-model...\n",
            "Stage 1 complete. Meta-model trained.\n",
            "------------------------------\n",
            "Generating predictions on validation set...\n",
            "Stacking Performance:\n",
            "Single Model (Decision Tree) MSE: 2184045784.67\n",
            "Stacked Model MSE: 1976573063.77\n"
          ]
        }
      ]
    }
  ]
}