{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGtSYKjk0FJKRgDsNl1Mov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_Deep_Neural_Network_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Classifying fully connected layers**"
      ],
      "metadata": {
        "id": "LXJNn3zfO-hw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iULh8FfXMT89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Classifying the initialization method**"
      ],
      "metadata": {
        "id": "H3P8u-sePllR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ],
      "metadata": {
        "id": "F8XwKLYfPvKB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Classifying optimization methods**"
      ],
      "metadata": {
        "id": "E6bqz6SOQysH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer"
      ],
      "metadata": {
        "id": "ZY8tevzBQ3xk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Classifying activation functions**"
      ],
      "metadata": {
        "id": "eb18CPXCRkMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.mask = (A > 0)\n",
        "        Z = A.copy()\n",
        "        Z[~self.mask] = 0\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = dA.copy()\n",
        "        dZ[~self.mask] = 0\n",
        "        return dZ\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss"
      ],
      "metadata": {
        "id": "-LO6OjJeRpiI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. ReLU class creation**"
      ],
      "metadata": {
        "id": "KY6bwUxzS7qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Storing input for backward pass\n",
        "        self.A = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        # Computing ReLU activation using np.maximum\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # The gradient is 1 where A > 0 and 0 otherwise.\n",
        "        dA = dZ.copy()\n",
        "        dA[self.A <= 0] = 0\n",
        "        return dA\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss"
      ],
      "metadata": {
        "id": "97rbKTnKTEzB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Initial value of weight**"
      ],
      "metadata": {
        "id": "VQmZEzYrUneO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with Xavier method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to Xavier formula\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with He method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to He formula\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Storing input for backward pass\n",
        "        self.A = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        # Computing ReLU activation using np.maximum\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # The gradient is 1 where A > 0 and 0 otherwise.\n",
        "        dA = dZ.copy()\n",
        "        dA[self.A <= 0] = 0\n",
        "        return dA\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss"
      ],
      "metadata": {
        "id": "0YKWMu2kUtn7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Optimization method**"
      ],
      "metadata": {
        "id": "Y1a8HTDCVzC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with Xavier method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to Xavier formula\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with He method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to He formula\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "# Optimization Methods\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimization method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.hW = None  # Sum of squared gradients for weights\n",
        "        self.hB = None  # Sum of squared gradients for biases\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer using AdaGrad.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Initializing hW and hB to zeros with the same shape as the weights/biases\n",
        "        if self.hW is None:\n",
        "            self.hW = np.zeros_like(layer.W)\n",
        "        if self.hB is None:\n",
        "            self.hB = np.zeros_like(layer.B)\n",
        "\n",
        "        # Accumulating the square of the gradients\n",
        "        self.hW += layer.dW ** 2\n",
        "        self.hB += layer.dB ** 2\n",
        "\n",
        "        # Updating weights and biases with AdaGrad formula\n",
        "        # Adding a small epsilon to avoid division by zero\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hW) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Storing input for backward pass\n",
        "        self.A = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        # Computing ReLU activation using np.maximum\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # The gradient is 1 where A > 0 and 0 otherwise.\n",
        "        dA = dZ.copy()\n",
        "        dA[self.A <= 0] = 0\n",
        "        return dA\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss"
      ],
      "metadata": {
        "id": "HUZPGVW0V6ZI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Class completion**"
      ],
      "metadata": {
        "id": "7lZNFRQWVyyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with Xavier method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to Xavier formula\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with He method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to He formula\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "# Optimization Methods\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimization method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.hW = None  # Sum of squared gradients for weights\n",
        "        self.hB = None  # Sum of squared gradients for biases\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer using AdaGrad.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Initializing hW and hB to zeros with the same shape as the weights/biases\n",
        "        if self.hW is None:\n",
        "            self.hW = np.zeros_like(layer.W)\n",
        "        if self.hB is None:\n",
        "            self.hB = np.zeros_like(layer.B)\n",
        "\n",
        "        # Accumulating the square of the gradients\n",
        "        self.hW += layer.dW ** 2\n",
        "        self.hB += layer.dB ** 2\n",
        "\n",
        "        # Updating weights and biases with AdaGrad formula\n",
        "        # Adding a small epsilon to avoid division by zero\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hW) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Storing input for backward pass\n",
        "        self.A = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        # Computing ReLU activation using np.maximum\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # The gradient is 1 where A > 0 and 0 otherwise.\n",
        "        dA = dZ.copy()\n",
        "        dA[self.A <= 0] = 0\n",
        "        return dA\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss\n",
        "\n",
        "class ScratchDeepNeuralNetrowkClassifier:\n",
        "    \"\"\"\n",
        "    A deep neural network classifier from scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_features : int\n",
        "        The number of features in the input data.\n",
        "    n_nodes : list of int\n",
        "        A list where each element represents the number of nodes in a hidden layer.\n",
        "    n_output : int\n",
        "        The number of nodes in the output layer (number of classes).\n",
        "    activations : list of class instances\n",
        "        A list of activation function class instances for each hidden layer.\n",
        "        Example: [ReLU(), Sigmoid()]\n",
        "    initializer: class instance\n",
        "        An instance of the weight initialization method.\n",
        "        Example: SimpleInitializer(sigma=0.01)\n",
        "    optimizer: class instance\n",
        "        An instance of the optimization method.\n",
        "        Example: SGD(lr=0.01)\n",
        "    epochs : int, optional\n",
        "        The number of iterations for training.\n",
        "    batch_size : int, optional\n",
        "        The size of each mini-batch for training.\n",
        "    verbose : bool, optional\n",
        "        Whether to print the loss at each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes, n_output, activations, initializer, optimizer, epochs=50, batch_size=20, verbose=True):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.loss = []\n",
        "        self.n_features = n_features\n",
        "        self.n_output = n_output\n",
        "        self.n_nodes = n_nodes\n",
        "        self.layers = []\n",
        "\n",
        "        # Creating FC layers and activation layers\n",
        "        # The number of FC layers is equal to the number of nodes in n_nodes + 1 (for the output layer)\n",
        "\n",
        "        # Inputing layer to first hidden layer\n",
        "        self.layers.append(FC(n_features, n_nodes[0], initializer, optimizer))\n",
        "        self.layers.append(activations[0])\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(n_nodes) - 1):\n",
        "            self.layers.append(FC(n_nodes[i], n_nodes[i+1], initializer, optimizer))\n",
        "            self.layers.append(activations[i+1])\n",
        "\n",
        "        # Last hidden layer to output layer\n",
        "        self.layers.append(FC(n_nodes[-1], n_output, initializer, optimizer))\n",
        "        self.layers.append(Softmax())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of training data.\n",
        "        y : The following forms of ndarray, shape (n_samples, 1)\n",
        "            Correct answer value of training data.\n",
        "        \"\"\"\n",
        "        # One-hot encode the target labels\n",
        "        y_one_hot = np.eye(self.n_output)[y.astype(int).reshape(-1)]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffling the data for each epoch\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y_one_hot[shuffled_indices]\n",
        "\n",
        "            # Looping through mini-batches\n",
        "            batch_loss = 0\n",
        "            for i in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X_shuffled[i:i + self.batch_size]\n",
        "                y_batch = y_shuffled[i:i + self.batch_size]\n",
        "\n",
        "                # Forward Propagation\n",
        "                A = X_batch\n",
        "                for layer in self.layers:\n",
        "                    A = layer.forward(A)\n",
        "\n",
        "                # Calculating loss\n",
        "                last_layer = self.layers[-1]\n",
        "                loss = last_layer.cross_entropy_error(A, y_batch)\n",
        "                batch_loss += loss\n",
        "\n",
        "                # Backward Propagation\n",
        "                # Softmax + Cross-Entropy gradient\n",
        "                dA = last_layer.backward(A, y_batch)\n",
        "\n",
        "                # Propagate gradient backward through all layers\n",
        "                for layer in reversed(self.layers[:-1]):\n",
        "                    dA = layer.backward(dA)\n",
        "\n",
        "            avg_loss = batch_loss / (X.shape[0] / self.batch_size)\n",
        "            self.loss.append(avg_loss)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the output.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of test data.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        pred : The following forms of ndarray, shape (n_samples, 1)\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        A = X\n",
        "        for layer in self.layers:\n",
        "            A = layer.forward(A)\n",
        "\n",
        "        # The predicted class is the index with the highest probability\n",
        "        pred = np.argmax(A, axis=1)\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "m5yL8lGmYmxp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Learning and estimation**"
      ],
      "metadata": {
        "id": "GGRvjRVNZ9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initializing self.W and self.B using the initializer method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Instantiating variables to store for backward propagation\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        # Storing the input X for use in the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Calculating the output of the fully connected layer\n",
        "        # A = X @ W + B\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind (the next layer)\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward (to the previous layer)\n",
        "        \"\"\"\n",
        "        # Getting the batch size from the input gradient\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculating the gradients for weights and biases\n",
        "        # dW = X^T @ dA\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "\n",
        "        # dB = sum(dA) along the batch dimension\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "\n",
        "        # Calculating the gradient to pass to the previous layer\n",
        "        # dZ = dA @ W^T\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Updating the weights and biases using the optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Initializing weights with a Gaussian distribution\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with Xavier method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to Xavier formula\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization with Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization with He method\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : numpy.ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        # Calculating standard deviation according to He formula\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : numpy.ndarray, shape (n_nodes2,)\n",
        "            Initialized biases (all zeros)\n",
        "        \"\"\"\n",
        "        # Initializing biases to zeros\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "\n",
        "# Optimization Methods\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Updating weights using the learning rate and the calculated gradient\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Updating biases using the learning rate and the calculated gradient\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimization method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.hW = None  # Sum of squared gradients for weights\n",
        "        self.hB = None  # Sum of squared gradients for biases\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer using AdaGrad.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Initialize hW and hB to zeros with the same shape as the weights/biases\n",
        "        if self.hW is None:\n",
        "            self.hW = np.zeros_like(layer.W)\n",
        "        if self.hB is None:\n",
        "            self.hB = np.zeros_like(layer.B)\n",
        "\n",
        "        # Accumulate the square of the gradients\n",
        "        self.hW += layer.dW ** 2\n",
        "        self.hB += layer.dB ** 2\n",
        "\n",
        "        # Update weights and biases with AdaGrad formula\n",
        "        # Add a small epsilon to avoid division by zero\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hW) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Gradient of sigmoid: dZ * (1 - Z)\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Storing input for backward pass\n",
        "        self.A = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        # Computing ReLU activation using np.maximum\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # The gradient is 1 where A > 0 and 0 otherwise.\n",
        "        dA = dZ.copy()\n",
        "        dA[self.A <= 0] = 0\n",
        "        return dA\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation function and Cross-Entropy Loss combined.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.Z = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, A):\n",
        "        A_stable = A - np.max(A, axis=1, keepdims=True)\n",
        "        exp_A = np.exp(A_stable)\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, y_true):\n",
        "        \"\"\"\n",
        "        Backward pass for Softmax with Cross-Entropy Loss.\n",
        "        The gradient simplifies to Z - y_true.\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[0]\n",
        "        dA = (Z - y_true) / batch_size\n",
        "        return dA\n",
        "\n",
        "    def cross_entropy_error(self, Z, y_true):\n",
        "        batch_size = y_true.shape[0]\n",
        "        # Adding a small value to Z to avoid log(0)\n",
        "        loss = -np.sum(y_true * np.log(Z + 1e-7)) / batch_size\n",
        "        return loss\n",
        "\n",
        "class ScratchDeepNeuralNetrowkClassifier:\n",
        "    \"\"\"\n",
        "    A deep neural network classifier from scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_features : int\n",
        "        The number of features in the input data.\n",
        "    n_nodes : list of int\n",
        "        A list where each element represents the number of nodes in a hidden layer.\n",
        "    n_output : int\n",
        "        The number of nodes in the output layer (number of classes).\n",
        "    activations : list of class instances\n",
        "        A list of activation function class instances for each hidden layer.\n",
        "        Example: [ReLU(), Sigmoid()]\n",
        "    initializer: class instance\n",
        "        An instance of the weight initialization method.\n",
        "        Example: SimpleInitializer(sigma=0.01)\n",
        "    optimizer: class instance\n",
        "        An instance of the optimization method.\n",
        "        Example: SGD(lr=0.01)\n",
        "    epochs : int, optional\n",
        "        The number of iterations for training.\n",
        "    batch_size : int, optional\n",
        "        The size of each mini-batch for training.\n",
        "    verbose : bool, optional\n",
        "        Whether to print the loss at each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes, n_output, activations, initializer, optimizer, epochs=50, batch_size=20, verbose=True):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.loss = []\n",
        "        self.n_features = n_features\n",
        "        self.n_output = n_output\n",
        "        self.n_nodes = n_nodes\n",
        "        self.layers = []\n",
        "\n",
        "        # Saving the optimizer class and its learning rate to create new instances\n",
        "        optimizer_class = optimizer.__class__\n",
        "        lr = optimizer.lr\n",
        "\n",
        "        # Creating FC layers and activation layers\n",
        "\n",
        "        # Inputting layer to first hidden layer\n",
        "        self.layers.append(FC(n_features, n_nodes[0], initializer, optimizer_class(lr=lr)))\n",
        "        self.layers.append(activations[0])\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(n_nodes) - 1):\n",
        "            self.layers.append(FC(n_nodes[i], n_nodes[i+1], initializer, optimizer_class(lr=lr)))\n",
        "            self.layers.append(activations[i+1])\n",
        "\n",
        "        # Last hidden layer to output layer\n",
        "        self.layers.append(FC(n_nodes[-1], n_output, initializer, optimizer_class(lr=lr)))\n",
        "        self.layers.append(Softmax())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of training data.\n",
        "        y : The following forms of ndarray, shape (n_samples, 1)\n",
        "            Correct answer value of training data.\n",
        "        \"\"\"\n",
        "        # One-hot encode the target labels\n",
        "        y_one_hot = np.eye(self.n_output)[y.astype(int).reshape(-1)]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffling the data for each epoch\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y_one_hot[shuffled_indices]\n",
        "\n",
        "            # Looping through mini-batches\n",
        "            batch_loss = 0\n",
        "            for i in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X_shuffled[i:i + self.batch_size]\n",
        "                y_batch = y_shuffled[i:i + self.batch_size]\n",
        "\n",
        "                # Forward Propagation\n",
        "                A = X_batch\n",
        "                for layer in self.layers:\n",
        "                    A = layer.forward(A)\n",
        "\n",
        "                # Calculate loss\n",
        "                last_layer = self.layers[-1]\n",
        "                loss = last_layer.cross_entropy_error(A, y_batch)\n",
        "                batch_loss += loss\n",
        "\n",
        "                # --- Backward Propagation ---\n",
        "                # Softmax + Cross-Entropy gradient\n",
        "                dA = last_layer.backward(A, y_batch)\n",
        "\n",
        "                # Propagate gradient backward through all layers\n",
        "                for layer in reversed(self.layers[:-1]):\n",
        "                    dA = layer.backward(dA)\n",
        "\n",
        "            avg_loss = batch_loss / (X.shape[0] / self.batch_size)\n",
        "            self.loss.append(avg_loss)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the output.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of test data.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        pred : The following forms of ndarray, shape (n_samples, 1)\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        A = X\n",
        "        for layer in self.layers:\n",
        "            A = layer.forward(A)\n",
        "\n",
        "        pred = np.argmax(A, axis=1)\n",
        "\n",
        "        return pred\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Flattening and normalizing the images\n",
        "    X_train = X_train.reshape(-1, 784).astype(np.float64) / 255.0\n",
        "    X_test = X_test.reshape(-1, 784).astype(np.float64) / 255.0\n",
        "\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}, Feature size: {X_train.shape[1]}\")\n",
        "\n",
        "    # Splitting training data to use a validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "    n_output = len(np.unique(y_train))\n",
        "\n",
        "    # Network Configuration\n",
        "    n_nodes_hidden = [128, 64]\n",
        "    activations = [ReLU(), ReLU()]\n",
        "    initializer = HeInitializer()\n",
        "    optimizer = AdaGrad(lr=0.01)\n",
        "    epochs = 20\n",
        "    batch_size = 100\n",
        "\n",
        "    print(\"Initializing the deep neural network...\")\n",
        "    nn = ScratchDeepNeuralNetrowkClassifier(\n",
        "        n_features=n_features,\n",
        "        n_nodes=n_nodes_hidden,\n",
        "        n_output=n_output,\n",
        "        activations=activations,\n",
        "        initializer=initializer,\n",
        "        optimizer=optimizer,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    nn.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Training complete. Evaluating on test set...\")\n",
        "    y_pred = nn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Final Test Accuracy on MNIST: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Plotting loss curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    epochs_range = range(1, len(nn.loss) + 1)\n",
        "    plt.plot(epochs_range, nn.loss, label=\"Training Loss\")\n",
        "    plt.title(\"Loss Curve\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Cross-Entropy Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Rnz4NjiaEhA",
        "outputId": "bcd305ba-ca0a-426d-a85e-3d5f8a6ea2e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "Training samples: 60000, Test samples: 10000, Feature size: 784\n",
            "Initializing the deep neural network...\n",
            "Starting training...\n",
            "Epoch 1/20 - Loss: 0.2798\n",
            "Epoch 2/20 - Loss: 0.1398\n",
            "Epoch 3/20 - Loss: 0.1102\n",
            "Epoch 4/20 - Loss: 0.0933\n",
            "Epoch 5/20 - Loss: 0.0815\n",
            "Epoch 6/20 - Loss: 0.0721\n",
            "Epoch 7/20 - Loss: 0.0649\n",
            "Epoch 8/20 - Loss: 0.0595\n",
            "Epoch 9/20 - Loss: 0.0546\n",
            "Epoch 10/20 - Loss: 0.0504\n",
            "Epoch 11/20 - Loss: 0.0468\n",
            "Epoch 12/20 - Loss: 0.0435\n",
            "Epoch 13/20 - Loss: 0.0410\n",
            "Epoch 14/20 - Loss: 0.0384\n",
            "Epoch 15/20 - Loss: 0.0358\n",
            "Epoch 16/20 - Loss: 0.0343\n",
            "Epoch 17/20 - Loss: 0.0320\n",
            "Epoch 18/20 - Loss: 0.0304\n",
            "Epoch 19/20 - Loss: 0.0289\n",
            "Epoch 20/20 - Loss: 0.0274\n",
            "Training complete. Evaluating on test set...\n",
            "Final Test Accuracy on MNIST: 97.68%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcetJREFUeJzt3Xd8VfX9x/H3vTfJzd4TCAQSZMlQEGQ4QZaKWqqI/gRp1TooKloVKwJqxVWLg6rVgrR14KiggihQg4oMZShbCHtkEMheN7nn90fIhZgAuXBz783N6/l45JHcc88993O/HNO8+10mwzAMAQAAAADOitnTBQAAAACALyBcAQAAAIALEK4AAAAAwAUIVwAAAADgAoQrAAAAAHABwhUAAAAAuADhCgAAAABcgHAFAAAAAC5AuAIAAAAAFyBcAQAAAIALEK4AAG7z9ttvy2Qy6ccff/R0KQ2yfv16/d///Z+Sk5NltVoVHR2tQYMGafbs2aqqqvJ0eQAAL+Pn6QIAAPBGb731lu68804lJCTolltuUfv27VVYWKilS5fq97//vQ4dOqRHH33U02UCALwI4QoAgF9ZuXKl7rzzTvXt21cLFy5UWFiY47n77rtPP/74ozZu3OiS9youLlZISIhLrgUA8CyGBQIAvM66des0bNgwhYeHKzQ0VAMHDtTKlStrnWOz2TRt2jS1b99egYGBiomJ0YABA7R48WLHOZmZmRo3bpxatWolq9WqpKQkXXPNNdq9e/cp33/atGkymUx65513agWrGr169dKtt94qSUpPT5fJZFJ6enqtc3bv3i2TyaS3337bcezWW29VaGioMjIyNHz4cIWFhenmm2/W+PHjFRoaqpKSkjrvNXr0aCUmJtYahvjFF1/ooosuUkhIiMLCwnTllVdq06ZNp/xMAIDGR7gCAHiVTZs26aKLLtJPP/2khx56SJMnT9auXbt06aWXatWqVY7zpk6dqmnTpumyyy7Tq6++qj//+c9q3bq11q5d6zhn5MiR+uSTTzRu3Dj9/e9/14QJE1RYWKi9e/ee9P1LSkq0dOlSXXzxxWrdurXLP19lZaWGDBmi+Ph4vfDCCxo5cqRGjRql4uJiLViwoE4tn332mX7729/KYrFIkv7973/ryiuvVGhoqJ599llNnjxZmzdv1oABA04bGgEAjYthgQAAr/LYY4/JZrPpu+++U7t27SRJY8aMUYcOHfTQQw9p2bJlkqQFCxZo+PDh+sc//lHvdfLy8vT999/r+eef14MPPug4PmnSpFO+/44dO2Sz2dS1a1cXfaLaysvLdf3112v69OmOY4ZhqGXLlpo7d66uv/56x/EFCxaouLhYo0aNkiQVFRVpwoQJuu2222p97rFjx6pDhw56+umnT9oeAIDGR88VAMBrVFVV6auvvtK1117rCFaSlJSUpJtuuknfffedCgoKJEmRkZHatGmTtm/fXu+1goKCFBAQoPT0dB09erTBNdRcv77hgK5y11131XpsMpl0/fXXa+HChSoqKnIcnzt3rlq2bKkBAwZIkhYvXqy8vDyNHj1ahw8fdnxZLBb16dNHX3/9daPVDAA4PcIVAMBr5OTkqKSkRB06dKjzXKdOnWS327Vv3z5J0hNPPKG8vDydc8456tq1q/70pz/p559/dpxvtVr17LPP6osvvlBCQoIuvvhiPffcc8rMzDxlDeHh4ZKkwsJCF36y4/z8/NSqVas6x0eNGqXS0lJ9+umnkqp7qRYuXKjrr79eJpNJkhxB8vLLL1dcXFytr6+++krZ2dmNUjMAoGEIVwCAJuniiy9WRkaGZs2apXPPPVdvvfWWzj//fL311luOc+677z798ssvmj59ugIDAzV58mR16tRJ69atO+l109LS5Ofnpw0bNjSojprg82sn2wfLarXKbK77P78XXnihUlJS9MEHH0iSPvvsM5WWljqGBEqS3W6XVD3vavHixXW+5s+f36CaAQCNg3AFAPAacXFxCg4O1rZt2+o8t3XrVpnNZiUnJzuORUdHa9y4cXrvvfe0b98+devWTVOnTq31utTUVD3wwAP66quvtHHjRlVUVOivf/3rSWsIDg7W5Zdfrm+++cbRS3YqUVFRkqrneJ1oz549p33tr91www1atGiRCgoKNHfuXKWkpOjCCy+s9VkkKT4+XoMGDarzdemllzr9ngAA1yFcAQC8hsVi0eDBgzV//vxaK99lZWXp3Xff1YABAxzD9nJzc2u9NjQ0VGlpaSovL5dUvdJeWVlZrXNSU1MVFhbmOOdkpkyZIsMwdMstt9SaA1VjzZo1mjNnjiSpTZs2slgs+uabb2qd8/e//71hH/oEo0aNUnl5uebMmaNFixbphhtuqPX8kCFDFB4erqefflo2m63O63Nycpx+TwCA67BaIADA7WbNmqVFixbVOX7vvffqqaee0uLFizVgwADdfffd8vPz0xtvvKHy8nI999xzjnM7d+6sSy+9VD179lR0dLR+/PFHffTRRxo/frwk6ZdfftHAgQN1ww03qHPnzvLz89Mnn3yirKws3Xjjjaesr1+/fpo5c6buvvtudezYUbfccovat2+vwsJCpaen69NPP9VTTz0lSYqIiND111+vV155RSaTSampqfr888/PaP7T+eefr7S0NP35z39WeXl5rSGBUvV8sNdee0233HKLzj//fN14442Ki4vT3r17tWDBAvXv31+vvvqq0+8LAHARAwAAN5k9e7Yh6aRf+/btMwzDMNauXWsMGTLECA0NNYKDg43LLrvM+P7772td66mnnjJ69+5tREZGGkFBQUbHjh2Nv/zlL0ZFRYVhGIZx+PBh45577jE6duxohISEGBEREUafPn2MDz74oMH1rlmzxrjpppuMFi1aGP7+/kZUVJQxcOBAY86cOUZVVZXjvJycHGPkyJFGcHCwERUVZfzhD38wNm7caEgyZs+e7Thv7NixRkhIyCnf889//rMhyUhLSzvpOV9//bUxZMgQIyIiwggMDDRSU1ONW2+91fjxxx8b/NkAAK5nMgzD8FiyAwAAAAAfwZwrAAAAAHABwhUAAAAAuADhCgAAAABcgHAFAAAAAC5AuAIAAAAAFyBcAQAAAIALsIlwPex2uw4ePKiwsDCZTCZPlwMAAADAQwzDUGFhoVq0aCGz+dR9U4Srehw8eFDJycmeLgMAAACAl9i3b59atWp1ynMIV/UICwuTVN2A4eHhHq7Gt9lsNn311VcaPHiw/P39PV1Os0Cbuxft7X60ufvR5u5Fe7sfbe5+3tTmBQUFSk5OdmSEUyFc1aNmKGB4eDjhqpHZbDYFBwcrPDzc4//hNBe0uXvR3u5Hm7sfbe5etLf70ebu541t3pDpQixoAQAAAAAuQLgCAAAAABcgXAEAAACACzDnCgAAAD7LMAxVVlaqqqrqjK9hs9nk5+ensrKys7oOGs6dbW6xWOTn5+eSLZgIVwAAAPBJFRUVOnTokEpKSs7qOoZhKDExUfv27WMPVDdxd5sHBwcrKSlJAQEBZ3UdwhUAAAB8jt1u165du2SxWNSiRQsFBASc8R/pdrtdRUVFCg0NPe0msnANd7W5YRiqqKhQTk6Odu3apfbt25/V+xGuAAAA4HMqKipkt9uVnJys4ODgs7qW3W5XRUWFAgMDCVdu4s42DwoKkr+/v/bs2eN4zzPF3QEAAACfRRhCQ7jqPuFuAwAAAAAXIFwBAAAAgAsQrgAAAAAfl5KSohkzZjT4/PT0dJlMJuXl5TVaTb6IcAUAAAB4CZPJdMqvqVOnntF1f/jhB91xxx0NPr9fv346dOiQIiIizuj9GsrXQhyrBQIAAABe4tChQ46f586dq8cff1zbtm1zHAsNDXX8bBiGqqqq5Od3+j/p4+LinKojICBAiYmJTr0G9FwBAACgmTAMQyUVlWf0VVpRdcavLamolGEYDaoxMTHR8RURESGTyeR4vHXrVoWFhemLL75Qz549ZbVa9d133ykjI0PXXHONEhISFBoaqgsuuEBLliypdd1fDws0mUx66623dN111yk4OFjt27fXp59+6nj+1z1Kb7/9tiIjI/Xll1+qU6dOCg0N1dChQ2uFwcrKSk2YMEGRkZGKiYnRww8/rLFjx+raa68943+zo0ePasyYMYqKilJwcLCGDRum7du3O57fs2ePrr76akVFRSkkJERdunTRwoULHa+9+eabFRcXp6CgILVv316zZ88+41oagp4rAAAANAultip1fvxLj7z35ieGKDjANX96P/LII3rhhRfUrl07RUVFad++fRo+fLj+8pe/yGq16l//+peuvvpqbdu2Ta1btz7pdaZNm6bnnntOzz//vF555RXdfPPN2rNnj6Kjo+s9v6SkRC+88IL+/e9/y2w26//+7//04IMP6p133pEkPfvss3rnnXc0e/ZsderUSS+99JLmzZunyy677Iw/67hx47Rjxw59+umnCg8P18MPP6zhw4dr8+bN8vf31z333KOKigp98803CgkJ0ebNmx29e5MnT9bmzZv1xRdfKDY2Vjt27FBpaekZ19IQhCsAAACgCXniiSd0xRVXOB5HR0ere/fujsdPPvmkPvnkE3366acaP378Sa9z6623avTo0ZKkp59+Wi+//LJWr16toUOH1nu+zWbT66+/rtTUVEnS+PHj9cQTTzief+WVVzRp0iRdd911kqRXX33V0Yt0JjIyMvTZZ59p+fLl6tevnyTpnXfeUXJysubNm6frr79ee/fu1ciRI9W1a1dJUrt27Ryv37t3r8477zz16tVLUnXvXWMjXHm5n/bl6ZesQg3qlKCokABPlwMAANBkBflbtPmJIU6/zm63q7CgUGHhYWe82WyQv+WMXlefmrBQo6ioSFOnTtWCBQt06NAhVVZWqrS0VHv37j3ldbp16+b4OSQkROHh4crOzj7p+cHBwY5gJUlJSUmO8/Pz85WVlaXevXs7nrdYLOrZs6fsdrtTn6/Gtm3b5Ofnpz59+jiOxcTEqEOHDtqyZYskacKECbrrrrv01VdfadCgQRo5cqTjc911110aOXKk1q5dq8GDB+vaa691hLTGwpwrLzfxg/X600c/a8OBfE+XAgAA0KSZTCYFB/id0VdQgOWMXxsc4CeTyeSyzxESElLr8YMPPqhPPvlETz/9tL799lutX79eXbt2VUVFxSmv4+/vX6d9ThWE6ju/oXPJGsttt92mnTt36pZbbtGGDRvUq1cvvfLKK5KkYcOGac+ePbr//vt18OBBDRw4UA8++GCj1kO48nJp8dVjRndkF3m4EgAAAHij5cuX69Zbb9V1112nrl27KjExUbt373ZrDREREUpISNAPP/zgOFZVVaW1a9ee8TU7dOigyspKrVq1ynEsNzdX27ZtU+fOnR3HkpOTdeedd+q///2vHnjgAb355puO5+Li4jR27Fj95z//0YwZM/SPf/zjjOtpCIYFerm0+FB9uSlLO3IIVwAAAKirffv2+u9//6urr75aJpNJkydPPuOheGfjj3/8o6ZPn660tDR17NhRr7zyio4ePdqgXrsNGzYoLCzM8dgwDKWmpmrEiBG6/fbb9cYbbygsLEyPPPKIWrZsqWuuuUaSdN9992nYsGE655xzdPToUX399dfq1KmTJOnxxx9Xz5491aVLF5WXl+vzzz93PNdYCFdejp4rAAAAnMqLL76o3/3ud+rXr59iY2P18MMPq6CgwO11PPzww8rMzNSYMWNksVh0xx13aMiQIbJYTj/f7OKLL6712GKx6PDhw5o1a5buv/9+XXXVVaqoqNDFF1+shQsXOoYoVlVV6Z577tH+/fsVHh6uoUOH6m9/+5uk6r26Jk2apN27dysoKEgXXXSR3n//fdd/8BOYDE8PlPRCBQUFioiIUH5+vsLDwz1ay8/78zTi1eWKDQ3Qj49dcfoXNDE2m00LFy7U8OHD64zjReOgzd2L9nY/2tz9aHP3or0bpqysTLt27VLbtm0VGBh4Vtey2+0qKChQeHj4GS9o0RzZ7XZ16tRJN9xwg5588kmnX+vONj/V/eJMNqDnysulxlX3XB0uqlBeSYUig1kxEAAAAN5nz549+uqrr3TJJZeovLxcr776qnbt2qWbbrrJ06W5DdHby4VY/dQiojo9MzQQAAAA3spsNuvtt9/WBRdcoP79+2vDhg1asmRJo89z8ib0XDUBqfGhOphfph3ZReqVUv+O2QAAAIAnJScna/ny5Z4uw6PouWoCWNQCAAAA8H6EqyagZt5VBsuxAwAAOIW129AQrrpPCFdNgKPninAFAADQIDUrKZaUlHi4EjQFNffJ2a7AyZyrJqAmXO0/WqoyW5UC/U+/VwAAAEBzZrFYFBkZqezsbElScHBwgzazrY/dbldFRYXKyspYit1N3NXmhmGopKRE2dnZioyMbNCeXKdCuGoCYkICFBnsr7wSmzJyitSlRYSnSwIAAPB6iYmJkuQIWGfKMAyVlpYqKCjojAManOPuNo+MjHTcL2eDcNUEmEwmpcaFas2eo8rIKSZcAQAANIDJZFJSUpLi4+Nls9nO+Do2m03ffPONLr74YjZudhN3trm/v/9Z91jVIFw1EWnHwhUrBgIAADjHYrGc1R/PFotFlZWVCgwMJFy5SVNtcwaNNhE1864yCFcAAACAVyJcNRHsdQUAAAB4N8JVE1ETrnYdLlaVnf0aAAAAAG9DuGoiWkQGyepnVkWVXfuOsF8DAAAA4G0IV02ExWxSuziGBgIAAADeinDVhDjmXeUQrgAAAABvQ7hqQtLouQIAAAC8FuGqCUmND5EkZdBzBQAAAHgdwlUTcuJy7IbBioEAAACANyFcNSFtY0NkNkmFZZXKKSz3dDkAAAAATkC4akKsfha1jg6WxLwrAAAAwNsQrpqY1GOLWjDvCgAAAPAuhKsm5sR5VwAAAAC8B+GqiUllrysAAADAKxGumhh6rgAAAADvRLhqYmrmXGUVlKuwzObhagAAAADUIFw1MRFB/ooLs0qSMnKKPVwNAAAAgBqEqyYoLY6hgQAAAIC3IVw1Qcy7AgAAALwP4aoJIlwBAAAA3odw1QTVLGqxk+XYAQAAAK9BuGqCanqu9hwpUUWl3cPVAAAAAJAIV01SQrhVoVY/VdkN7c5lxUAAAADAGxCumiCTyaRU5l0BAAAAXoVw1USlxoVIkjIIVwAAAIBXIFw1UY4VA1nUAgAAAPAKhKsmio2EAQAAAO9CuGqianquMnKKZLcbHq4GAAAAAOGqiWodHSx/i0llNrsO5pd6uhwAAACg2SNcNVF+FrNSYqoXtWBoIAAAAOB5hKsmLI3l2AEAAACv4RXhaubMmUpJSVFgYKD69Omj1atXn/TcN998UxdddJGioqIUFRWlQYMG1Tn/1ltvlclkqvU1dOjQxv4YbnfivCsAAAAAnuXxcDV37lxNnDhRU6ZM0dq1a9W9e3cNGTJE2dnZ9Z6fnp6u0aNH6+uvv9aKFSuUnJyswYMH68CBA7XOGzp0qA4dOuT4eu+999zxcdyKnisAAADAe3g8XL344ou6/fbbNW7cOHXu3Fmvv/66goODNWvWrHrPf+edd3T33XerR48e6tixo9566y3Z7XYtXbq01nlWq1WJiYmOr6ioKHd8HLdKjavpuSr2cCUAAAAA/Dz55hUVFVqzZo0mTZrkOGY2mzVo0CCtWLGiQdcoKSmRzWZTdHR0rePp6emKj49XVFSULr/8cj311FOKiYmp9xrl5eUqLy93PC4oKJAk2Ww22Ww2Zz+W2yRHBkiSjhRXKCuvWNEhAR6uyHk17evN7exraHP3or3djzZ3P9rcvWhv96PN3c+b2tyZGkyGYXhsk6SDBw+qZcuW+v7779W3b1/H8YceekjLli3TqlWrTnuNu+++W19++aU2bdqkwMBASdL777+v4OBgtW3bVhkZGXr00UcVGhqqFStWyGKx1LnG1KlTNW3atDrH3333XQUHB5/FJ2x809ZadKTcpAldKpUa7ulqAAAAAN9SUlKim266Sfn5+QoPP/Uf3B7tuTpbzzzzjN5//32lp6c7gpUk3XjjjY6fu3btqm7duik1NVXp6ekaOHBgnetMmjRJEydOdDwuKChwzOU6XQN62seH1+ib7bmKS+2m4Re08nQ5TrPZbFq8eLGuuOIK+fv7e7qcZoE2dy/a2/1oc/ejzd2L9nY/2tz9vKnNa0a1NYRHw1VsbKwsFouysrJqHc/KylJiYuIpX/vCCy/omWee0ZIlS9StW7dTntuuXTvFxsZqx44d9YYrq9Uqq9Va57i/v7/H/zFPp31CuL7ZnqvdR0q9vtZTaQpt7Wtoc/eivd2PNnc/2ty9aG/3o83dzxva3Jn39+iCFgEBAerZs2etxShqFqc4cZjgrz333HN68skntWjRIvXq1eu077N//37l5uYqKSnJJXV7k5pFLVgxEAAAAPAsj68WOHHiRL355puaM2eOtmzZorvuukvFxcUaN26cJGnMmDG1Frx49tlnNXnyZM2aNUspKSnKzMxUZmamioqqw0VRUZH+9Kc/aeXKldq9e7eWLl2qa665RmlpaRoyZIhHPmNjYjl2AAAAwDt4fM7VqFGjlJOTo8cff1yZmZnq0aOHFi1apISEBEnS3r17ZTYfz4CvvfaaKioq9Nvf/rbWdaZMmaKpU6fKYrHo559/1pw5c5SXl6cWLVpo8ODBevLJJ+sd+tfU1YSrA3mlKqmoVHCAx/9JAQAAgGbJK/4SHz9+vMaPH1/vc+np6bUe7969+5TXCgoK0pdffumiyrxfdEiAokMCdKS4QjtzinVuywhPlwQAAAA0Sx4fFoizlxoXIknKyGFoIAAAAOAphCsfwLwrAAAAwPMIVz6AFQMBAAAAzyNc+QB6rgAAAADPI1z5gJqeq925xaqssnu4GgAAAKB5Ilz5gJaRQQryt8hWZWjvkRJPlwMAAAA0S4QrH2A2m9Tu2IqBDA0EAAAAPINw5SMc865Yjh0AAADwCMKVj0g7Nu8qI7vYw5UAAAAAzRPhykek0nMFAAAAeBThykfUDAvMyC6SYRgergYAAABofghXPiIlJkQWs0lF5ZXKKij3dDkAAABAs0O48hEBfma1iQ6WJGUwNBAAAABwO8KVD2l3bFELlmMHAAAA3I9w5UMcy7ETrgAAAAC3I1z5EMIVAAAA4DmEKx/CRsIAAACA5xCufEi7uBBJUk5hufJLbR6uBgAAAGheCFc+JDzQXwnhVkmsGAgAAAC4G+HKxzDvCgAAAPAMwpWPSTu2HHsG4QoAAABwK8KVj6npuWJYIAAAAOBehCsfk8pGwgAAAIBHEK58TE3P1d4jJSqzVXm4GgAAAKD5IFz5mLgwq8IC/WQ3pN25xZ4uBwAAAGg2CFc+xmQyHZ93lU24AgAAANyFcOWDmHcFAAAAuB/hygc59rpixUAAAADAbQhXPiiNnisAAADA7QhXPqim52pnTpGq7IaHqwEAAACaB8KVD0qODlaAxazySrsO5pV6uhwAAACgWSBc+SCL2aS2sSGSGBoIAAAAuAvhykc5FrUgXAEAAABuQbjyUamEKwAAAMCtCFc+yrGRMMuxAwAAAG5BuPJRqXHH5lzlFMkwWDEQAAAAaGyEKx+VGhcqk0nKK7Ept7jC0+UAAAAAPo9w5aMC/S1qFRUkiXlXAAAAgDsQrnxYWhzzrgAAAAB3IVz5sNQ4VgwEAAAA3IVw5cPY6woAAABwH8KVD3Msx064AgAAABod4cqH1YSrg/llKi6v9HA1AAAAgG8jXPmwyOAAxYYGSJJ25hR7uBoAAADAtxGufFy7mkUtcgo9XAkAAADg2whXPo5FLQAAAAD3IFz5uDSWYwcAAADcgnDl4xwrBjLnCgAAAGhUhCsfl3osXO0+XCxbld3D1QAAAAC+i3Dl41pEBCo4wKJKu6E9uSWeLgcAAADwWYQrH2cymZTKvCsAAACg0RGumoHj864IVwAAAEBjIVw1A45wRc8VAAAA0GgIV81AalyIJGkHPVcAAABAoyFcNQMn9lwZhuHhagAAAADfRLhqBtrEhMjPbFJxRZUO5Zd5uhwAAADAJxGumgF/i1ltYoIlsagFAAAA0FgIV80Ey7EDAAAAjYtw1UzUzLsiXAEAAACNg3DVTBCuAAAAgMZFuGomjm8kXOzhSgAAAADfRLhqJtodm3N1uKhc+SU2D1cDAAAA+B7CVTMRavVTUkSgJGlHTqGHqwEAAAB8D+GqGWHeFQAAANB4CFfNCMuxAwAAAI2HcNWMsKgFAAAA0HgIV80IPVcAAABA4yFcNSM1PVf7jpaozFbl4WoAAAAA30K4akZiQwMUEeQvw5B2MjQQAAAAcCnCVTNiMplOmHfF0EAAAADAlQhXzUxqXIgk5l0BAAAArka4amYce13RcwUAAAC4FOGqmXEMC6TnCgAAAHAprwhXM2fOVEpKigIDA9WnTx+tXr36pOe++eabuuiiixQVFaWoqCgNGjSozvmGYejxxx9XUlKSgoKCNGjQIG3fvr2xP0aTkBYXJknaebhYVXbDw9UAAAAAvsPj4Wru3LmaOHGipkyZorVr16p79+4aMmSIsrOz6z0/PT1do0eP1tdff60VK1YoOTlZgwcP1oEDBxznPPfcc3r55Zf1+uuva9WqVQoJCdGQIUNUVlbmro/ltVpGBSnAz6yKSrv2Hy3xdDkAAACAz/DzdAEvvviibr/9do0bN06S9Prrr2vBggWaNWuWHnnkkTrnv/POO7Uev/XWW/r444+1dOlSjRkzRoZhaMaMGXrsscd0zTXXSJL+9a9/KSEhQfPmzdONN95Y55rl5eUqLy93PC4oKJAk2Ww22Ww2l31Wb9EuJlhbs4q09VC+WoQHeLSWmvb1xXb2VrS5e9He7kebux9t7l60t/vR5u7nTW3uTA0mwzA8NjasoqJCwcHB+uijj3Tttdc6jo8dO1Z5eXmaP3/+aa9RWFio+Ph4ffjhh7rqqqu0c+dOpaamat26derRo4fjvEsuuUQ9evTQSy+9VOcaU6dO1bRp0+ocf/fddxUcHHxGn82bvf2LWetyzRrRukoDWzI0EAAAADiZkpIS3XTTTcrPz1d4ePgpz/Voz9Xhw4dVVVWlhISEWscTEhK0devWBl3j4YcfVosWLTRo0CBJUmZmpuMav75mzXO/NmnSJE2cONHxuKCgwDHc8HQN2BRlBGZo3dcZ8o9N1vDh53q0FpvNpsWLF+uKK66Qv7+/R2tpLmhz96K93Y82dz/a3L1ob/ejzd3Pm9q8ZlRbQ3h8WODZeOaZZ/T+++8rPT1dgYGBZ3wdq9Uqq9Va57i/v7/H/zEbwzlJ1YFx1+ESr/l8vtrW3ow2dy/a2/1oc/ejzd2L9nY/2tz9vKHNnXl/jy5oERsbK4vFoqysrFrHs7KylJiYeMrXvvDCC3rmmWf01VdfqVu3bo7jNa87k2s2F6lxx/a6yi6SB0eFAgAAAD7Fo+EqICBAPXv21NKlSx3H7Ha7li5dqr59+570dc8995yefPJJLVq0SL169ar1XNu2bZWYmFjrmgUFBVq1atUpr9mctI0NkdkkFZRVKqeo/PQvAAAAAHBaHh8WOHHiRI0dO1a9evVS7969NWPGDBUXFztWDxwzZoxatmyp6dOnS5KeffZZPf7443r33XeVkpLimEcVGhqq0NBQmUwm3XfffXrqqafUvn17tW3bVpMnT1aLFi1qLZrRnAX6W5QcHaw9uSXakV2k+LAzH1IJAAAAoJrHw9WoUaOUk5Ojxx9/XJmZmerRo4cWLVrkWJBi7969MpuPd7C99tprqqio0G9/+9ta15kyZYqmTp0qSXrooYdUXFysO+64Q3l5eRowYIAWLVp0VvOyfE1aXKj25JYoI6dY/VJjPV0OAAAA0OR5PFxJ0vjx4zV+/Ph6n0tPT6/1ePfu3ae9nslk0hNPPKEnnnjCBdX5ptT4UC3dmq2M7CJPlwIAAAD4BI/OuYLnpJ2wqAUAAACAs0e4aqZS4wlXAAAAgCsRrpqptGPhKrOgTEXllR6uBgAAAGj6CFfNVESQv+LCqjdOZt4VAAAAcPYIV81YalyIJIYGAgAAAK5AuGrGaoYG7sghXAEAAABny+lwNWfOHC1YsMDx+KGHHlJkZKT69eunPXv2uLQ4NC5WDAQAAABcx+lw9fTTTysoKEiStGLFCs2cOVPPPfecYmNjdf/997u8QDSetPgwSVIGPVcAAADAWXN6E+F9+/YpLS1NkjRv3jyNHDlSd9xxh/r3769LL73U1fWhEaXGV8+52pNboopKuwL8GCUKAAAAnCmn/5oODQ1Vbm6uJOmrr77SFVdcIUkKDAxUaWmpa6tDo0oMD1So1U9VdkN7cos9XQ4AAADQpDkdrq644grddtttuu222/TLL79o+PDhkqRNmzYpJSXF1fWhEZlMJlYMBAAAAFzE6XA1c+ZM9e3bVzk5Ofr4448VExMjSVqzZo1Gjx7t8gLRuFKPrRjIvCsAAADg7Dg95yoyMlKvvvpqnePTpk1zSUFwr1RWDAQAAABcwumeq0WLFum7775zPJ45c6Z69Oihm266SUePHnVpcWh87HUFAAAAuIbT4epPf/qTCgoKJEkbNmzQAw88oOHDh2vXrl2aOHGiywtE46oJVxnZxbLbDQ9XAwAAADRdTg8L3LVrlzp37ixJ+vjjj3XVVVfp6aef1tq1ax2LW6DpaBMdLH+LSaW2Kh0qKFPLyCBPlwQAAAA0SU73XAUEBKikpESStGTJEg0ePFiSFB0d7ejRQtPhZzErJYYVAwEAAICz5XS4GjBggCZOnKgnn3xSq1ev1pVXXilJ+uWXX9SqVSuXF4jGx6IWAAAAwNlzOly9+uqr8vPz00cffaTXXntNLVu2lCR98cUXGjp0qMsLRONzLGpBuAIAAADOmNNzrlq3bq3PP/+8zvG//e1vLikI7nd8UQvCFQAAAHCmnA5XklRVVaV58+Zpy5YtkqQuXbpoxIgRslgsLi0O7pHGRsIAAADAWXM6XO3YsUPDhw/XgQMH1KFDB0nS9OnTlZycrAULFig1NdXlRaJxtYurXtAit7hCR4srFBUS4OGKAAAAgKbH6TlXEyZMUGpqqvbt26e1a9dq7dq12rt3r9q2basJEyY0Ro1oZMEBfo4l2NlMGAAAADgzTvdcLVu2TCtXrlR0dLTjWExMjJ555hn179/fpcXBfVLjQ3Ugr1Q7sot0QUr06V8AAAAAoBane66sVqsKCwvrHC8qKlJAAMPJmqq0OBa1AAAAAM6G0+Hqqquu0h133KFVq1bJMAwZhqGVK1fqzjvv1IgRIxqjRrhBavyxjYQZFggAAACcEafD1csvv6zU1FT17dtXgYGBCgwMVP/+/ZWWlqYZM2Y0QolwhzQ2EgYAAADOitNzriIjIzV//nzt2LHDsRR7p06dlJaW5vLi4D41y7EfyCtVaUWVggJYVh8AAABwxhntcyVJaWlptQLVzz//rF69eqmiosIlhcG9YkKtigr219ESm3YeLlKXFhGeLgkAAABoUpweFngyhmGoqqrKVZeDB9T0XjE0EAAAAHCey8IVmr5UVgwEAAAAzhjhCg6OnitWDAQAAACc1uA5VwUFBad8vr69r9C0pDIsEAAAADhjDQ5XkZGRMplMJ33eMIxTPg/vV7Mc++7DJaqsssvPQscmAAAA0FANDldff/11Y9YBL9AyMkiB/maV2ezad7RUbWNDPF0SAAAA0GQ0OFxdcskljVkHvIDZbFK72FBtPlSgHdlFhCsAAADACYz7Qi0sxw4AAACcGcIVaqkJVxmsGAgAAAA4hXCFWui5AgAAAM4M4Qq1nLiRsGEYHq4GAAAAaDqcDlezZ89WSUlJY9QCL5ASGyyzSSosr1R2YbmnywEAAACaDKfD1SOPPKLExET9/ve/1/fff98YNcGDrH4WtYmpXiUwg6GBAAAAQIM5Ha4OHDigOXPm6PDhw7r00kvVsWNHPfvss8rMzGyM+uABNUMDd7CoBQAAANBgTocrPz8/XXfddZo/f7727dun22+/Xe+8845at26tESNGaP78+bLb7Y1RK9wkNb6654pFLQAAAICGO6sFLRISEjRgwAD17dtXZrNZGzZs0NixY5Wamqr09HQXlQh3S4tjxUAAAADAWWcUrrKysvTCCy+oS5cuuvTSS1VQUKDPP/9cu3bt0oEDB3TDDTdo7Nixrq4VbsJy7AAAAIDznA5XV199tZKTk/X222/r9ttv14EDB/Tee+9p0KBBkqSQkBA98MAD2rdvn8uLhXukHgtX2YXlKiizebgaAAAAoGnwc/YF8fHxWrZsmfr27XvSc+Li4rRr166zKgyeEx7or/gwq7ILy5WRXaTzWkd5uiQAAADA6zkdrv75z3+e9hyTyaQ2bdqcUUHwDmnxocouLNcOwhUAAADQIGc052rp0qW66qqrlJqaqtTUVF111VVasmSJq2uDBznmXbEcOwAAANAgToerv//97xo6dKjCwsJ077336t5771V4eLiGDx+umTNnNkaN8ICacJWRXezhSgAAAICmwelhgU8//bT+9re/afz48Y5jEyZMUP/+/fX000/rnnvucWmB8Iya5dgz6LkCAAAAGsTpnqu8vDwNHTq0zvHBgwcrPz/fJUXB82pWDNyTW6zyyioPVwMAAAB4P6fD1YgRI/TJJ5/UOT5//nxdddVVLikKnhcfZlWY1U92Q9p9uMTT5QAAAABez+lhgZ07d9Zf/vIXpaenO5ZjX7lypZYvX64HHnhAL7/8suPcCRMmuK5SuJXJZFJqfKjW78tTRk6ROiSGebokAAAAwKud0VLsUVFR2rx5szZv3uw4HhkZWWuZdpPJRLhq4tKOhasd2cy7AgAAAE7H6XDF5sDNR+qxRS0IVwAAAMDpndE+VzUMw5BhGK6qBV7GsdcV4QoAAAA4rTMKV//617/UtWtXBQUFKSgoSN26ddO///1vV9cGD6sJVzsPF8luJ0QDAAAAp+L0sMAXX3xRkydP1vjx49W/f39J0nfffac777xThw8f1v333+/yIuEZyVFBCrCYVWaz60BeqZKjgz1dEgAAAOC1nA5Xr7zyil577TWNGTPGcWzEiBHq0qWLpk6dSrjyIX4Ws9rGhmhbVqF25BQRrgAAAIBTcHpY4KFDh9SvX786x/v166dDhw65pCh4j9T4EElSBvOuAAAAgFNyOlylpaXpgw8+qHN87ty5at++vUuKgvdIY8VAAAAAoEGcHhY4bdo0jRo1St98841jztXy5cu1dOnSekMXmrbUY4taZOQQrgAAAIBTcbrnauTIkVq9erViY2M1b948zZs3T7GxsVq9erWuu+66xqgRHsRy7AAAAEDDONVzZbPZ9Ic//EGTJ0/Wf/7zn8aqCV6kXWyoTCbpaIlNuUXligm1erokAAAAwCs51XPl7++vjz/+uLFqgRcKCrCoZWSQJHqvAAAAgFNxeljgtddeq3nz5jVCKfBWaY55V8UergQAAADwXk4vaNG+fXs98cQTWr58uXr27KmQkJBaz0+YMMFlxcE7pMWFKn1bDj1XAAAAwCk4Ha7++c9/KjIyUmvWrNGaNWtqPWcymQhXPqhmxcAdrBgIAAAAnJTT4WrXrl2NUQe8mGNYID1XAAAAwEk5PefqiSeeUElJSZ3jpaWleuKJJ1xSFLxLzUbCB/JKVVxe6eFqAAAAAO/kdLiaNm2aiorq9mCUlJRo2rRpThcwc+ZMpaSkKDAwUH369NHq1atPeu6mTZs0cuRIpaSkyGQyacaMGXXOmTp1qkwmU62vjh07Ol0XjosKCVBMSIAkaddhFrUAAAAA6uN0uDIMQyaTqc7xn376SdHR0U5da+7cuZo4caKmTJmitWvXqnv37hoyZIiys7PrPb+kpETt2rXTM888o8TExJNet0uXLjp06JDj67vvvnOqLtSVymbCAAAAwCk1eM5VVFSUoyfonHPOqRWwqqqqVFRUpDvvvNOpN3/xxRd1++23a9y4cZKk119/XQsWLNCsWbP0yCOP1Dn/ggsu0AUXXCBJ9T5fw8/P75ThC85LjQvV6l1HCFcAAADASTQ4XM2YMUOGYeh3v/udpk2bpoiICMdzAQEBSklJUd++fRv8xhUVFVqzZo0mTZrkOGY2mzVo0CCtWLGiwdepz/bt29WiRQsFBgaqb9++mj59ulq3bn3S88vLy1VeXu54XFBQIEmy2Wyy2WxnVYuvaBtTvZHwL1kFLm2TmmvRzu5Dm7sX7e1+tLn70ebuRXu7H23uft7U5s7UYDIMw3Dm4suWLVO/fv3k7+/vdGEnOnjwoFq2bKnvv/++Vih76KGHtGzZMq1ateqUr09JSdF9992n++67r9bxL774QkVFRerQoYMOHTqkadOm6cCBA9q4caPCwsLqvdbUqVPrnS/27rvvKjg42PkP54O25Jn0+haLEoMMTepR5elyAAAAALcoKSnRTTfdpPz8fIWHh5/yXKeXYr/kkktkt9v1yy+/KDs7W3a7vdbzF198sbOXdKlhw4Y5fu7WrZv69OmjNm3a6IMPPtDvf//7el8zadIkTZw40fG4oKBAycnJGjx48GkbsLnokVeq17d8q9wKswYPuUJ+Fqen69XLZrNp8eLFuuKKK846sKNhaHP3or3djzZ3P9rcvWhv96PN3c+b2rxmVFtDOB2uVq5cqZtuukl79uzRrzu9TCaTqqoa1qsRGxsri8WirKysWsezsrJcOl8qMjJS55xzjnbs2HHSc6xWq6xWa53j/v7+Hv/H9BbJMX4K8reo1Falg4U2pR5bnt1VaGv3o83di/Z2P9rc/Whz96K93Y82dz9vaHNn3t/p7oc777xTvXr10saNG3XkyBEdPXrU8XXkyJEGXycgIEA9e/bU0qVLHcfsdruWLl3q1Nyt0ykqKlJGRoaSkpJcds3myGw2KTU+RBIrBgIAAAD1cbrnavv27froo4+UlpZ21m8+ceJEjR07Vr169VLv3r01Y8YMFRcXO1YPHDNmjFq2bKnp06dLql4EY/PmzY6fDxw4oPXr1ys0NNRRz4MPPqirr75abdq00cGDBzVlyhRZLBaNHj36rOtt7tLiQrXxQIEycghXAAAAwK85Ha769OmjHTt2uCRcjRo1Sjk5OXr88ceVmZmpHj16aNGiRUpISJAk7d27V2bz8c61gwcP6rzzznM8fuGFF/TCCy/okksuUXp6uiRp//79Gj16tHJzcxUXF6cBAwZo5cqViouLO+t6m7s09roCAAAATsrpcPXHP/5RDzzwgDIzM9W1a9c6YxC7devm1PXGjx+v8ePH1/tcTWCqkZKSUmee16+9//77Tr0/Gq5mnlUG4QoAAACow+lwNXLkSEnS7373O8cxk8kkwzCcWtACTU9Nz1VGTrHj3xsAAABANafD1a5duxqjDjQBbWJCZDGbVFReqcyCMiVFBHm6JAAAAMBrOB2u2rRp0xh1oAkI8DOrTUywduYUKyO7mHAFAAAAnKDBS7HffffdKio6PtfmvffeU3FxseNxXl6ehg8f7trq4HXS4moWtSj0cCUAAACAd2lwuHrjjTdUUlLiePyHP/yh1gbA5eXl+vLLL11bHbxOas2KgSzHDgAAANTS4HD161X6TrdqH3zT8Z4rwhUAAABwogaHK0CqvWIgAAAAgOMIV3BKzbDAnMJy5ZfaPFwNAAAA4D2cWi3w8ccfV3BwsCSpoqJCf/nLXxQRESFJteZjwXeFWv2UGB6ozIIy7cguUs82UZ4uCQAAAPAKDQ5XF198sbZt2+Z43K9fP+3cubPOOfB95ySGKbOgTB+t2Ue4AgAAAI5pcLhKT09vxDLQlNxxUTt9uz1H763epwvbxeiaHi09XRIAAADgcWc152r58uUqLy93VS1oIga0j9X4y9IkSY/+d4MyWJYdAAAAOLtwNWzYMB04cMBVtaAJuW/QObqwXbSKK6p0zztrVWar8nRJAAAAgEedVbhir6vmy2I26eUbz1NsaIC2ZhZq2mebPV0SAAAA4FEsxY4zFh8eqBmjzpPJJL23eq/mr6cXEwAAAM3XWYWrN954QwkJCa6qBU3QgPax+uPl7SVJk5h/BQAAgGbsrMLVTTfdpKqqKs2bN09btmxxVU1oYu4d2F4XtotWCfOvAAAA0Iw5Ha5uuOEGvfrqq5Kk0tJS9erVSzfccIO6deumjz/+2OUFwvv9ev7V1E83ebokAAAAwO2cDlfffPONLrroIknSJ598IsMwlJeXp5dffllPPfWUywtE0xAfHqiXbqyef/X+D/v0ybr9ni4JAAAAcCunw1V+fr6io6MlSYsWLdLIkSMVHBysK6+8Utu3b3d5gWg6+qfFasKx+Vd//mSjdmQz/woAAADNh9PhKjk5WStWrFBxcbEWLVqkwYMHS5KOHj2qwMBAlxeIpmXCwPbq2y7GMf+qtIL5VwAAAGgenA5X9913n26++Wa1atVKLVq00KWXXiqperhg165dXV0fmhiL2aSXRvdQbKhV27KYfwUAAIDmw+lwdffdd2vFihWaNWuWvvvuO5nN1Zdo164dc64gSYoPC9RLN/aQySTN/XGf/ruW+VcAAADwfWe0FHuvXr103XXXKTQ0VFVVVVq/fr369eun/v37u7o+NFH902J178AT518VergiAAAAoHGd0bDAf/7zn5KkqqoqXXLJJTr//POVnJys9PR0V9eHJuyPl7dXv9QYldqqdM8765h/BQAAAJ/mdLj66KOP1L17d0nSZ599pl27dmnr1q26//779ec//9nlBaLpsphNmnHj8flXUz7d6OmSAAAAgEbjdLg6fPiwEhMTJUkLFy7U9ddfr3POOUe/+93vtGHDBpcXiKYtPixQL9/YQ2aT9MGP+/XxGuZfAQAAwDc5Ha4SEhK0efNmVVVVadGiRbriiiskSSUlJbJYLC4vEE1fv7RY3TvwHEnSY/OYfwUAAADf5HS4GjdunG644Qade+65MplMGjRokCRp1apV6tixo8sLhG8Yf3ma+qdVz7+6m/2vAAAA4IOcDldTp07VW2+9pTvuuEPLly+X1WqVJFksFj3yyCMuLxC+wWI2acao8xQXZtUvWUV6fD7zrwAAAOBb/M7kRb/97W/rHBs7duxZFwPfFhdm1Us39tD/vbVKH67Zrz7tYnRNtwRPlwUAAAC4xBntc7Vs2TJdffXVSktLU1pamkaMGKFvv/3W1bXBB/VLjdV9g6rnX02et1Hbs4s8XBEAAADgGk6Hq//85z8aNGiQgoODNWHCBE2YMEFBQUEaOHCg3n333caoET7mnsvSNCAtVqW2Kk14/yeVM/0KAAAAPsDpYYF/+ctf9Nxzz+n+++93HJswYYJefPFFPfnkk7rppptcWiB8j8Vs0t9G9dDwl7/VjpxifSSzrvN0UQAAAMBZcrrnaufOnbr66qvrHB8xYoR27drlkqLg++LCrHr5xvNkNkmrc8z6eO0BT5cEAAAAnBWnw1VycrKWLl1a5/iSJUuUnJzskqLQPPRNjdGEy9MkSVM/36Jfstj/CgAAAE2X08MCH3jgAU2YMEHr169Xv379JEnLly/X22+/rZdeesnlBcK33XlxW33x4y/ali/d885azR/fX8EBZ7SIJQAAAOBRTv8Ve9dddykxMVF//etf9cEHH0iSOnXqpLlz5+qaa65xeYHwbRazSbe0t+vlbUHanl2kyfM26a83dPd0WQAAAIDTnApXlZWVevrpp/W73/1O3333XWPVhGYmzF968fquGjP7R328dr8ubBet63sxxBQAAABNi1Nzrvz8/PTcc8+psrKysepBM9WnbbQmXnFs/6v5G5l/BQAAgCbH6QUtBg4cqGXLljVGLWjm7r40TRe1j1WZza6731mrkgpCPAAAAJoOp+dcDRs2TI888og2bNignj17KiQkpNbzI0aMcFlxaF7Mx/a/uvLlb7Uju0iPzduov17fXSaTydOlAQAAAKfldLi6++67JUkvvvhinedMJpOqqqrOvio0W7Gh1ftfjX5zpf679oAubBejG5h/BQAAgCbA6WGBdrv9pF8EK7hCn3YxemBwB0nS4/M3alsm868AAADg/ZwOV4A73HVJ6gnzr9aouJz5VwAAAPBuDQ5X//vf/9S5c2cVFBTUeS4/P19dunTRN99849Li0HyZzSbNGNVDCeFWZeQUa/K8jTIMw9NlAQAAACfV4HA1Y8YM3X777QoPD6/zXEREhP7whz/ob3/7m0uLQ/MWE2rVK6PPl9kk/XfdAX34435PlwQAAACcVIPD1U8//aShQ4ee9PnBgwdrzZo1LikKqNG7bbRj/tXk+Ru1NbNuzykAAADgDRocrrKysuTv73/S5/38/JSTk+OSooAT3XVJqi4+J07llXbd885a5l8BAADAKzU4XLVs2VIbN2486fM///yzkpKSXFIUcCKz2aS/3dBdieGBysgp1mPMvwIAAIAXanC4Gj58uCZPnqyysrI6z5WWlmrKlCm66qqrXFocUCMm1KpXbjpPFrNJn6w7oA9+3OfpkgAAAIBaGhyuHnvsMR05ckTnnHOOnnvuOc2fP1/z58/Xs88+qw4dOujIkSP685//3Ji1opm7ICVaDww+R5L0+PxNzL8CAACAV/Fr6IkJCQn6/vvvddddd2nSpEmOYVkmk0lDhgzRzJkzlZCQ0GiFApJ058WpWr3riNK35ejud9bqs/EDFGJt8G0MAAAANBqn/ipt06aNFi5cqKNHj2rHjh0yDEPt27dXVFRUY9UH1GI2m/TiDT00/KVvtTOnWH/+ZIP+NqqHTCaTp0sDAABAM9fgYYEnioqK0gUXXKDevXsTrOB20SEBjvlX89Yf1NwfmH8FAAAAzzujcAV42gUp0Xrw2P5XUz7dpC2HmH8FAAAAzyJcocn6w8XtdFmH4/tfFbH/FQAAADyIcIUmy2w26a839FBSRKB2Hq6ef8X+VwAAAPAUwhWatOiQAL0yunr+1fz1B/XE55tVUWn3dFkAAABohghXaPJ6pUTrsSs7SZJmL9+tUf9YoQN5pR6uCgAAAM0N4Qo+YVz/tvrHLT0VHuindXvzdOXL3+rrrdmeLgsAAADNCOEKPmNwl0QtmHCRurWKUF6JTePe/kHPLtqqyiqGCQIAAKDxEa7gU5Kjg/XhnX11a78USdJr6Rm66c1Vyioo82xhAAAA8HmEK/gcq59FU0d00cybzleo1U+rdx/R8Je+1bfbczxdGgAAAHwY4Qo+68puSfrsjwPUKSlcucUVGjNrtf62+BdV2VmuHQAAAK5HuIJPaxsbok/u7qfRvVvLMKSXlm7XmFmrlFNY7unSAAAA4GMIV/B5gf4WTf9NV/1tVHcF+Vu0fEeurnz5W63cmevp0gAAAOBDCFdoNq47r5U+Hd9f7eNDlV1YrpveXKmZX++QnWGCAAAAcAHCFZqV9glhmj++v35zfkvZDen5L7fpd3N+0NHiCk+XBgAAgCaOcIVmJzjAT3+9vrueG9lNVj+z0rfl6MqXv9WaPUc9XRoAAACaMMIVmiWTyaQbLkjWvHv6q21siA7ml2nUGyv01rc7ZRgMEwQAAIDzCFdo1jolhevT8f11VbckVdoNPbVgi/7w7zXKL7F5ujQAAAA0MR4PVzNnzlRKSooCAwPVp08frV69+qTnbtq0SSNHjlRKSopMJpNmzJhx1tcEwgL99cro8/TkNV0UYDHrq81ZuurVb/Xz/jxPlwYAAIAmxKPhau7cuZo4caKmTJmitWvXqnv37hoyZIiys7PrPb+kpETt2rXTM888o8TERJdcE5Cqhwne0jdFH9/VT8nRQdp3pFS/fW2F/rViN8MEAQAA0CAeDVcvvviibr/9do0bN06dO3fW66+/ruDgYM2aNave8y+44AI9//zzuvHGG2W1Wl1yTeBEXVtF6PM/XqTBnRNUUWXX4/M3afx761RYxjBBAAAAnJqfp964oqJCa9as0aRJkxzHzGazBg0apBUrVrj1muXl5SovL3c8LigokCTZbDbZbPxR3Zhq2teb2jnYT3r1xm56e8VePfflL1rw8yFtOpCvl0d1V6ekME+Xd9a8sc19Ge3tfrS5+9Hm7kV7ux9t7n7e1ObO1OCxcHX48GFVVVUpISGh1vGEhARt3brVrdecPn26pk2bVuf4V199peDg4DOqBc5ZvHixp0uoI0HSHztLs3+xaHduiUa+9r1GtrXrwnhDJpOnqzt73tjmvoz2dj/a3P1oc/eivd2PNnc/b2jzkpKSBp/rsXDlTSZNmqSJEyc6HhcUFCg5OVmDBw9WeHi4ByvzfTabTYsXL9YVV1whf39/T5dTr9ElFfrTxxu17JfDen+nRWVhSZo2opOCA5rmfz5Noc19Ce3tfrS5+9Hm7kV7ux9t7n7e1OY1o9oawmN/HcbGxspisSgrK6vW8aysrJMuVtFY17RarfXO4fL39/f4P2Zz4c1tHR/hr9m39tbr32Tor1/9onk/HdKmQ4X6+83nq31C0x0m6M1t7otob/ejzd2PNncv2tv9aHP384Y2d+b9PbagRUBAgHr27KmlS5c6jtntdi1dulR9+/b1mmsCkmQ2m3T3pWl697Y+ig+zant2kUa8ulyfrNvv6dIAAADgJTy6WuDEiRP15ptvas6cOdqyZYvuuusuFRcXa9y4cZKkMWPG1FqcoqKiQuvXr9f69etVUVGhAwcOaP369dqxY0eDrwmcjT7tYrTw3os0IC1WpbYq3T/3Jz3y8c8qs1V5ujQAAAB4mEcnjYwaNUo5OTl6/PHHlZmZqR49emjRokWOBSn27t0rs/l4/jt48KDOO+88x+MXXnhBL7zwgi655BKlp6c36JrA2YoNtWrO73rrlf9t10tLt+v9H/Zp/b48/f3m89UuLtTT5QEAAMBDPD4jf/z48Ro/fny9z9UEphopKSkN2tD1VNcEXMFiNum+QeeoV5to3fv+Om3NLNSIV5frmZFddVW3Fp4uDwAAAB7g0WGBQFM3oH2sFt57kXq3jVZReaXGv7tOU+ZvVHklwwQBAACaG8IVcJYSwgP17m19dPelqZKkOSv26PrXV2jfkYbviQAAAICmj3AFuICfxayHhnbU7FsvUGSwv37en68rX/5WX23K9HRpAAAAcBPCFeBCl3WM14IJF+m81pEqKKvUHf9eownvrdPuw8WeLg0AAACNjHAFuFjLyCDNvaOvbhvQVpL06U8HNejFZfrzJxuUVVDm4eoAAADQWAhXQCMI8DPrsas66/M/DtClHeJUaTf0zqq9uuT5r/XMF1uVX2LzdIkAAABwMcIV0IjObRmht8f11tw7LlTPNlEqs9n1+rIMDXjuf5r59Q6VVFR6ukQAAAC4COEKcIM+7WL00Z199c+xvdQxMUyFZZV6/sttuvi5dP1rxW5VVNo9XSIAAADOEuEKcBOTyaSBnRK0cMJFmjGqh5Kjg3S4qFyPz9+kgS+ma966A7LbT79JNgAAALwT4QpwM7PZpGvPa6mlEy/Vk9d0UWyoVfuOlOq+ues1/OVvtXRLlgyDkAUAANDUEK4ADwnwM+uWvin65qFL9achHRQW6KetmYX6/Zwf9dvXV2jVzlxPlwgAAAAnEK4ADwsO8NM9l6Xp24cu052XpCrQ36w1e45q1D9W6tbZq7XpYL6nSwQAAEADEK4ALxEZHKBHhnXUsj9dppv7tJaf2aT0bTm68uXv9Ec2IgYAAPB6hCvAyySEB+ov13XVkomXaET3FpKkz45tRPwoGxEDAAB4LcIV4KVSYkP08ujztGDCAF12bCPid1ft1cXPfa3pX2xRXkmFp0sEAADACQhXgJfr0iJCs8f11gd/6KtebaJUXmnXG8t26qLnvmYjYgAAAC9CuAKaiN5to/XhnX0161Y2IgYAAPBGhCugCTGZTLq8Y/VGxC/d2EOto4NrbUT8ybr9qmIjYgAAAI8gXAFNkNls0jU9WmrJxEv05LXnKi6seiPi++f+pOEvfaslm9mIGAAAwN0IV0ATFuBn1i0XttGyP12qh4Z2UHign7ZlFeq2f/2oka99z0bEAAAAbkS4AnxAcICf7r40Td8+dLnuurR6I+K1e/M06h8rNXbWam08wEbEAAAAjY1wBfiQiGB/PTy0o77502X6vwurNyJe9kuOrnrlO41/d612sRExAABAo/HzdAEAXC8+PFBPXdtVt1/UTi8u/kWf/nRQn/98SF9szNRvz2+pDlWerhAAAMD30HMF+LA2MSF66cbztOCPF+nyjvGqshua++N+PbnOoj/8Z52++SVHdlYXBAAAcAl6roBmoHOLcM269QL9sPuIXlryi77bkav/bcvR/7blqF1ciMZc2EYje7ZSWKC/p0sFAABosui5ApqRC1KiNXtsTz3ao1JjLmytUKufduYUa+pnm3Xh00v1+PyN2pFd6OkyAQAAmiTCFdAMJQRJk6/sqJWPDtST13RRWnyoiiuq9K8VezToxW9081sr9dWmTDYkBgAAcALDAoFmLNTqp1v6puj/Lmyj7zNyNef73VqyJUvLd+Rq+Y5ctYwM0v9d2EY3XpCsqJAAT5cLAADg1QhXAGQymdQ/LVb902K1/2iJ3lm1V++v3qsDeaV6dtFWzVjyi0Z0b6Gx/VJ0bssIT5cLAADglQhXAGppFRWsh4d21L0D2+uznw5qzord2nigQB+u2a8P1+xXzzZRGtO3jYadm6QAP0YWAwAA1CBcAahXoL9F1/dK1m97ttLavXn614rdWrjhkNbsOao1e47qydAtuqlPa93cp7USwgM9XS4AAIDHEa4AnJLJZFLPNlHq2SZKf76yk95btU/vrNqj7MJyvbx0u/7+9Q4NPTdRY/ulqFebKJlMJk+XDAAA4BGEKwANFh8WqHsHtdfdl6Xqy02ZmvP9bv2w+6g+//mQPv/5kDonhWtsvzYa0b2lggIsni4XAADArQhXAJzmbzHrqm4tdFW3Ftp0MF//XrFH89Yf0OZDBXr44w16euFWjbogWbdc2EbJ0cGeLhcAAMAtmI0O4Kx0aRGhZ0Z208pJA/Xo8I5qFRWk/FKb/vHNTl38/Ne6bc4P+uaXHNnZMwsAAPg4eq4AuERkcIDuuDhVvx/QTl9vzdacFbv17fbDWrIlW0u2ZKtdXIjGXNhGI3u2Uligv6fLBQAAcDnCFQCXsphNGtQ5QYM6Jygjp0j/XrFHH63Zr505xZr62WY9/+U2jezZSmP6tlFafJinywUAAHAZhgUCaDSpcaGaOqKLVj46UE9e00Vp8aEqrqjSv1bs0aAXv9HNb63UV5syVcWQQQAA4APouQLQ6EKtfrqlb4r+78I2+j4jV3O+360lW7K0fEeulu/IVcvIIP3m/JYa0iVRXVqEs5w7AABokghXANzGZDKpf1qs+qfFat+REr2zaq/e/2GvDuSV6pX/7dAr/9uhVlFBGtolUUPPTdT5raNkNhO0AABA00C4AuARydHBemRYR903qL2+3JSpRRszlb4tR/uPluqt73bpre92KS7MqsGdEzT03ERd2C5G/hZGMgMAAO9FuALgUYH+Fl3To6Wu6dFSpRVVWvZLjr7clKklW7KUU1iud1bt1Tur9io80E+DOidoaJdEXXxOnAL92aQYAAB4F8IVAK8RFGDR0HOrhwRWVNq1YmeuFm3M1OLNmTpcVKH/rj2g/649oCB/iy7rGKchXRJ1ecd4lnYHAABegXAFwCsF+Jl1yTlxuuScOD117blas+eoFm3M1JebMnUgr1QLN2Rq4YZMBVjM6p8Wo6HnJmpQpwTFhFo9XToAAGimCFcAvJ7FbFLvttHq3TZak6/qpI0HCrRo0yF9sTFTO3OK9fW2HH29LUdm0wb1bhutoV0SNeTcRCVFBHm6dAAA0IwQrgA0KSaTSV1bRahrqwj9aUhH7cgu1KKNmVq0KVMbDxRo5c4jWrnziKZ+tlndkyMdKw+2jQ3xdOkAAMDHEa4ANGlp8WEaf3mYxl/eXvuOlOjLTdVDB3/cc1Q/7cvTT/vy9OyireqQEKYh5yZqaJdEdUoKYy8tAADgcoQrAD4jOTpYt13UTrdd1E7ZhWVavDlLizZmakVGrrZlFWpbVqFeXrpdraODNfTcRA3pkqjzkiPZSwsAALgE4QqAT4oPC9TNfdro5j5tlF9i09Kt1UFr2S852nukRP/4Zqf+8c1OxYdZNeTY0MHebaPZSwsAAJwxwhUAnxcR7K/fnN9Kvzm/lUoqKrVsW44WbcrU0i3Zyi4s179X7tG/V+5RZLC/BnWq3ktrQPtY9tICAABOIVwBaFaCA/w0rGuShnVNUnlllb7fcWwvrS1ZOlJcoY/W7NdHa/YrJMCivqkx6tM2Rn3aRatzUrj86NUCAACnQLgC0GxZ/Sy6rGO8LusYr79U2fXD7qP6clOmFm3MVGZBmZZsydaSLdmSpFCrn3q2iVKfdtHq0zZG3VpFMIQQAADUQrgCAEl+FrP6psaob2qMHr+qszYezNfKnblatfOIVu8+osKySi37JUfLfsmRJAX5W9SzTZR6t41Wn7bR6p4cyTBCAACaOcIVAPyK2WxSt1aR6tYqUndcnKoqu6Ethwq0atcRrd6Vq9W7juhoiU3f7Tis73YcliQF+Jl1XnKk+rSL0YVto3Ve6ygFBRC2AABoTghXAHAaFrNJ57aM0LktI/T7AW1ltxvanl2kVbuqe7ZW7Tqiw0XlWrWr+ueXJflbqgPaBW0ipTyTLimvVKS/v6c/CgAAaESEKwBwktlsUofEMHVIDNOYvikyDEM7DxcfC1rVgSuzoExr9hzVmj1HJVn05tNf69yWEbqwbbR6t41Wr5RoRQQRtgAA8CWEKwA4SyaTSalxoUqNC9VNfVrLMAztO1KqlbtytSLjsJZtPqAj5dJP+/L00748vfHNTplMUuekcMdqhL1TohUVEuDpjwIAAM4C4QoAXMxkMql1TLBaxwTruu6JWhi4Vz36Xaa1+wscwwh3HS7WpoMF2nSwQLOW75IkdUgIc6xG2LtttOLCrB7+JAAAwBmEKwBwgxaRQWoTF67rzmslScoqKHMskLFq5xFtzy7StqxCbcsq1L9W7JEkpcaFqE+7GPVpWx24EiMCPfkRAADAaRCuAMADEsIDNaJ7C43o3kKSlFtUrtXHFsRYuTNX27IKlZFTrIycYr27aq8kqU1MsHq1iVaP1pE6LzlSHRPD2NgYAAAvQrgCAC8QE2rVsK5JGtY1SZKUV1KhH3Yf1aqduVq164g2HczXntwS7ckt0cdr90uSAv3N6tYyUue1jlSP5Eid1zqK3i0AADyIcAUAXigyOEBXdE7QFZ0TJEmFZTb9uOeo1u3N07q9R/XTvjwVlFVq9e7qTY5rJIYH1gpbXVtGsN8WAABuQrgCgCYgLNBfl3WI12Ud4iVJdnv18u/r9h7V+n15Wrc3T9uyCpVZUKYvNmbqi42Zkqr36OqYGOYIWz2SI9UuNkRms8mTHwcAAJ9EuAKAJshsNiktPlRp8aG6vleyJKmkolIb9udr3b48rd+bp3X7jiqroNyxKuE7x+ZuhQf6qcexoHVe60j1aBXJMvAAALgA4QoAfERwgF/16oLtYhzHDuWXat3evGO9W0e14UC+Csoq9c0vOfrmlxzHeW1jQ46HreRIdUwMV4Afi2UAAOAMwhUA+LCkiCAldQ3S8GMLZdiq7NqWWah1e486erh2Hi7WrmNfn6w7IEmy+pl1bssInZccWb06YesotYgIlMnEcEIAAE6GcAUAzYi/pTo0ndsyQrf0rT6WV1Kh9fvyHHO31u/LU36pTWv2HNWaPUcdr40Ps9aau9WtVYRCrPzPCAAANfhfRQBo5iKDA3Rph3hdemyxDMMwtOtw8fHhhPuOauuhQmUXluurzVn6anOWJMlsks5JCFP3VpHq0jJcXVqEq2NiOIELANBs8b+AAIBaTCaT2sWFql1cqEb2bCVJKq2o0saD+Y6FMtbtzdOh/DJtzSzU1sxC6cea11bP3+rSIkJdWoQf+4pQNAtmAACaAcIVAOC0ggIsuiAlWhekRDuOZeaXaf2+o9p4oECbDuZr08ECZReWa2dOsXbmFOuznw46zk0MD3SErc7HglerqCDmcAEAfArhCgBwRhIjAjU0IklDz01yHMspLHcErc0Hq0PX7twSZRaUKbOgTEu3ZjvOjQjyV+ekY71bLat7uNrFhsjPwiqFAICmiXAFAHCZuDBrrflbklRUXqkthwq06UC+Y8+t7dmFyi+1acXOXK3Ymes41+pnVsdjgasmeHVKClegv8UTHwcAAKd4RbiaOXOmnn/+eWVmZqp79+565ZVX1Lt375Oe/+GHH2ry5MnavXu32rdvr2effVbDhw93PH/rrbdqzpw5tV4zZMgQLVq0qNE+AwCgfqFWvzpDCisq7folq9DRu7XpYIG2HCpQcUWVftqXp5/25TnONZuk1LhQx/ytmu8Rwf4e+DQAAJycx8PV3LlzNXHiRL3++uvq06ePZsyYoSFDhmjbtm2Kj4+vc/7333+v0aNHa/r06brqqqv07rvv6tprr9XatWt17rnnOs4bOnSoZs+e7XhstVrd8nkAAKcX4Hd8SXgpWZJktxvac6TEEbaqhxbm63BRhbZnF2l7dpHmrT8+j6tlZFDtwNUyXInhgR76RAAAeEG4evHFF3X77bdr3LhxkqTXX39dCxYs0KxZs/TII4/UOf+ll17S0KFD9ac//UmS9OSTT2rx4sV69dVX9frrrzvOs1qtSkxMdM+HAACcNbPZpLaxIWobG6KrurWQVL0sfHbNPK4D1YFr06F87TtSqgN51V81S8NLUnRIgDomhspaYlbxmv3qkBShtLgwerkAAG7h0XBVUVGhNWvWaNKkSY5jZrNZgwYN0ooVK+p9zYoVKzRx4sRax4YMGaJ58+bVOpaenq74+HhFRUXp8ssv11NPPaWYmJh6r1leXq7y8nLH44KCAkmSzWaTzWY7k4+GBqppX9rZfWhz96K9z150kEUXpUbrotTjwwoLSm3aklmozYcKteVQgTYfKtSOnGIdKa7Q9xlHJJn19bzNjvNjQwOUGhdy7CvU8XNCmJUVC12A+9y9aG/3o83dz5va3JkaPBquDh8+rKqqKiUkJNQ6npCQoK1bt9b7mszMzHrPz8zMdDweOnSofvOb36ht27bKyMjQo48+qmHDhmnFihWyWOpOip4+fbqmTZtW5/hXX32l4ODgM/locNLixYs9XUKzQ5u7F+3dOBIkJQRJl7aTbCnSoRJpf7FJmSUmZZVKmaUm5VWYdLioQoeLKrRq19Farw+0GEoIkhKCjGNf1T/HBlbP9YJzuM/di/Z2P9rc/byhzUtKShp8rseHBTaGG2+80fFz165d1a1bN6Wmpio9PV0DBw6sc/6kSZNq9YYVFBQoOTlZgwcPVnh4uFtqbq5sNpsWL16sK664Qv7+DNtxB9rcvWhv9/t1mxeVV2pnTrEyHF9Fysgp1t6jpSqrkvYUSXuKaicpf4tJ7WJD6vR2tY0JlpWVC+vgPncv2tv9aHP386Y2rxnV1hAeDVexsbGyWCzKysqqdTwrK+uk86USExOdOl+S2rVrp9jYWO3YsaPecGW1Wutd8MLf39/j/5jNBW3tfrS5e9He7lfT5lH+/uoZGqSebWNrPV9eWaU9uSXakV1U62vn4SKV2ezallWkbVlFtV5jNknJ0cFKiwtVWnyoUuOrv6fFhyo8kH9f7nP3or3djzZ3P29oc2fe36PhKiAgQD179tTSpUt17bXXSpLsdruWLl2q8ePH1/uavn37aunSpbrvvvscxxYvXqy+ffue9H3279+v3NxcJSUlnfQcAEDzYvWz6JyEMJ2TEFbruN1u6EBeae3QlVP9Pb/Upj25JdqTW1JrQ2RJig+zOoJWWnyoI4DFMa8LAJoNjw8LnDhxosaOHatevXqpd+/emjFjhoqLix2rB44ZM0YtW7bU9OnTJUn33nuvLrnkEv31r3/VlVdeqffff18//vij/vGPf0iSioqKNG3aNI0cOVKJiYnKyMjQQw89pLS0NA0ZMsRjnxMA0DSYzSYlRwcrOTpYl3U8viWIYRjKKSrXjuwiZfwqdGUVlCu7sPrr+4zcWtcLC/RzhK2U2BC1iw1RSmyIUmJCFBTAEEMA8CUeD1ejRo1STk6OHn/8cWVmZqpHjx5atGiRY9GKvXv3ymw2O87v16+f3n33XT322GN69NFH1b59e82bN8+xx5XFYtHPP/+sOXPmKC8vTy1atNDgwYP15JNPstcVAOCMmUwmxYcFKj4sUP1Saw8xLCiz1QpcNT/vPVKiwrJKrdubp3V78+pcMykiUG2Pha12xwJX27gQJUcFK8DPXOd8AIB383i4kqTx48efdBhgenp6nWPXX3+9rr/++nrPDwoK0pdffunK8gAAOKXwQH+d1zpK57WOqnW8zFal3bnF1XO5coq1+3Cxdh4u1q7DxcovtelQfpkO5ZfV6e2ymE1qFRVUHbxiQtQu7ljwig1Ri8ggWVjKEAC8kleEKwAAfFGgv0UdE8PVMbHuyrNHiyu083B14Np1uFi7cou1K6dYu3OLVVJR5ZjbJeXUel2Axaw2McG1hhjWbL4cz/wuAPAowhUAAB4QFRKgniEB6tmmdm+XYRjKLiyvDly/+tqbW6KKKru2Zxdpe3ZRnWuGBFiq53P9aphh25gQRYUEuOujAUCzRbgCAMCLmEwmJYQHKiE8UBe2i6n1XJXd0MG80jqha3dusfYdKVFxRZU2HSzQpoN192SJDPavHmJ4Qm9Xm5hgJUUEKTY0gB4vAHABwhUAAE2E5YSVDC8+J67WcxWVdu07WqJdOXWHGR7KL1NeiU3rS/K0fl9enesG+JmVFBGopIhAtYgIUlJkoJIigtSi5ntEkMKD/AhgAHAahCsAAHxAgJ9ZqXGhSo0LrfNcaUX1whq1ersOF2vvkRLlFJWrotJ+whyv+gUHWKrDV2SQ43tNEIsL8Vd5VWN+OgBoGghXAAD4uKAAizolhatTUt2FNSoq7coqKDu2cmGpDuaV6WBeqePnQ/mlOlpiU0lFlTJyipWRU3ySd/HTXzb871iPV1C9QSwhwiqrH3t7AfBdhCsAAJqxAD+zY6jhyZRWVOlQfqkO5dcEr9pB7GB+qYrLq5RfWqn80kJtzSw86bViQ63Hhhv+aujhse/xYVb5WdjjC0DTRLgCAACnFBRgUbu4ULWrZ8ihJNlsNn386UJ17XORsosrdSivrFbPV00oK6+063BRuQ4Xlevn/fn1XstiNikxPPB4j1dkdfBqEXH854ggf+Z/AfBKhCsAAHDWgvykcxLC1MXfv97nDcPQkeKKWr1fB/NLawWxrIIyVdoNHcgr1YG8UmnP0XqvFRxgcQw5bBkZVOfnxIhABfoz/BCA+xGuAABAozOZTIoJtSom1KpzW0bUe06V3VBOYbkO5pdWB7C8Mh3IKz0exvJKlVtcoZKKKu3ILtKOevb6qhEbGlBr0Y1fh7DYUKvMZnq/ALgW4QoAAHgFi9mkxIhAJUYE6vzWUfWeU2arcgQtR/DKq+4FO3Ds51JblQ4XVehwUcVJhx/6W6rfq0VE0PHgFVk9FLHlsRAWFlh/LxwAnAzhCgAANBmB/ha1PbYJcn0Mw1Beia06aB0LYdWLbhz/OaugTLYqQ/uOlGrfkdKTvldYoJ8jaNU3/yshPFABfiy+AeA4whUAAPAZJpNJUSEBigoJOOnww8oqu7IKy48Hr7y6ISy/1KbCskptzTz56ocmkxQXaq130Y3qIYhBig0NYPENoBkhXAEAgGbFz2JWy2PD/06muLxSh/JLdaBm36+8E37Orw5hFZV2ZReWK7uwXOv31X+dAD9zdc9XneBVPfcrKTJIoVb+HAN8Bf81AwAA/EqI1U9p8WFKiw+r93nDMJRbXFGn5+tQftmxIYmlyi4sV0WlXXtyS7Qnt+Sk7xUe6Fdr2GHSifPAjs1B82fvL6BJIFwBAAA4yWQyKTbUqthQq7q1qv+cikq7sgqOb7R8MK+s1sqHB/NKVVBWWf11muGH8WFWx+qHieEBOnLIJNv6g4oJC1JEsL8ig/wVFRyg8CB/WVgFEfAYwhUAAEAjCPAzKzk6WMnRwSc9p6i88tiQw+ObLh84YQXEQ3llqqiyK6ugXFkF5VqnvGOvtOiT3RvrvWZ4oJ8igwMUFeyviOAARQb5KzLYX5G1fj7xcYDCA/3kR+8YcNYIVwAAAB4SavVT+4QwtU+of/ih3V49/PDQsb2/DuSV6cCRYq3btktBkbEqKKtUXolN+SU2FZZXSpKjN2zvEedqCQv0U1RwgCKD/RVxLHRFHesVqwlpUSH+igiqPicyqPo8QhlwHOEKAADAS5nNJsWFWRUXZlW3VpGSJJvNpoVGhoYP7yV//+N7cdmq7MovtVWHrdIK5ZXYdLTEprySCsfxoyf8nHfsnMKy6lBWWFapwjMMZZHB1cMS40Ktig8PVHyYVQknfg+3KiYkgCAGn0e4AgAA8AH+FrNjHpgzbFV2FZTalFcTukoqjoUvm/JLKpRXWn9I+3UoO9WeYZJkNkkxoVYlhFsVHxaohHCr4o59jz/he2woIQxNF+EKAACgGfO3mBUTalWMk6Gsssp+bFhidQA7UlShnKJyZRWUVS9Rf+x7VkGZDhdVqMpuKKewXDmF5ZIKTnpdk0mKCakJYTU9X3V7wwhh8EaEKwAAADjNz2JWdEiAokMCTntuld1QbnG5sgvKlV1YpqyC6p+zCsscx7ILypVTVK4qu6HDReU6XFSuTae4Zk0Iqw5bJ/SGhQcqIez48MTokAAF+ltc98GBUyBcAQAAoFFZzCbFhwUqPixQUsRJz6uyGzpSXKGsgjLlHOv1yjohkOXUfP9VCNt86NTvH+hvVmTNQhzH5ofVrJhYvWhH7ccRx44F+NEzBucQrgAAAOAVLCcs4HEqNSEs+4Ser6xa36uHJeYUlqvSbqjMZlemrUyZBWVO1RNq9Tu2cqKfbMVmfVX4s6JCAxQVHKCIY3uL1aygGHUsnEWw11izRrgCAABAk3JiCOvS4uTnGYahwvJK5R9bhOPoiQt2nLB64lHHsepzCspsMozqfciKyit1IE+SzPolP7NB9YUH+ikqJMCxl1hN8KpZwj4yOMCx+XMEe435FMIVAAAAfJLJZFJ4oL/CA/1PuZnzr1XZDccKikdLKpRbWKplK35Um3M6q7CsqtYKio6QVs9eY3tyS5yqN8zqVx26avYaC6oOYdU/Hz9es9dYxLFjQf4WmUz0lnkDwhUAAABwAovZpKiQAEWFBKitQmSzhapkh6HhfdvU2lvs147vNVZ7n7GaAJZXWntJ+7wSmwpKj4eywvJKFZZXav/RUy9r/2sBFrPCjwWtmt6wmnljNQEsMti/+pyg48MX6S1zPcIVAAAA4AJnu9dYfmnN/mK24yHt2PH8mr3HHAGtUvmlFbJVGaqosjsW93BWWKCfI4DV9JaF13pc01vmf7wXLThAIQH0ltWHcAUAAAB40JnuNWYYhkoqqo73hJVWVA9nrBXEbMovPUVv2bFNoJ3tLfMzm+qGLsfjgFqPjw9nrD7X6ue7S+MTrgAAAIAmyGQyKcTqpxCrn1pEBjn12hN7y44eC1wn9orlHQtkBY6AdrxXraLKrkq7odziCuUWVzhdd83S+PUFs5ogFhpgVkZ+0+sZI1wBAAAAzczZ9JaV2Y7NLSutqDVc0TGcsfRYQCupcCwMUhPU7IYavDR+rNWse8/mQ3oA4QoAAABAg5hMJgUFWBQUYFFiRKBTr7Xbq5fGr90bdnzI4onHj5aUq7Iwt5E+ReMhXAEAAABodOYT5mklR5/6XJvNpoULF7qnMBdi7UUAAAAAcAHCFQAAAAC4AOEKAAAAAFyAcAUAAAAALkC4AgAAAAAXIFwBAAAAgAsQrgAAAADABQhXAAAAAOAChCsAAAAAcAHCFQAAAAC4AOEKAAAAAFyAcAUAAAAALkC4AgAAAAAXIFwBAAAAgAsQrgAAAADABQhXAAAAAOAChCsAAAAAcAHCFQAAAAC4gJ+nC/BGhmFIkgoKCjxcie+z2WwqKSlRQUGB/P39PV1Os0Cbuxft7X60ufvR5u5Fe7sfbe5+3tTmNZmgJiOcCuGqHoWFhZKk5ORkD1cCAAAAwBsUFhYqIiLilOeYjIZEsGbGbrfr4MGDCgsLk8lk8nQ5Pq2goEDJycnat2+fwsPDPV1Os0Cbuxft7X60ufvR5u5Fe7sfbe5+3tTmhmGosLBQLVq0kNl86llV9FzVw2w2q1WrVp4uo1kJDw/3+H84zQ1t7l60t/vR5u5Hm7sX7e1+tLn7eUubn67HqgYLWgAAAACACxCuAAAAAMAFCFfwKKvVqilTpshqtXq6lGaDNncv2tv9aHP3o83di/Z2P9rc/Zpqm7OgBQAAAAC4AD1XAAAAAOAChCsAAAAAcAHCFQAAAAC4AOEKAAAAAFyAcIVGM336dF1wwQUKCwtTfHy8rr32Wm3btu2Ur3n77bdlMplqfQUGBrqp4qZv6tSpddqvY8eOp3zNhx9+qI4dOyowMFBdu3bVwoUL3VRt05eSklKnvU0mk+655556z+f+dt4333yjq6++Wi1atJDJZNK8efNqPW8Yhh5//HElJSUpKChIgwYN0vbt20973ZkzZyolJUWBgYHq06ePVq9e3UifoOk5VZvbbDY9/PDD6tq1q0JCQtSiRQuNGTNGBw8ePOU1z+R3U3Nyuvv81ltvrdN+Q4cOPe11uc/rd7r2ru/3uslk0vPPP3/Sa3KPn1xD/h4sKyvTPffco5iYGIWGhmrkyJHKyso65XXP9Pd/YyNcodEsW7ZM99xzj1auXKnFixfLZrNp8ODBKi4uPuXrwsPDdejQIcfXnj173FSxb+jSpUut9vvuu+9Oeu7333+v0aNH6/e//73WrVuna6+9Vtdee602btzoxoqbrh9++KFWWy9evFiSdP3115/0NdzfzikuLlb37t01c+bMep9/7rnn9PLLL+v111/XqlWrFBISoiFDhqisrOyk15w7d64mTpyoKVOmaO3aterevbuGDBmi7OzsxvoYTcqp2rykpERr167V5MmTtXbtWv33v//Vtm3bNGLEiNNe15nfTc3N6e5zSRo6dGit9nvvvfdOeU3u85M7XXuf2M6HDh3SrFmzZDKZNHLkyFNel3u8fg35e/D+++/XZ599pg8//FDLli3TwYMH9Zvf/OaU1z2T3/9uYQBukp2dbUgyli1bdtJzZs+ebURERLivKB8zZcoUo3v37g0+/4YbbjCuvPLKWsf69Olj/OEPf3BxZc3Dvffea6Smphp2u73e57m/z44k45NPPnE8ttvtRmJiovH88887juXl5RlWq9V47733Tnqd3r17G/fcc4/jcVVVldGiRQtj+vTpjVJ3U/brNq/P6tWrDUnGnj17TnqOs7+bmrP62nzs2LHGNddc49R1uM8bpiH3+DXXXGNcfvnlpzyHe7zhfv33YF5enuHv7298+OGHjnO2bNliSDJWrFhR7zXO9Pe/O9BzBbfJz8+XJEVHR5/yvKKiIrVp00bJycm65pprtGnTJneU5zO2b9+uFi1aqF27drr55pu1d+/ek567YsUKDRo0qNaxIUOGaMWKFY1dps+pqKjQf/7zH/3ud7+TyWQ66Xnc366za9cuZWZm1rqHIyIi1KdPn5PewxUVFVqzZk2t15jNZg0aNIj7/gzl5+fLZDIpMjLylOc587sJdaWnpys+Pl4dOnTQXXfdpdzc3JOey33uOllZWVqwYIF+//vfn/Zc7vGG+fXfg2vWrJHNZqt1v3bs2FGtW7c+6f16Jr//3YVwBbew2+2677771L9/f5177rknPa9Dhw6aNWuW5s+fr//85z+y2+3q16+f9u/f78Zqm64+ffro7bff1qJFi/Taa69p165duuiii1RYWFjv+ZmZmUpISKh1LCEhQZmZme4o16fMmzdPeXl5uvXWW096Dve3a9Xcp87cw4cPH1ZVVRX3vYuUlZXp4Ycf1ujRoxUeHn7S85z93YTahg4dqn/9619aunSpnn32WS1btkzDhg1TVVVVvedzn7vOnDlzFBYWdtohatzjDVPf34OZmZkKCAio83/QnOp+PZPf/+7i59F3R7Nxzz33aOPGjacdf9y3b1/17dvX8bhfv37q1KmT3njjDT355JONXWaTN2zYMMfP3bp1U58+fdSmTRt98MEHDfp/3XDm/vnPf2rYsGFq0aLFSc/h/oYvsdlsuuGGG2QYhl577bVTnsvvprNz4403On7u2rWrunXrptTUVKWnp2vgwIEerMz3zZo1SzfffPNpFx/iHm+Yhv492JTRc4VGN378eH3++ef6+uuv1apVK6de6+/vr/POO087duxopOp8W2RkpM4555yTtl9iYmKd1XiysrKUmJjojvJ8xp49e7RkyRLddtttTr2O+/vs1NynztzDsbGxslgs3PdnqSZY7dmzR4sXLz5lr1V9Tve7CafWrl07xcbGnrT9uM9d49tvv9W2bduc/t0ucY/X52R/DyYmJqqiokJ5eXm1zj/V/Xomv//dhXCFRmMYhsaPH69PPvlE//vf/9S2bVunr1FVVaUNGzYoKSmpESr0fUVFRcrIyDhp+/Xt21dLly6tdWzx4sW1eldwerNnz1Z8fLyuvPJKp17H/X122rZtq8TExFr3cEFBgVatWnXSezggIEA9e/as9Rq73a6lS5dy3zdQTbDavn27lixZopiYGKevcbrfTTi1/fv3Kzc396Ttx33uGv/85z/Vs2dPde/e3enXco8fd7q/B3v27Cl/f/9a9+u2bdu0d+/ek96vZ/L73208upwGfNpdd91lREREGOnp6cahQ4ccXyUlJY5zbrnlFuORRx5xPJ42bZrx5ZdfGhkZGcaaNWuMG2+80QgMDDQ2bdrkiY/Q5DzwwANGenq6sWvXLmP58uXGoEGDjNjYWCM7O9swjLrtvXz5csPPz8944YUXjC1bthhTpkwx/P39jQ0bNnjqIzQ5VVVVRuvWrY2HH364znPc32evsLDQWLdunbFu3TpDkvHiiy8a69atc6xM98wzzxiRkZHG/PnzjZ9//tm45pprjLZt2xqlpaWOa1x++eXGK6+84nj8/vvvG1ar1Xj77beNzZs3G3fccYcRGRlpZGZmuv3zeaNTtXlFRYUxYsQIo1WrVsb69etr/W4vLy93XOPXbX66303N3anavLCw0HjwwQeNFStWGLt27TKWLFlinH/++Ub79u2NsrIyxzW4zxvudL9XDMMw8vPzjeDgYOO1116r9xrc4w3XkL8H77zzTqN169bG//73P+PHH380+vbta/Tt27fWdTp06GD897//dTxuyO9/TyBcodFIqvdr9uzZjnMuueQSY+zYsY7H9913n9G6dWsjICDASEhIMIYPH26sXbvW/cU3UaNGjTKSkpKMgIAAo2XLlsaoUaOMHTt2OJ7/dXsbhmF88MEHxjnnnGMEBAQYXbp0MRYsWODmqpu2L7/80pBkbNu2rc5z3N9n7+uvv67390hNu9rtdmPy5MlGQkKCYbVajYEDB9b5t2jTpo0xZcqUWsdeeeUVx79F7969jZUrV7rpE3m/U7X5rl27Tvq7/euvv3Zc49dtfrrfTc3dqdq8pKTEGDx4sBEXF2f4+/sbbdq0MW6//fY6IYn7vOFO93vFMAzjjTfeMIKCgoy8vLx6r8E93nAN+XuwtLTUuPvuu42oqCgjODjYuO6664xDhw7Vuc6Jr2nI739PMBmGYTROnxgAAAAANB/MuQIAAAAAFyBcAQAAAIALEK4AAAAAwAUIVwAAAADgAoQrAAAAAHABwhUAAAAAuADhCgAAAABcgHAFAAAAAC5AuAIAwMVMJpPmzZvn6TIAAG5GuAIA+JRbb71VJpOpztfQoUM9XRoAwMf5eboAAABcbejQoZo9e3atY1ar1UPVAACaC3quAAA+x2q1KjExsdZXVFSUpOohe6+99pqGDRumoKAgtWvXTh999FGt12/YsEGXX365goKCFBMTozvuuENFRUW1zpk1a5a6dOkiq9WqpKQkjR8/vtbzhw8f1nXXXafg4GC1b99en376aeN+aACAxxGuAADNzuTJkzVy5Ej99NNPuvnmm3XjjTdqy5YtkqTi4mINGTJEUVFR+uGHH/Thhx9qyZIltcLTa6+9pnvuuUd33HGHNmzYoE8//VRpaWm13mPatGm64YYb9PPPP2v48OG6+eabdeTIEbd+TgCAe5kMwzA8XQQAAK5y66236j//+Y8CAwNrHX/00Uf16KOPymQy6c4779Rrr73meO7CCy/U+eefr7///e9688039fDDD2vfvn0KCQmRJC1cuFBXX321Dh48qISEBLVs2VLjxo3TU089VW8NJpNJjz32mJ588klJ1YEtNDRUX3zxBXO/AMCHMecKAOBzLrvsslrhSZKio6MdP/ft27fWc3379tX69eslSVu2bFH37t0dwUqS+vfvL7vdrm3btslkMungwYMaOHDgKWvo1q2b4+eQkBCFh4crOzv7TD8SAKAJIFwBAHxOSEhInWF6rhIUFNSg8/z9/Ws9NplMstvtjVESAMBLMOcKANDsrFy5ss7jTp06SZI6deqkn376ScXFxY7nly9fLrPZrA4dOigsLEwpKSlaunSpW2sGAHg/eq4AAD6nvLxcmZmZtY75+fkpNjZWkvThhx+qV69eGjBggN555x2tXr1a//znPyVJN998s6ZMmaKxY8dq6tSpysnJ0R//+EfdcsstSkhIkCRNnTpVd955p+Lj4zVs2DAVFhZq+fLl+uMf/+jeDwoA8CqEKwCAz1m0aJGSkpJqHevQoYO2bt0qqXolv/fff1933323kpKS9N5776lz586SpODgYH355Ze69957dcEFFyg4OFgjR47Uiy++6LjW2LFjVVZWpr/97W968MEHFRsbq9/+9rfu+4AAAK/EaoEAgGbFZDLpk08+0bXXXuvpUgAAPoY5VwAAAADgAoQrAAAAAHAB5lwBAJoVRsMDABoLPVcAAAAA4AKEKwAAAABwAcIVAAAAALgA4QoAAAAAXIBwBQAAAAAuQLgCAAAAABcgXAEAAACACxCuAAAAAMAF/h+mydW4bij84gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}