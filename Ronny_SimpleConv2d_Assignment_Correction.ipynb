{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDSdtyNQXn3MwCL86kOC5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_SimpleConv2d_Assignment_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "NH-KHsGITMcD",
        "outputId": "c5687656-3e6d-4969-d67e-51d0a3ec6ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n",
            "Loading and preparing MNIST data...\n",
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloaded train-images-idx3-ubyte.gz.\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloaded train-labels-idx1-ubyte.gz.\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloaded t10k-images-idx3-ubyte.gz.\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Downloaded t10k-labels-idx1-ubyte.gz.\n",
            "Unzipping train-images-idx3-ubyte.gz...\n",
            "Unzipped train-images-idx3-ubyte.gz.\n",
            "Unzipping train-labels-idx1-ubyte.gz...\n",
            "Unzipped train-labels-idx1-ubyte.gz.\n",
            "Unzipping t10k-images-idx3-ubyte.gz...\n",
            "Unzipped t10k-images-idx3-ubyte.gz.\n",
            "Unzipping t10k-labels-idx1-ubyte.gz...\n",
            "Unzipped t10k-labels-idx1-ubyte.gz.\n",
            "Data loaded successfully. Training set: (60000, 1, 28, 28), Test set: (10000, 1, 28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with 937 batches per epoch...\n",
            "Epoch 1/3, Average Loss: 0.2263, Validation Accuracy: 96.89%\n",
            "Epoch 2/3, Average Loss: 0.0880, Validation Accuracy: 97.96%\n",
            "Epoch 3/3, Average Loss: 0.0640, Validation Accuracy: 97.94%\n",
            "\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "!pip install python-mnist\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import MNIST\n",
        "\n",
        "# Optimized Layer Implementations\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    An optimized 2D convolutional layer using vectorized operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, filter_size, padding=0, stride=1):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "        # He initialization for better convergence\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * np.sqrt(2.0 / (in_channels * self.filter_h * self.filter_w))\n",
        "        self.b = np.zeros((1, out_channels, 1, 1))\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.X = None\n",
        "        self.X_padded = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        H_out = (H + 2 * self.padding - self.filter_h) // self.stride + 1\n",
        "        W_out = (W + 2 * self.padding - self.filter_w) // self.stride + 1\n",
        "        self.output_shape = (N, self.out_channels, H_out, W_out)\n",
        "\n",
        "        # Pad the input\n",
        "        if self.padding > 0:\n",
        "            self.X_padded = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding),\n",
        "                                      (self.padding, self.padding)), mode='constant')\n",
        "        else:\n",
        "            self.X_padded = X\n",
        "\n",
        "        # Use im2col technique for vectorization\n",
        "        cols = self.im2col(self.X_padded, self.filter_h, self.filter_w, self.stride)\n",
        "        W_reshaped = self.W.reshape(self.out_channels, -1).T\n",
        "\n",
        "        # Perform convolution as a single matrix multiplication\n",
        "        output = np.dot(cols, W_reshaped) + self.b.reshape(1, -1)\n",
        "        output = output.reshape(N, H_out, W_out, self.out_channels).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def im2col(self, X, filter_h, filter_w, stride):\n",
        "        N, C, H, W = X.shape\n",
        "        H_out = (H - filter_h) // stride + 1\n",
        "        W_out = (W - filter_w) // stride + 1\n",
        "\n",
        "        # Create matrix of image patches\n",
        "        cols = np.zeros((N, C, filter_h, filter_w, H_out, W_out))\n",
        "        for h in range(filter_h):\n",
        "            h_end = h + H_out * stride\n",
        "            for w in range(filter_w):\n",
        "                w_end = w + W_out * stride\n",
        "                cols[:, :, h, w, :, :] = X[:, :, h:h_end:stride, w:w_end:stride]\n",
        "\n",
        "        cols = cols.transpose(0, 4, 5, 1, 2, 3).reshape(N * H_out * W_out, -1)\n",
        "        return cols\n",
        "\n",
        "    def col2im(self, cols, X_shape, filter_h, filter_w, stride):\n",
        "        N, C, H, W = X_shape\n",
        "        H_out = (H - filter_h) // stride + 1\n",
        "        W_out = (W - filter_w) // stride + 1\n",
        "\n",
        "        # Convert columns back to image format\n",
        "        cols = cols.reshape(N, H_out, W_out, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "        X = np.zeros(X_shape)\n",
        "\n",
        "        for h in range(filter_h):\n",
        "            h_end = h + H_out * stride\n",
        "            for w in range(filter_w):\n",
        "                w_end = w + W_out * stride\n",
        "                X[:, :, h:h_end:stride, w:w_end:stride] += cols[:, :, h, w, :, :]\n",
        "\n",
        "        return X\n",
        "\n",
        "    def backward(self, dA):\n",
        "        N, C, H, W = self.X.shape\n",
        "        dA_reshaped = dA.transpose(0, 2, 3, 1).reshape(-1, self.out_channels)\n",
        "\n",
        "        # Calculate gradients\n",
        "        cols = self.im2col(self.X_padded, self.filter_h, self.filter_w, self.stride)\n",
        "        self.dW = np.dot(cols.T, dA_reshaped).T.reshape(self.W.shape)\n",
        "        self.db = np.sum(dA_reshaped, axis=0).reshape(self.b.shape)\n",
        "\n",
        "        # Calculate input gradient\n",
        "        dcols = np.dot(dA_reshaped, self.W.reshape(self.out_channels, -1))\n",
        "        dX_padded = self.col2im(dcols, self.X_padded.shape,\n",
        "                               self.filter_h, self.filter_w, self.stride)\n",
        "\n",
        "        # Remove padding\n",
        "        if self.padding > 0:\n",
        "            dX = dX_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            dX = dX_padded\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    Optimized ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.mask = (X <= 0)\n",
        "        out = X.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dA[self.mask] = 0\n",
        "        return dA\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    Optimized 2D maximum pooling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride = stride\n",
        "        self.X = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        H_out = (H - self.pool_h) // self.stride + 1\n",
        "        W_out = (W - self.pool_w) // self.stride + 1\n",
        "\n",
        "        # Reshape input for vectorized operations\n",
        "        X_reshaped = X.reshape(N, C, H // self.pool_h, self.pool_h,\n",
        "                              W // self.pool_w, self.pool_w)\n",
        "\n",
        "        # Find max values and their indices\n",
        "        out = X_reshaped.max(axis=3).max(axis=4)\n",
        "        self.arg_max = X_reshaped.argmax(axis=3).argmax(axis=4)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dA):\n",
        "        N, C, H_out, W_out = dA.shape\n",
        "        dX = np.zeros_like(self.X)\n",
        "\n",
        "        # Distribute gradients to max positions\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        idx = self.arg_max[n, c, i, j]\n",
        "                        i1, i2 = np.unravel_index(idx, (self.pool_h, self.pool_w))\n",
        "                        dX[n, c, i*self.stride+i1, j*self.stride+i2] = dA[n, c, i, j]\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "class Flatten:\n",
        "    \"\"\"\n",
        "    Optimized flattening layer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_shape = X.shape\n",
        "        return X.reshape(X.shape[0], -1)\n",
        "\n",
        "    def backward(self, dA):\n",
        "        return dA.reshape(self.input_shape)\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    \"\"\"\n",
        "    Optimized fully-connected layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        # He initialization\n",
        "        self.W = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
        "        self.b = np.zeros(output_size)\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.b\n",
        "\n",
        "    def backward(self, dA):\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        self.db = np.sum(dA, axis=0)\n",
        "        return np.dot(dA, self.W.T)\n",
        "\n",
        "\n",
        "# Loss Function\n",
        "class CrossEntropyLoss:\n",
        "    \"\"\"\n",
        "    Optimized Cross-Entropy Loss with Softmax.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.y_true = None\n",
        "        self.softmax_output = None\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        self.y_true = y_true\n",
        "\n",
        "        # Numerical stability\n",
        "        logits = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        exp_logits = np.exp(logits)\n",
        "        self.softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = -np.sum(y_true * np.log(self.softmax_output + 1e-12)) / logits.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        return (self.softmax_output - self.y_true) / self.y_true.shape[0]\n",
        "\n",
        "\n",
        "# Optimizer - SGD with Mini-Batch Support\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimizer with mini-batch support.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity = {}\n",
        "\n",
        "    def update(self, layers):\n",
        "        for i, layer in enumerate(layers):\n",
        "            if hasattr(layer, 'W') and hasattr(layer, 'dW'):\n",
        "                # Initialize velocity if not exists\n",
        "                if i not in self.velocity:\n",
        "                    self.velocity[i] = {'W': np.zeros_like(layer.W), 'b': np.zeros_like(layer.b)}\n",
        "\n",
        "                # Update with momentum\n",
        "                self.velocity[i]['W'] = self.momentum * self.velocity[i]['W'] - self.learning_rate * layer.dW\n",
        "                self.velocity[i]['b'] = self.momentum * self.velocity[i]['b'] - self.learning_rate * layer.db\n",
        "\n",
        "                layer.W += self.velocity[i]['W']\n",
        "                layer.b += self.velocity[i]['b']\n",
        "\n",
        "\n",
        "# Model and Training Loop\n",
        "class LeNet:\n",
        "    \"\"\"\n",
        "    Optimized LeNet-5 architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.layers = [\n",
        "            Conv2d(in_channels=1, out_channels=6, filter_size=(5, 5), padding=2),  # Added padding to maintain size\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=2),\n",
        "            Conv2d(in_channels=6, out_channels=16, filter_size=(5, 5)),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=2),\n",
        "            Flatten(),\n",
        "            Dense(input_size=16 * 5 * 5, output_size=120),  # Corrected input size\n",
        "            ReLU(),\n",
        "            Dense(input_size=120, output_size=84),\n",
        "            ReLU(),\n",
        "            Dense(input_size=84, output_size=10)\n",
        "        ]\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dA):\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA)\n",
        "        return dA\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Optimized trainer with mini-batch implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, loss_function, epochs=3, batch_size=64):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.log_loss = []\n",
        "        self.log_acc = []\n",
        "\n",
        "    def fit(self, X_train, y_train_one_hot, X_val, y_val):\n",
        "        num_samples = len(X_train)\n",
        "        num_batches = num_samples // self.batch_size\n",
        "        print(f\"Starting training with {num_batches} batches per epoch...\")\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            indices = np.random.permutation(num_samples)\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                batch_indices = indices[i * self.batch_size:(i + 1) * self.batch_size]\n",
        "                X_batch = X_train[batch_indices]\n",
        "                y_batch_one_hot = y_train_one_hot[batch_indices]\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.model.forward(X_batch)\n",
        "                loss = self.loss_function.forward(output, y_batch_one_hot)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                dA = self.loss_function.backward()\n",
        "                self.model.backward(dA)\n",
        "\n",
        "                # Update parameters\n",
        "                self.optimizer.update(self.model.layers)\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            self.log_loss.append(avg_loss)\n",
        "\n",
        "            # Validation\n",
        "            val_preds = self.predict(X_val)\n",
        "            val_accuracy = np.mean(val_preds == y_val)\n",
        "            self.log_acc.append(val_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Average Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "        print(\"\\nTraining complete.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Use batch prediction for efficiency\n",
        "        batch_size = 256\n",
        "        num_samples = len(X)\n",
        "        num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, num_samples)\n",
        "            X_batch = X[start_idx:end_idx]\n",
        "\n",
        "            output = self.model.forward(X_batch)\n",
        "            batch_preds = np.argmax(output, axis=1)\n",
        "            predictions.append(batch_preds)\n",
        "\n",
        "        return np.concatenate(predictions)\n",
        "\n",
        "\n",
        "def download_mnist_data():\n",
        "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
        "    files = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "             \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            url = base_url + file_name\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f\"Downloaded {file_name}.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {file_name}: {e}\")\n",
        "                raise FileNotFoundError(f\"Could not download {file_name}\")\n",
        "\n",
        "def load_data():\n",
        "    download_mnist_data()\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    files = [\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\",\n",
        "             \"t10k-images-idx3-ubyte\", \"t10k-labels-idx1-ubyte\"]\n",
        "\n",
        "    for file_name in files:\n",
        "        zipped_path = os.path.join(data_dir, file_name + \".gz\")\n",
        "        unzipped_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(unzipped_path):\n",
        "            print(f\"Unzipping {file_name}.gz...\")\n",
        "            with gzip.open(zipped_path, 'rb') as f_in:\n",
        "                with open(unzipped_path, 'wb') as f_out:\n",
        "                    f_out.write(f_in.read())\n",
        "            print(f\"Unzipped {file_name}.gz.\")\n",
        "\n",
        "    mndata = MNIST(data_dir)\n",
        "    images, labels = mndata.load_training()\n",
        "    test_images, test_labels = mndata.load_testing()\n",
        "\n",
        "    X_train = np.array(images, dtype=np.float32).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_train = np.array(labels)\n",
        "    X_test = np.array(test_images, dtype=np.float32).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    y_train_one_hot = np.eye(10)[y_train]\n",
        "\n",
        "    return X_train, y_train_one_hot, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def display_mnist_images(images, labels, num_images=5):\n",
        "    \"\"\"\n",
        "    Displays sample MNIST images with their corresponding labels.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(f\"Label: {labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"Sample MNIST Images\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main Training Script\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading and preparing MNIST data...\")\n",
        "    X_train, y_train_one_hot, y_train, X_test, y_test = load_data()\n",
        "    print(f\"Data loaded successfully. Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "    # Display sample images\n",
        "    display_mnist_images(X_train, y_train)\n",
        "\n",
        "    # Instantiate model, loss, and optimizer\n",
        "    model = LeNet()\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(learning_rate=0.01, momentum=0.9)  # Added momentum for faster convergence\n",
        "\n",
        "    # Instantiate and run the trainer with mini-batch implementation\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        loss_function=criterion,\n",
        "        epochs=3,\n",
        "        batch_size=64  # Mini-batch size\n",
        "    )\n",
        "\n",
        "    trainer.fit(X_train, y_train_one_hot, X_test, y_test)"
      ]
    }
  ]
}