{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuSaSooZoPNo+WfuJqN8ht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_Keras_series_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Sharing and executing the official tutorial model**\n",
        "\n",
        "I will share the \"TensorFlow 2 quickstart for beginners\" tutorial, which demonstrates a simple image classification model using Keras and the MNIST dataset."
      ],
      "metadata": {
        "id": "D-mtL1uWybVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWrKFtoyGBY",
        "outputId": "d2fc48d9-7de2-4b81-d9b9-70c38445bcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.8607 - loss: 0.4815\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9548 - loss: 0.1522\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.1114\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9754 - loss: 0.0830\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0752\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07403755187988281, 0.9782000184059143]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizing the data: Scale pixel values from [0, 255] to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Building the Keras Sequential model\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Reshaping the 28x28 images into a flat 1D array of 784 pixels\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    # Adding a fully connected (Dense) hidden layer with 128 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # Adding a Dropout layer to prevent overfitting by randomly dropping 20% of neurons\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # Adding the final output layer with 10 neurons (for 10 classes) and Softmax activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "# Configuring the training process using an optimizer, loss function, and metrics\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training the model for 5 epochs\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluating the model on the test data\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Executing Various Methods**\n",
        "\n",
        "**Part 1: Executing a Model**\n",
        "\n",
        "I've selected the retinanet model from the tensorflow/models/research repository as an example of an advanced, real-world application. RetinaNet is a powerful single-stage object detection model known for its high accuracy. Executing this model directly can be complex due to its dependencies and the large dataset it requires (e.g., COCO or Pascal VOC). Therefore, I'll focus on a code reading and provide a high-level explanation of its components and workflow, as you suggested for cases where direct execution is difficult.\n",
        "\n",
        "High-Level Code Reading of RetinaNet\n",
        "\n",
        "The retinanet code is structured to handle various stages of the object detection pipeline.\n",
        "\n",
        "1. Model Definition: The core of the code defines the RetinaNet architecture. It typically involves:\n",
        "\n",
        "***Backbone Network: A deep convolutional neural network (e.g., ResNet) that extracts features from the input image. This is a foundational component.\n",
        "\n",
        "***Feature Pyramid Network (FPN): This component takes the features from the backbone and builds a top-down pathway with lateral connections, creating a feature pyramid with high-level semantic information at all scales.\n",
        "\n",
        "***Subnets: Two parallel subnets are attached to the FPN. One predicts the class of the object at each location, and the other predicts the bounding box regression (the coordinates of the box).\n",
        "\n",
        "2. Loss Function: RetinaNet uses a unique loss function called the Focal Loss. This is a key innovation designed to address the problem of class imbalance in object detection (where the number of background locations is far greater than the number of foreground objects). The code implements this loss, which down-weights the loss assigned to well-classified examples, thus focusing training on hard, misclassified examples.\n",
        "\n",
        "3. Data Input Pipeline: The model uses a robust tf.data pipeline to load and preprocess the massive datasets required for object detection. This includes parsing data formats like TFRecords, augmenting images (e.g., random flips, scaling), and batching.\n",
        "\n",
        "4. Training and Evaluation Loops: The code includes functions for training the model using a custom training loop, which involves a single pass through the network for both classification and regression tasks, followed by loss calculation and backpropagation.\n",
        "\n",
        "Part 2: Rewriting Models with Keras\n",
        "\n",
        "I will rewrite the four specified models using the tf.keras module. I'll use the Sequential API for simplicity, as it's a straightforward way to stack layers for most of these tasks.\n",
        "\n",
        "2.1 Iris (Binary Classification)\n",
        "\n",
        "Objective: Classify Iris-versicolor vs. Iris-virginica.\n",
        "\n",
        "Methodology: I'll use a binary cross-entropy loss function.\n",
        "\n"
      ],
      "metadata": {
        "id": "tiJlvZB22vow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 1 (Iris Binary Classification)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Data Preparation\n",
        "iris = load_iris()\n",
        "X = iris.data[50:]  # Versicolor and Virginica are at indices 50-149\n",
        "y = iris.target[50:] - 1 # Relabel to 0 and 1 for binary classification\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Model Definition (Sequential API)\n",
        "model_iris_binary = keras.Sequential([\n",
        "    keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# 3. Model Compilation\n",
        "model_iris_binary.compile(optimizer='adam',\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "# 4. Training and Evaluation\n",
        "model_iris_binary.fit(X_train, y_train, epochs=20, verbose=0)\n",
        "loss, accuracy = model_iris_binary.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Iris (Binary) Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rjCNJMO4ElR",
        "outputId": "7b94d34b-4aa5-4c16-d092-e3ae3784de40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris (Binary) Test Accuracy: 0.7667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 2: Iris (Multi-value Classification)**"
      ],
      "metadata": {
        "id": "xskqTbKc7gqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Data Preparation (all three classes)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Model Definition\n",
        "model_iris_multi = keras.Sequential([\n",
        "    keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(3, activation='softmax')  # 3 neurons for 3 classes\n",
        "])\n",
        "\n",
        "# 3. Model Compilation\n",
        "model_iris_multi.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# 4. Training and Evaluation\n",
        "model_iris_multi.fit(X_train, y_train, epochs=20, verbose=0)\n",
        "loss, accuracy = model_iris_multi.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Iris (Multi-value) Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wE1Empl7vMk",
        "outputId": "3a9674cf-7970-49c4-a90d-d050e0d4539e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris (Multi-value) Test Accuracy: 0.2889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 3: House Prices**"
      ],
      "metadata": {
        "id": "VllqQett728K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Data Preparation\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data Scaling (Crucial for regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Model Definition\n",
        "model_house_prices = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(1) # Single neuron for regression output\n",
        "])\n",
        "\n",
        "# 3. Model Compilation\n",
        "model_house_prices.compile(optimizer='rmsprop',\n",
        "                           loss='mse',\n",
        "                           metrics=['mae']) # Mean Absolute Error as a metric\n",
        "\n",
        "# 4. Training and Evaluation\n",
        "model_house_prices.fit(X_train_scaled, y_train, epochs=20, verbose=0)\n",
        "loss, mae = model_house_prices.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"House Prices Test MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLjP7ll28HK0",
        "outputId": "1dc038ef-8e11-4955-fecd-dd5126033fe3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "House Prices Test MAE: 0.3733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 4: MNIST (Image Classification)**"
      ],
      "metadata": {
        "id": "uADyfi3g8S6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Data Preparation\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# 2. Model Definition\n",
        "model_mnist = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Model Compilation\n",
        "model_mnist.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# 4. Training and Evaluation\n",
        "model_mnist.fit(x_train, y_train, epochs=5, verbose=0)\n",
        "loss, accuracy = model_mnist.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"MNIST Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuXb5P5g8fbg",
        "outputId": "49e04bc1-0a1b-4c6d-9fce-e7a462bd1ba3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST Test Accuracy: 0.9792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Learning Iris (binary classification) with Keras**"
      ],
      "metadata": {
        "id": "2gcuLZsL8S4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "# Loading the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Only extracting the data for Iris-versicolor and Iris-virginica, which are the last 100 samples from the dataset.\n",
        "X = iris.data[50:]\n",
        "y = iris.target[50:]\n",
        "\n",
        "# Relabeling the targets from 1 and 2 to 0 and 1 for binary classification compatibility\n",
        "y = y - 1\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j83RQdpQ9YUL",
        "outputId": "922b9c77-b8eb-42d7-bd2a-fff7018fd0f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (70, 4)\n",
            "Training labels shape: (70,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Definition (Using Keras Sequential API to build a simple neural network)**"
      ],
      "metadata": {
        "id": "HQDF3i5J-j6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "model = keras.Sequential([\n",
        "    # Input layer with 8 neurons and ReLU activation.\n",
        "    # The input_shape is specified only for the first layer.\n",
        "    keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "\n",
        "    # Output layer with a single neuron and a sigmoid activation function.\n",
        "    # Sigmoid squashes the output to a value between 0 and 1,\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Displaying the model summary to see the architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ecfJt0vG-ziZ",
        "outputId": "841dc7d4-689e-441f-cefb-1de126f1bbeb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49\u001b[0m (196.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> (196.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49\u001b[0m (196.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> (196.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Compilation (configuring the model for training)**"
      ],
      "metadata": {
        "id": "ieMF_rwg_F6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "l90Ph-GX_TGR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation (training the model on the training data and evaluating it on the test data)**"
      ],
      "metadata": {
        "id": "b820C9OV_c2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
        "\n",
        "# Evaluating the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIegBE7S_pIM",
        "outputId": "755e81d1-b46e-4fff-9640-001230dfc7b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7710\n",
            "Test Accuracy: 0.5667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Learn Iris (multi-level classification) with Keras**"
      ],
      "metadata": {
        "id": "fZ7r7ZdD_w2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "# Loading the full Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98j3H4_AIZg",
        "outputId": "1719ecf6-6d6f-420c-ea20-6415ccb96a69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (105, 4)\n",
            "Training labels shape: (105,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Definition**"
      ],
      "metadata": {
        "id": "Yts_5eBtAZHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "model = keras.Sequential([\n",
        "    # Input layer with 8 neurons and ReLU activation\n",
        "    keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "\n",
        "    # Output layer with 3 neurons (one for each class) and a softmax activation function.\n",
        "    # Softmax converts the output into a probability distribution over the classes.\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Displaying the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "AMepZU55Agfw",
        "outputId": "e436d722-3cfe-4166-d776-b529624fb713"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m27\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67\u001b[0m (268.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67</span> (268.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67\u001b[0m (268.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67</span> (268.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Compilation**"
      ],
      "metadata": {
        "id": "ArnfoB1JAq3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OWRBiSZKAwOs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "MgGO5LBdA1YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
        "\n",
        "# Evaluating the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFGi-L4AA9ev",
        "outputId": "272647c8-70c8-47fa-f7b0-a75f675114ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7467\n",
            "Test Accuracy: 0.6444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Learning House Prices with Keras**"
      ],
      "metadata": {
        "id": "CsnSkWyeBGmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data Scaling (Crucial for regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Scaled training data shape:\", X_train_scaled.shape)\n",
        "print(\"Scaled training labels shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lWLcA-0BaJ7",
        "outputId": "20561c93-f359-43e4-fc91-30ae2317ec27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled training data shape: (16512, 8)\n",
            "Scaled training labels shape: (16512,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Definition**"
      ],
      "metadata": {
        "id": "5G8s0OsHBy24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture using the Sequential API\n",
        "model = keras.Sequential([\n",
        "    # Hidden layer with 64 neurons and ReLU activation\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "\n",
        "    # Another hidden layer to learn more complex patterns\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "    # The final output layer has a single neuron. No activation function is used\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Displaying the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "jMJTofj_B6S6",
        "outputId": "1b2dc975-f838-4a02-cace-07a3fd821883"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Compilation**"
      ],
      "metadata": {
        "id": "VK6TzUdmCMnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse',  # Mean Squared Error for regression\n",
        "              metrics=['mae']) # Mean Absolute Error for easier interpretation"
      ],
      "metadata": {
        "id": "lTbCIpDfCSVZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "wqyMY1h1CZTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(X_train_scaled, y_train, epochs=20, verbose=0)\n",
        "\n",
        "# Evaluating the model on the test data\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss (MSE): {loss:.4f}\")\n",
        "print(f\"Test MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LedLLJWyCfOg",
        "outputId": "a2d1706d-693a-49c9-9fdc-c03956eeaf4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (MSE): 21.9519\n",
            "Test MAE: 3.3538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DzKzyXBICnXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Learning MNIST with Keras**"
      ],
      "metadata": {
        "id": "x4Za14oSMqA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizing the data: Scale pixel values from [0, 255] to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGlSmN8INZb1",
        "outputId": "8ff9f87c-834f-4ecf-b356-2dc7ac4a814b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Training labels shape: (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Definition**"
      ],
      "metadata": {
        "id": "AxJVUgvdNdZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "model = keras.Sequential([\n",
        "    # Flatten the 28x28 image into a 1D array of 784 pixels\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    # A fully connected hidden layer with 128 neurons and ReLU activation\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "\n",
        "    # Dropout layer to reduce overfitting\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    # The output layer with 10 neurons (for 10 classes) and softmax activation\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Displaying the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "ZjgdSsVsNlRV",
        "outputId": "a2888523-847a-4217-aee3-0f5b11d94bf7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Compilation**"
      ],
      "metadata": {
        "id": "z-FB1D_iNtss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7VFJACJPNyxD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "n_mvIkNNN35z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model for 5 epochs\n",
        "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
        "\n",
        "# Evaluating the model on the test data\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmEONJxzN8o9",
        "outputId": "2b8b0fec-10c5-4267-8706-d5c4b5aa5287"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.0714\n",
            "Test Accuracy: 0.9761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Rewriting to PyTorch**"
      ],
      "metadata": {
        "id": "XCRzT2oAOEFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris (Binary Classification)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Data Preparation\n",
        "iris = load_iris()\n",
        "X = iris.data[50:]\n",
        "y = iris.target[50:] - 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converting to PyTorch tensors and creating DataLoader\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 2. Model Definition\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 8)\n",
        "        self.fc2 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "# 3. Loss, Optimizer, and Training\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 4. Evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    predicted_classes = (torch.sigmoid(test_outputs) > 0.5).float()\n",
        "    accuracy = (predicted_classes.squeeze() == y_test_tensor.squeeze()).float().mean()\n",
        "    print(f\"Iris (Binary) Test Accuracy: {accuracy.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8clGicyGOWtK",
        "outputId": "b4363e3d-af1f-45ec-9990-3f855e45e23b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris (Binary) Test Accuracy: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iris (Multi-class Classification)**"
      ],
      "metadata": {
        "id": "2YpW5cw8Oraq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Data Preparation\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converting to PyTorch tensors and creating DataLoader\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 2. Model Definition\n",
        "class MultiClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 8)\n",
        "        self.fc2 = nn.Linear(8, 3) # 3 neurons for 3 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = MultiClassifier()\n",
        "\n",
        "# 3. Loss, Optimizer, and Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 4. Evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted_classes = torch.max(test_outputs, 1)\n",
        "    accuracy = (predicted_classes == y_test_tensor).float().mean()\n",
        "    print(f\"Iris (Multi-class) Test Accuracy: {accuracy.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrwYHUg7OtA9",
        "outputId": "b92be3f0-3d29-43c6-8fdf-4d5955bf88ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris (Multi-class) Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**House Prices (Regression)**"
      ],
      "metadata": {
        "id": "gFZ6GbEmO3Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Data Preparation\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converting to PyTorch tensors and creating DataLoader\n",
        "# Converting to PyTorch tensors and creating DataLoader\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. Model Definition\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1) # Single output for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "model = RegressionModel()\n",
        "\n",
        "# 3. Loss, Optimizer, and Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 4. Evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    mae = torch.mean(torch.abs(test_outputs - y_test_tensor))\n",
        "    print(f\"House Prices Test MAE: {mae.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8MjlCM6O9AK",
        "outputId": "e3b9ec4f-70de-4d9f-f1c6-cbde63979757"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "House Prices Test MAE: 0.3732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST (Multi-class Classification)**"
      ],
      "metadata": {
        "id": "Ob2jqLLIPfn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1. Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 2. Model Definition (using a CNN)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# 3. Loss, Optimizer, and Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 4. Evaluation\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f\"\\nMNIST Test Accuracy: {100. * correct / len(test_loader.dataset):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOR2RqboPkWw",
        "outputId": "5ad939e4-c100-46c4-96ee-942b857736c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 58.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.76MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.90MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Comparison of frameworks**\n",
        "\n",
        "1. Calculation Speed\n",
        "\n",
        "Keras: When running on top of TensorFlow, Keras utilizes a computational graph that is built and optimized before execution. This static graph approach allows for powerful optimizations and is very efficient for deployment on a variety of hardware, including mobile and production servers. However, it can sometimes be less flexible for research and rapid prototyping where you need to change the model's structure on the fly.\n",
        "\n",
        "PyTorch: PyTorch uses a dynamic computational graph, which is built on the fly as the code is executed. This \"eager execution\" approach makes it behave more like standard Python, simplifying debugging and allowing for more flexibility. For basic operations, there's little difference in speed, but the dynamic nature of PyTorch can make it a favorite for research. Recently, PyTorch has added features like TorchScript to compile models into a static graph for performance optimization and deployment, bridging the gap with TensorFlow's approach.\n",
        "\n",
        "2. Number of Lines of Code and Readability\n",
        "\n",
        "Keras: As a high-level API, Keras is renowned for its simplicity and minimalism. You can define and train a complex model in just a few lines of code. The automated training loop (model.fit()) abstracts away much of the boilerplate, making it exceptionally easy to read and understand for beginners. This abstraction, however, can sometimes obscure what's happening under the hood.\n",
        "\n",
        "PyTorch: PyTorch is a lower-level, more verbose framework. You must manually define the model as a class, create a Dataset and DataLoader, and write the entire training loop yourself. While this results in more lines of code, it provides explicit control over every part of the process, which is highly valued by researchers and advanced users. The code is very readable for those familiar with object-oriented programming in Python.\n",
        "\n",
        "3. Functions Provided\n",
        "\n",
        "Keras: Keras provides a simplified, user-friendly set of tools focused on rapid prototyping and deployment. Its core functions are designed to streamline the common deep learning workflow:\n",
        "\n",
        ">Sequential and Functional APIs: For building models in a simple, intuitive way.\n",
        "\n",
        ">Pre-built layers: A wide range of pre-configured layers (Dense, Conv2D, LSTM, etc.).\n",
        "\n",
        ">Built-in functions: Standardized loss functions, optimizers, and metrics that can be specified by a simple string (e.g., 'adam', 'mse').\n",
        "\n",
        ">model.fit(): An all-in-one function for training with automatic handling of batches, epochs, and progress tracking.\n",
        "\n",
        "PyTorch: PyTorch offers a more comprehensive and flexible set of tools for research and fine-grained control. Its key features include:\n",
        "\n",
        ">torch.nn.Module: The foundational class for building models, providing a flexible, object-oriented approach.\n",
        "\n",
        ">Dynamic computational graph: Enables complex, dynamic models and makes debugging much easier.\n",
        "\n",
        ">torch.utils.data: Powerful tools for creating custom datasets and iterators (DataLoader), which are ideal for large or non-standard datasets.\n",
        "\n",
        ">Extensive library: A vast ecosystem of libraries like torchvision (for computer vision), torchaudio (for audio), and torchtext (for NLP).\n",
        "\n",
        ">Manual control: You have direct access to every component of the training loop, including gradient calculations, which is essential for advanced research."
      ],
      "metadata": {
        "id": "IXQFUmw-P0VI"
      }
    }
  ]
}