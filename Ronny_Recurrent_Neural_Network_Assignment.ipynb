{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeqQZYGdwAS880+D2G26oi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_Recurrent_Neural_Network_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Simple Forward propagation implementation of RNN**"
      ],
      "metadata": {
        "id": "mWPyTukvXG8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM2nuwyIXEFk",
        "outputId": "b5b0bfcf-769b-4aa7-e0fb-cddf56c5bee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (4, 5, 6)\n",
            "First time-step hidden state:\n",
            " [[ 0.01744804 -0.01809214 -0.00733537  0.02353047 -0.01589661 -0.00291828]\n",
            " [ 0.02475063  0.01492887 -0.00131434  0.01434083 -0.00562717 -0.00838413]\n",
            " [ 0.0045935   0.0196712   0.01454075  0.00940329  0.00687638 -0.00491381]\n",
            " [-0.02007635 -0.02387972 -0.0103665  -0.01783901 -0.00104322  0.00930331]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    A minimal RNN layer implementing only the forward pass.\n",
        "\n",
        "    Args:\n",
        "      n_features  (int): dimensionality of each time-step input x_t\n",
        "      n_nodes     (int): number of hidden units in the RNN\n",
        "      activation  (callable): nonlinearity applied to pre-activations (default: np.tanh)\n",
        "      seed        (int, optional): random seed for reproducibility\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes, activation=np.tanh, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes    = n_nodes\n",
        "        self.activation = activation\n",
        "\n",
        "        # Weight: input → hidden\n",
        "        self.W_x = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        # Weight: hidden(prev) → hidden\n",
        "        self.W_h = np.random.randn(n_nodes, n_nodes) * 0.01\n",
        "        # Bias for hidden units\n",
        "        self.b   = np.zeros((n_nodes,))\n",
        "\n",
        "        # Placeholder for hidden state at current time\n",
        "        self.h   = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Run the RNN forward over an input sequence.\n",
        "\n",
        "        Args:\n",
        "          x (ndarray): shape (batch_size, n_sequences, n_features)\n",
        "\n",
        "        Returns:\n",
        "          outputs (ndarray): shape (batch_size, n_sequences, n_nodes)\n",
        "        \"\"\"\n",
        "        batch_size, n_sequences, _ = x.shape\n",
        "\n",
        "        # Initialize h₀ = 0\n",
        "        self.h = np.zeros((batch_size, self.n_nodes))\n",
        "\n",
        "        # Collect hidden states at each time step\n",
        "        hidden_seq = []\n",
        "\n",
        "        for t in range(n_sequences):\n",
        "            x_t = x[:, t, :]                               # (batch_size, n_features)\n",
        "            a_t = x_t.dot(self.W_x)                        \\\n",
        "                + self.h.dot(self.W_h)                     \\\n",
        "                + self.b                                   # (batch_size, n_nodes)\n",
        "\n",
        "            # Activation (default: tanh; swap in ReLU or others by passing activation=...)\n",
        "            self.h = self.activation(a_t)                  # (batch_size, n_nodes)\n",
        "            hidden_seq.append(self.h)\n",
        "\n",
        "        # Stack along time axis → (batch_size, n_sequences, n_nodes)\n",
        "        return np.stack(hidden_seq, axis=1)\n",
        "\n",
        "# Example Usage\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size, n_sequences, n_features = 4, 5, 3\n",
        "    n_nodes = 6\n",
        "\n",
        "    np.random.seed(0)\n",
        "    x = np.random.randn(batch_size, n_sequences, n_features)\n",
        "\n",
        "    rnn = SimpleRNN(n_features, n_nodes, seed=42)\n",
        "    y   = rnn.forward(x)\n",
        "\n",
        "    print(\"Output shape:\", y.shape)\n",
        "    print(\"First time-step hidden state:\\n\", y[:, 0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Experiment of forward propagation with small sequence**"
      ],
      "metadata": {
        "id": "auYUNWiddUDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100      # shape (1, 3, 2)\n",
        "w_x = np.array([[1, 3, 5, 7],\n",
        "                [3, 5, 7, 8]]) / 100              # shape (2, 4)\n",
        "w_h = np.array([[ 1,  3,  5,  7],\n",
        "                [ 2,  4,  6,  8],\n",
        "                [ 3,  5,  7,  8],\n",
        "                [ 4,  6,  8, 10]]) / 100          # shape (4, 4)\n",
        "b   = np.array([1, 1, 1, 1])                         # shape (4,)\n",
        "\n",
        "batch_size, n_sequences, _ = x.shape\n",
        "n_nodes = w_x.shape[1]\n",
        "\n",
        "# Initialize hidden state to zeros\n",
        "h = np.zeros((batch_size, n_nodes))\n",
        "\n",
        "# Forward pass through all time steps\n",
        "for t in range(n_sequences):\n",
        "    x_t = x[:, t, :]                     # (1, 2)\n",
        "    a_t = x_t.dot(w_x) + h.dot(w_h) + b  # (1, 4)\n",
        "    h   = np.tanh(a_t)                   # (1, 4)\n",
        "\n",
        "print(\"Final hidden state h₃:\\n\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS17kBzFf1vN",
        "outputId": "2ed98e1e-cd74-4507-cee9-7d3ff0176644"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden state h₃:\n",
            " [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is the same as the expected output."
      ],
      "metadata": {
        "id": "jv1qqW38ggRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. (Advance assignment) Implementation of backpropagation**"
      ],
      "metadata": {
        "id": "VgqTqFtug0rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    Minimal RNN layer with forward and backward (BPTT) implementations.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes, activation=np.tanh, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.n_features       = n_features\n",
        "        self.n_nodes          = n_nodes\n",
        "        self.activation       = activation\n",
        "        self.activation_deriv = lambda a: 1 - np.tanh(a)**2\n",
        "\n",
        "        # Parameters\n",
        "        self.W_x = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        self.W_h = np.random.randn(n_nodes, n_nodes)    * 0.01\n",
        "        self.b   = np.zeros((n_nodes,))\n",
        "\n",
        "        # For storing intermediate states during forward pass\n",
        "        self.h_states = []\n",
        "        self.a_states = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (batch, T, n_features)\n",
        "        Returns:\n",
        "          h_seq: (batch, T, n_nodes)\n",
        "        \"\"\"\n",
        "        batch, T, _ = x.shape\n",
        "        h_t = np.zeros((batch, self.n_nodes))\n",
        "        self.h_states = [h_t]\n",
        "        self.a_states = []\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            x_t = x[:, t, :]\n",
        "            a_t = x_t.dot(self.W_x) + h_t.dot(self.W_h) + self.b\n",
        "            h_t = self.activation(a_t)\n",
        "\n",
        "            self.a_states.append(a_t)\n",
        "            self.h_states.append(h_t)\n",
        "            outputs.append(h_t)\n",
        "\n",
        "        return np.stack(outputs, axis=1)\n",
        "\n",
        "    def backward(self, x, dh_out):\n",
        "        \"\"\"\n",
        "        Backprop through time.\n",
        "        Args:\n",
        "          x:      (batch, T, n_features)\n",
        "          dh_out: (batch, T, n_nodes) upstream grad on h_t\n",
        "        Returns:\n",
        "          dW_x, dW_h, db  (normalized by batch size)\n",
        "        \"\"\"\n",
        "        batch, T, _ = x.shape\n",
        "        dW_x   = np.zeros_like(self.W_x)\n",
        "        dW_h   = np.zeros_like(self.W_h)\n",
        "        db     = np.zeros_like(self.b)\n",
        "        dh_next = np.zeros((batch, self.n_nodes))\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            dh_total = dh_out[:, t, :] + dh_next      # include next‐time gradient\n",
        "            a_t      = self.a_states[t]\n",
        "            da_t     = dh_total * self.activation_deriv(a_t)\n",
        "\n",
        "            x_t    = x[:, t, :]\n",
        "            h_prev = self.h_states[t]\n",
        "\n",
        "            dW_x += x_t.T.dot(da_t)\n",
        "            dW_h += h_prev.T.dot(da_t)\n",
        "            db   += da_t.sum(axis=0)\n",
        "\n",
        "            dh_next = da_t.dot(self.W_h.T)\n",
        "\n",
        "        return dW_x / batch, dW_h / batch, db / batch\n",
        "\n",
        "\n",
        "class ScratchSimpleRNNClassifier:\n",
        "    \"\"\"\n",
        "    Wraps SimpleRNN + a softmax head for sequence classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes, n_classes, lr=1e-2, seed=None):\n",
        "        self.rnn   = SimpleRNN(n_features, n_nodes, seed=seed)\n",
        "        self.W_out = np.random.randn(n_nodes, n_classes) * 0.01\n",
        "        self.b_out = np.zeros((n_classes,))\n",
        "        self.lr    = lr\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_seq      = self.rnn.forward(x)         # (batch, T, n_nodes)\n",
        "        h_last     = h_seq[:, -1, :]             # (batch, n_nodes)\n",
        "        logits     = h_last.dot(self.W_out) + self.b_out\n",
        "        self.cache = {'x': x, 'h_last': h_last, 'logits': logits}\n",
        "        return logits\n",
        "\n",
        "    def compute_loss_and_grad(self, logits, y_true):\n",
        "        batch_size = logits.shape[0]\n",
        "        # stable softmax\n",
        "        exp_s   = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
        "        probs   = exp_s / exp_s.sum(axis=1, keepdims=True)\n",
        "        correct = probs[np.arange(batch_size), y_true]\n",
        "        loss    = -np.log(correct + 1e-12).mean()\n",
        "\n",
        "        dlogits = probs.copy()\n",
        "        dlogits[np.arange(batch_size), y_true] -= 1\n",
        "        dlogits /= batch_size\n",
        "        return loss, dlogits\n",
        "\n",
        "    def backward(self, y_true):\n",
        "        x, h_last, logits = (self.cache[k] for k in ('x', 'h_last', 'logits'))\n",
        "        loss, dlogits     = self.compute_loss_and_grad(logits, y_true)\n",
        "\n",
        "        # head grads\n",
        "        dW_out = h_last.T.dot(dlogits)\n",
        "        db_out = dlogits.sum(axis=0)\n",
        "\n",
        "        # grad into last hidden state\n",
        "        dh_last = dlogits.dot(self.W_out.T)\n",
        "        batch, T, _ = x.shape\n",
        "        dh_out = np.zeros((batch, T, self.rnn.n_nodes))\n",
        "        dh_out[:, -1, :] = dh_last\n",
        "\n",
        "        # BPTT\n",
        "        dW_x, dW_h, db = self.rnn.backward(x, dh_out)\n",
        "\n",
        "        self.grads = {\n",
        "            'W_x':   dW_x,\n",
        "            'W_h':   dW_h,\n",
        "            'b':     db,\n",
        "            'W_out': dW_out,\n",
        "            'b_out': db_out\n",
        "        }\n",
        "        return loss\n",
        "\n",
        "    def update_params(self):\n",
        "        # RNN params\n",
        "        self.rnn.W_x -= self.lr * self.grads['W_x']\n",
        "        self.rnn.W_h -= self.lr * self.grads['W_h']\n",
        "        self.rnn.b   -= self.lr * self.grads['b']\n",
        "        # head params\n",
        "        self.W_out   -= self.lr * self.grads['W_out']\n",
        "        self.b_out   -= self.lr * self.grads['b_out']\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Toy example\n",
        "    batch, T, f, C = 2, 5, 4, 3\n",
        "    n_nodes = 6\n",
        "    np.random.seed(0)\n",
        "\n",
        "    x = np.random.randn(batch, T, f)\n",
        "    y = np.random.randint(0, C, size=(batch,))\n",
        "\n",
        "    model = ScratchSimpleRNNClassifier(f, n_nodes, C, lr=1e-2, seed=0)\n",
        "    logits = model.forward(x)\n",
        "    loss   = model.backward(y)\n",
        "    model.update_params()\n",
        "\n",
        "    print(\"Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzWOacPgzt2",
        "outputId": "e2fb1cd9-9c0a-4b45-b93a-9a7d410c0265"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0985512704761449\n"
          ]
        }
      ]
    }
  ]
}