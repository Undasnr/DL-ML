{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Undasnr/DL-ML/blob/main/Ronny_SimpleConv2d_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXaGFoMVZGo6"
      },
      "source": [
        "**1. Creating a 2-D convolutional layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JieMKK7UY-Ry"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initializing weights (W) and biases (b).\n",
        "        # Shape of W: (out_channels, in_channels, filter_h, filter_w)\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Initializing biases to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Perform the convolution using nested loops.\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        # Extract the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Performing the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Looping to calculate dW and db.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Adding the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Checking for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DRmZwd4c16-"
      },
      "source": [
        "**2. Experiments with 2D convolutional layers on small arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5DaAmxDc8x7",
        "outputId": "8bb0e20c-505f-4526-9581-6ac8aa64a596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Problem 2: Testing Forward and Backward Propagation ---\n",
            "\n",
            "--- Forward Propagation ---\n",
            "Output (A):\n",
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Expected Output:\n",
            "[[[-4 -4]\n",
            "  [-4 -4]]\n",
            "\n",
            " [[ 1  1]\n",
            "  [ 1  1]]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Backward Propagation ---\n",
            "Gradient to previous layer (dX):\n",
            "[[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n",
            "\n",
            "Expected dX (based on manual calculation):\n",
            "[[[-5  4  0  0]\n",
            "  [13 27 -4  0]\n",
            "  [ 1  1 10  0]\n",
            "  [ 0  0  0  0]]]\n",
            "\n",
            "Backward pass matches expected dX: False\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initializing weights (W) and biases (b).\n",
        "        # Weights are initialized with small random values to break symmetry.\n",
        "        # Shape of W: (out_channels, in_channels, filter_h, filter_w)\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Biases are initialized to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Performing the convolution using nested loops.\n",
        "        for n in range(N):  # Loop over each sample in the batch\n",
        "            for m in range(self.out_channels):  # Loop over each output channel (filter)\n",
        "                for i in range(N_out_h):  # Loop over output height\n",
        "                    for j in range(N_out_w):  # Loop over output width\n",
        "                        # Extracting the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Performing the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Loop to calculate dW and db.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Adding the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Check for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "# Problem 2: Testing with small arrays\n",
        "print(\"--- Problem 2: Testing Forward and Backward Propagation ---\")\n",
        "\n",
        "# Input data for the forward pass\n",
        "# Shape: (batch_size=1, in_channels=1, height=4, width=4)\n",
        "x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])\n",
        "\n",
        "# Weights (filters) for the convolutional layer\n",
        "# Shape: (out_channels=2, in_channels=1, filter_h=3, filter_w=3)\n",
        "w = np.array(\n",
        "    [\n",
        "        [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],\n",
        "        [[0.0, 0.0, 0.0], [0.0, -1.0, 1.0], [0.0, 0.0, 0.0]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Biases (set to zeros for this test)\n",
        "b = np.zeros(2)\n",
        "\n",
        "# Creating an instance of the Conv2d layer\n",
        "conv_layer = Conv2d(in_channels=1, out_channels=2, filter_size=(3, 3))\n",
        "\n",
        "# Manually set the weights and biases to the specified values\n",
        "conv_layer.W = w.reshape(2, 1, 3, 3)\n",
        "conv_layer.b = b\n",
        "\n",
        "# Forward Propagation\n",
        "print(\"\\n--- Forward Propagation ---\")\n",
        "output_a = conv_layer.forward(x)\n",
        "print(f\"Output (A):\\n{output_a}\")\n",
        "\n",
        "# Expected output from the problem description\n",
        "expected_output_a = np.array([[[-4, -4], [-4, -4]], [[1, 1], [1, 1]]])\n",
        "print(f\"\\nExpected Output:\\n{expected_output_a}\")\n",
        "\n",
        "# Checking if the calculated output matches the expected output\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(output_a, expected_output_a)}\")\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "print(\"\\n--- Backward Propagation ---\")\n",
        "\n",
        "# Error from the subsequent layer (dA)\n",
        "# Shape: (batch_size=2, out_channels=2, H_out=2, W_out=2)\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "da_to_pass = delta.reshape(1, 2, 2, 2)\n",
        "\n",
        "# Pass the gradient to the backward method\n",
        "dx_to_pass = conv_layer.backward(da_to_pass)\n",
        "\n",
        "print(f\"Gradient to previous layer (dX):\\n{dx_to_pass}\")\n",
        "\n",
        "# Manually calculating the expected dX\n",
        "expected_dx = np.array([[[-5, 4, 0, 0], [13, 27, -4, 0], [1, 1, 10, 0], [0, 0, 0, 0]]])\n",
        "print(f\"\\nExpected dX (based on manual calculation):\\n{expected_dx}\")\n",
        "\n",
        "# Checking if the calculated dX matches the manually calculated dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(dx_to_pass, expected_dx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrHMjPMc15A"
      },
      "source": [
        "**3. Output size after 2-dimensional convolution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1jztZJVfFFg",
        "outputId": "4a7387ff-5f55-43a7-c47d-d9cad5e49ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Problem 2: Testing Forward and Backward Propagation ---\n",
            "\n",
            "--- Forward Propagation ---\n",
            "Output (A):\n",
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Expected Output:\n",
            "[[[-4 -4]\n",
            "  [-4 -4]]\n",
            "\n",
            " [[ 1  1]\n",
            "  [ 1  1]]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Backward Propagation ---\n",
            "Gradient to previous layer (dX):\n",
            "[[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n",
            "\n",
            "Expected dX (based on manual calculation):\n",
            "[[[-5  4  0  0]\n",
            "  [13 27 -4  0]\n",
            "  [ 1  1 10  0]\n",
            "  [ 0  0  0  0]]]\n",
            "\n",
            "Backward pass matches expected dX: False\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initialize weights (W) and biases (b).\n",
        "        # Weights are initialized with small random values to break symmetry.\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Biases are initialized to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Performing the convolution using nested loops.\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        # Extracting the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Perform the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Loop to calculate dW and db.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Adding the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Check for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_output_size(input_size, filter_size, padding, stride):\n",
        "        \"\"\"\n",
        "        Calculates the output size of a 2D convolutional layer based on the\n",
        "        input size, filter size, padding, and stride.\n",
        "\n",
        "        Args:\n",
        "            input_size (tuple): A tuple (input_h, input_w) representing the input size.\n",
        "            filter_size (tuple): A tuple (filter_h, filter_w) for the filter size.\n",
        "            padding (tuple): A tuple (padding_h, padding_w) for the padding.\n",
        "            stride (tuple): A tuple (stride_h, stride_w) for the stride.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (output_h, output_w) representing the output size.\n",
        "        \"\"\"\n",
        "        input_h, input_w = input_size\n",
        "        filter_h, filter_w = filter_size\n",
        "        padding_h, padding_w = padding\n",
        "        stride_h, stride_w = stride\n",
        "\n",
        "        # Calculating the output height and width using the provided formulas.\n",
        "        output_h = int((input_h + 2 * padding_h - filter_h) / stride_h) + 1\n",
        "        output_w = int((input_w + 2 * padding_w - filter_w) / stride_w) + 1\n",
        "\n",
        "        return output_h, output_w\n",
        "\n",
        "\n",
        "# Problem 2: Testing with small arrays\n",
        "print(\"--- Problem 2: Testing Forward and Backward Propagation ---\")\n",
        "\n",
        "# Input data for the forward pass\n",
        "# Shape: (batch_size=1, in_channels=1, height=4, width=4)\n",
        "x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])\n",
        "\n",
        "# Weights (filters) for the convolutional layer\n",
        "# Shape: (out_channels=2, in_channels=1, filter_h=3, filter_w=3)\n",
        "w = np.array(\n",
        "    [\n",
        "        [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],\n",
        "        [[0.0, 0.0, 0.0], [0.0, -1.0, 1.0], [0.0, 0.0, 0.0]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Biases (set to zeros for this test)\n",
        "b = np.zeros(2)\n",
        "\n",
        "# Creating an instance of the Conv2d layer\n",
        "conv_layer = Conv2d(in_channels=1, out_channels=2, filter_size=(3, 3))\n",
        "\n",
        "# Manually setting the weights and biases to the specified values\n",
        "conv_layer.W = w.reshape(2, 1, 3, 3)\n",
        "conv_layer.b = b\n",
        "\n",
        "# Forward Propagation\n",
        "print(\"\\n--- Forward Propagation ---\")\n",
        "output_a = conv_layer.forward(x)\n",
        "print(f\"Output (A):\\n{output_a}\")\n",
        "\n",
        "# Expected output from the problem description\n",
        "expected_output_a = np.array([[[-4, -4], [-4, -4]], [[1, 1], [1, 1]]])\n",
        "print(f\"\\nExpected Output:\\n{expected_output_a}\")\n",
        "\n",
        "# Checking if the calculated output matches the expected output\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(output_a, expected_output_a)}\")\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "print(\"\\n--- Backward Propagation ---\")\n",
        "\n",
        "# Error from the subsequent layer (dA)\n",
        "# Shape: (batch_size=2, out_channels=2, H_out=2, W_out=2)\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "da_to_pass = delta.reshape(1, 2, 2, 2)\n",
        "\n",
        "# Passing the gradient to the backward method\n",
        "dx_to_pass = conv_layer.backward(da_to_pass)\n",
        "\n",
        "print(f\"Gradient to previous layer (dX):\\n{dx_to_pass}\")\n",
        "\n",
        "# Manually calculating the expected dX\n",
        "expected_dx = np.array([[[-5, 4, 0, 0], [13, 27, -4, 0], [1, 1, 10, 0], [0, 0, 0, 0]]])\n",
        "print(f\"\\nExpected dX (based on manual calculation):\\n{expected_dx}\")\n",
        "\n",
        "# Checking if the calculated dX matches the manually calculated dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(dx_to_pass, expected_dx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFWGuEsEhRUW"
      },
      "source": [
        "**4. Creation of maximum pooling layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKvjkMehWhM",
        "outputId": "e3d4762d-d3aa-4f45-9dfc-a13be56d2a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\n",
            "\n",
            "--- Forward Propagation ---\n",
            "Output (A):\n",
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Expected Output:\n",
            "[[[-4 -4]\n",
            "  [-4 -4]]\n",
            "\n",
            " [[ 1  1]\n",
            "  [ 1  1]]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Backward Propagation ---\n",
            "Gradient to previous layer (dX):\n",
            "[[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n",
            "\n",
            "Expected dX (based on manual calculation):\n",
            "[[[-5  4  0  0]\n",
            "  [13 27 -4  0]\n",
            "  [ 1  1 10  0]\n",
            "  [ 0  0  0  0]]]\n",
            "\n",
            "Backward pass matches expected dX: False\n",
            "\n",
            "\n",
            "--- Problem 4: Testing MaxPool2D Layer ---\n",
            "\n",
            "--- MaxPool2D Forward Pass ---\n",
            "Input for pooling:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "Pooled Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Expected Pooled Output:\n",
            "[6 8]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- MaxPool2D Backward Pass ---\n",
            "Gradient passed to previous layer (dX):\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Expected dX:\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Backward pass matches expected dX: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initialize weights (W) and biases (b).\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Biases are initialized to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Performing the convolution using nested loops.\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        # Extracting the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Performing the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Loop to calculate dW and db.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Adding the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Checking for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_output_size(input_size, filter_size, padding, stride):\n",
        "        \"\"\"\n",
        "        Calculates the output size of a 2D convolutional layer based on the\n",
        "        input size, filter size, padding, and stride.\n",
        "\n",
        "        Args:\n",
        "            input_size (tuple): A tuple (input_h, input_w) representing the input size.\n",
        "            filter_size (tuple): A tuple (filter_h, filter_w) for the filter size.\n",
        "            padding (tuple): A tuple (padding_h, padding_w) for the padding.\n",
        "            stride (tuple): A tuple (stride_h, stride_w) for the stride.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (output_h, output_w) representing the output size.\n",
        "        \"\"\"\n",
        "        input_h, input_w = input_size\n",
        "        filter_h, filter_w = filter_size\n",
        "        padding_h, padding_w = padding\n",
        "        stride_h, stride_w = stride\n",
        "\n",
        "        # Calculating the output height and width using the provided formulas.\n",
        "        output_h = int((input_h + 2 * padding_h - filter_h) / stride_h) + 1\n",
        "        output_w = int((input_w + 2 * padding_w - filter_w) / stride_w) + 1\n",
        "\n",
        "        return output_h, output_w\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    A 2D maximum pooling layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs downsampling by taking the maximum value in a\n",
        "    specific window. It retains the indices of the maximum values for\n",
        "    the backward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        \"\"\"\n",
        "        Initializes the MaxPool2D layer.\n",
        "\n",
        "        Args:\n",
        "            pool_size (tuple): The height and width of the pooling window.\n",
        "            stride (tuple): The stride for the pooling window.\n",
        "        \"\"\"\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "\n",
        "        # Storing the indices of the maximum values for the backward pass\n",
        "        self.max_indices = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the max pooling layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array after pooling.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "\n",
        "        # Initializing output array and a mask to store the max indices\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "        self.max_indices = np.zeros_like(X, dtype=bool)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        # Defining the pooling region\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Finding the maximum value in the region and its index\n",
        "                        max_val = np.max(region)\n",
        "                        max_val_idx = np.argmax(region)\n",
        "\n",
        "                        # Storing the maximum value in the output array\n",
        "                        A[n, c, i, j] = max_val\n",
        "\n",
        "                        # Updating the mask with the correct indices\n",
        "                        h_idx = h_start + max_val_idx // self.pool_w\n",
        "                        w_idx = w_start + max_val_idx % self.pool_w\n",
        "\n",
        "                        self.max_indices[n, c, h_idx, w_idx] = True\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the max pooling layer.\n",
        "\n",
        "        The error is passed only to the neuron that had the maximum\n",
        "        activation during the forward pass.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the\n",
        "                             same shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer.\n",
        "        \"\"\"\n",
        "        # Initializing the gradient to the previous layer with zeros\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dX[self.max_indices] = dA.ravel()\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "# Problem 2: Testing with small arrays\n",
        "print(\"--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\")\n",
        "\n",
        "# Input data for the forward pass\n",
        "# Shape: (batch_size=1, in_channels=1, height=4, width=4)\n",
        "x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])\n",
        "\n",
        "# Weights (filters) for the convolutional layer\n",
        "# Shape: (out_channels=2, in_channels=1, filter_h=3, filter_w=3)\n",
        "w = np.array(\n",
        "    [\n",
        "        [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],\n",
        "        [[0.0, 0.0, 0.0], [0.0, -1.0, 1.0], [0.0, 0.0, 0.0]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Biases (set to zeros for this test)\n",
        "b = np.zeros(2)\n",
        "\n",
        "# Creating an instance of the Conv2d layer\n",
        "conv_layer = Conv2d(in_channels=1, out_channels=2, filter_size=(3, 3))\n",
        "\n",
        "# Manually setting the weights and biases to the specified values\n",
        "conv_layer.W = w.reshape(2, 1, 3, 3)\n",
        "conv_layer.b = b\n",
        "\n",
        "# Forward Propagation\n",
        "print(\"\\n--- Forward Propagation ---\")\n",
        "output_a = conv_layer.forward(x)\n",
        "print(f\"Output (A):\\n{output_a}\")\n",
        "\n",
        "# Expected output from the problem description\n",
        "expected_output_a = np.array([[[-4, -4], [-4, -4]], [[1, 1], [1, 1]]])\n",
        "print(f\"\\nExpected Output:\\n{expected_output_a}\")\n",
        "\n",
        "# Checking if the calculated output matches the expected output\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(output_a, expected_output_a)}\")\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "print(\"\\n--- Backward Propagation ---\")\n",
        "\n",
        "# Error from the subsequent layer (dA)\n",
        "# Shape: (batch_size=2, out_channels=2, H_out=2, W_out=2)\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "da_to_pass = delta.reshape(1, 2, 2, 2)\n",
        "\n",
        "# Passing the gradient to the backward method\n",
        "dx_to_pass = conv_layer.backward(da_to_pass)\n",
        "\n",
        "print(f\"Gradient to previous layer (dX):\\n{dx_to_pass}\")\n",
        "\n",
        "# Manually calculating the expected dX\n",
        "expected_dx = np.array([[[-5, 4, 0, 0], [13, 27, -4, 0], [1, 1, 10, 0], [0, 0, 0, 0]]])\n",
        "print(f\"\\nExpected dX (based on manual calculation):\\n{expected_dx}\")\n",
        "\n",
        "# Checking if the calculated dX matches the manually calculated dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(dx_to_pass, expected_dx)}\")\n",
        "\n",
        "\n",
        "# Problem 4: Testing MaxPool2D\n",
        "print(\"\\n\\n--- Problem 4: Testing MaxPool2D Layer ---\")\n",
        "\n",
        "# Creating a sample input for pooling\n",
        "# Shape: (batch_size=1, channels=1, height=4, width=4)\n",
        "pool_x = np.array([[[[1, 2, 3, 4],\n",
        "                     [5, 6, 7, 8],\n",
        "                     [9, 10, 11, 12],\n",
        "                     [13, 14, 15, 16]]]])\n",
        "\n",
        "# Defining pooling parameters\n",
        "pool_size = (2, 2)\n",
        "stride = (2, 2)\n",
        "\n",
        "# Creating a MaxPool2D instance\n",
        "max_pool_layer = MaxPool2D(pool_size=pool_size, stride=stride)\n",
        "\n",
        "# Forward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Forward Pass ---\")\n",
        "pooled_output = max_pool_layer.forward(pool_x)\n",
        "print(f\"Input for pooling:\\n{pool_x[0, 0]}\")\n",
        "print(f\"\\nPooled Output:\\n{pooled_output[0, 0]}\")\n",
        "\n",
        "# Expected pooled output\n",
        "expected_pooled_output = np.array([[[6, 8], [14, 16]]])\n",
        "print(f\"\\nExpected Pooled Output:\\n{expected_pooled_output[0, 0]}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(pooled_output, expected_pooled_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 1, 2, 2)\n",
        "pooled_grad = np.array([[[[1, 2], [3, 4]]]])\n",
        "grad_to_pass = max_pool_layer.backward(pooled_grad)\n",
        "print(f\"Gradient passed to previous layer (dX):\\n{grad_to_pass[0, 0]}\")\n",
        "\n",
        "# Manually calculating the expected gradient\n",
        "expected_grad = np.array([[[[0, 0, 0, 0],\n",
        "                           [0, 1, 0, 2],\n",
        "                           [0, 0, 0, 0],\n",
        "                           [0, 3, 0, 4]]]])\n",
        "print(f\"\\nExpected dX:\\n{expected_grad[0, 0]}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(grad_to_pass, expected_grad)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LnRkOkyj_fV"
      },
      "source": [
        "**5. Creating average pooling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HSKFOi2kEnc",
        "outputId": "e2524aef-e68f-4233-d8a6-bc90593ef315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\n",
            "\n",
            "--- Forward Propagation ---\n",
            "Output (A):\n",
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Expected Output:\n",
            "[[[-4 -4]\n",
            "  [-4 -4]]\n",
            "\n",
            " [[ 1  1]\n",
            "  [ 1  1]]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Backward Propagation ---\n",
            "Gradient to previous layer (dX):\n",
            "[[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n",
            "\n",
            "Expected dX (based on manual calculation):\n",
            "[[[-5  4  0  0]\n",
            "  [13 27 -4  0]\n",
            "  [ 1  1 10  0]\n",
            "  [ 0  0  0  0]]]\n",
            "\n",
            "Backward pass matches expected dX: False\n",
            "\n",
            "\n",
            "--- Problem 4: Testing MaxPool2D Layer ---\n",
            "\n",
            "--- MaxPool2D Forward Pass ---\n",
            "Input for pooling:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "Pooled Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Expected Pooled Output:\n",
            "[6 8]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- MaxPool2D Backward Pass ---\n",
            "Gradient passed to previous layer (dX):\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Expected dX:\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Backward pass matches expected dX: True\n",
            "\n",
            "\n",
            "--- Problem 5: Testing AveragePool2D Layer ---\n",
            "\n",
            "--- AveragePool2D Forward Pass ---\n",
            "Input for pooling:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "Average Pooled Output:\n",
            "[[ 3.5  5.5]\n",
            " [11.5 13.5]]\n",
            "\n",
            "Expected Average Pooled Output:\n",
            "[3.5 5.5]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- AveragePool2D Backward Pass ---\n",
            "Gradient passed to previous layer (dX):\n",
            "[[0.25 0.25 0.5  0.5 ]\n",
            " [0.25 0.25 0.5  0.5 ]\n",
            " [0.75 0.75 1.   1.  ]\n",
            " [0.75 0.75 1.   1.  ]]\n",
            "\n",
            "Expected dX:\n",
            "[[0.25 0.25 0.5  0.5 ]\n",
            " [0.25 0.25 0.5  0.5 ]\n",
            " [0.75 0.75 1.   1.  ]\n",
            " [0.75 0.75 1.   1.  ]]\n",
            "\n",
            "Backward pass matches expected dX: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initialize weights (W) and biases (b).\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Biases are initialized to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Performing the convolution using nested loops.\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        # Extracting the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Perform the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Loop to calculate dW and db using the provided formulas.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Add the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Check for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_output_size(input_size, filter_size, padding, stride):\n",
        "        \"\"\"\n",
        "        Calculates the output size of a 2D convolutional layer based on the\n",
        "        input size, filter size, padding, and stride.\n",
        "\n",
        "        Args:\n",
        "            input_size (tuple): A tuple (input_h, input_w) representing the input size.\n",
        "            filter_size (tuple): A tuple (filter_h, filter_w) for the filter size.\n",
        "            padding (tuple): A tuple (padding_h, padding_w) for the padding.\n",
        "            stride (tuple): A tuple (stride_h, stride_w) for the stride.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (output_h, output_w) representing the output size.\n",
        "        \"\"\"\n",
        "        input_h, input_w = input_size\n",
        "        filter_h, filter_w = filter_size\n",
        "        padding_h, padding_w = padding\n",
        "        stride_h, stride_w = stride\n",
        "\n",
        "        # Calculating the output height and width.\n",
        "        output_h = int((input_h + 2 * padding_h - filter_h) / stride_h) + 1\n",
        "        output_w = int((input_w + 2 * padding_w - filter_w) / stride_w) + 1\n",
        "\n",
        "        return output_h, output_w\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    A 2D maximum pooling layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs downsampling by taking the maximum value in a\n",
        "    specific window. It retains the indices of the maximum values for\n",
        "    the backward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        \"\"\"\n",
        "        Initializes the MaxPool2D layer.\n",
        "\n",
        "        Args:\n",
        "            pool_size (tuple): The height and width of the pooling window.\n",
        "            stride (tuple): The stride for the pooling window.\n",
        "        \"\"\"\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "\n",
        "        # Storing the indices of the maximum values for the backward pass\n",
        "        self.max_indices = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the max pooling layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array after pooling.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "\n",
        "        # Initializing output array and a mask to store the max indices\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "        self.max_indices = np.zeros_like(X, dtype=bool)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        # Defining the pooling region\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Finding the maximum value in the region and its index\n",
        "                        max_val = np.max(region)\n",
        "                        max_val_idx = np.argmax(region)\n",
        "\n",
        "                        # Storing the maximum value in the output array\n",
        "                        A[n, c, i, j] = max_val\n",
        "\n",
        "                        # Updating the mask with the correct indices\n",
        "                        h_idx = h_start + max_val_idx // self.pool_w\n",
        "                        w_idx = w_start + max_val_idx % self.pool_w\n",
        "\n",
        "                        self.max_indices[n, c, h_idx, w_idx] = True\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the max pooling layer.\n",
        "\n",
        "        The error is passed only to the neuron that had the maximum\n",
        "        activation during the forward pass.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the\n",
        "                             same shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer.\n",
        "        \"\"\"\n",
        "        # Initializing the gradient to the previous layer with zeros\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dX[self.max_indices] = dA.ravel()\n",
        "\n",
        "        return dX\n",
        "\n",
        "class AveragePool2D:\n",
        "    \"\"\"\n",
        "    A 2D average pooling layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs downsampling by taking the average value in a\n",
        "    specific window.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        \"\"\"\n",
        "        Initializes the AveragePool2D layer.\n",
        "\n",
        "        Args:\n",
        "            pool_size (tuple): The height and width of the pooling window.\n",
        "            stride (tuple): The stride for the pooling window.\n",
        "        \"\"\"\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the average pooling layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array after pooling.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        # Defining the pooling region\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Calculating the average of the region\n",
        "                        avg_val = np.mean(region)\n",
        "\n",
        "                        # Storing the average value in the output array\n",
        "                        A[n, c, i, j] = avg_val\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the average pooling layer.\n",
        "\n",
        "        The error is distributed equally to all neurons in the pooling\n",
        "        region, since the forward pass is a sum divided by the number of elements.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer.\n",
        "        \"\"\"\n",
        "        # Initializing the gradient to the previous layer with zeros, ensuring it's a float type\n",
        "        dX = np.zeros(self.X.shape, dtype=np.float64)\n",
        "        N, C, H_out, W_out = dA.shape\n",
        "\n",
        "        # Calculating the number of elements in the pooling window\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        distributed_grad = dA[n, c, i, j] / pool_size\n",
        "\n",
        "                        # Defining the region in the original input\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        # Adding the distributed gradient to the corresponding region in dX\n",
        "                        dX[n, c, h_start:h_end, w_start:w_end] += distributed_grad\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "# Problem 2: Testing with small arrays\n",
        "print(\"--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\")\n",
        "\n",
        "# Input data for the forward pass\n",
        "# Shape: (batch_size=1, in_channels=1, height=4, width=4)\n",
        "x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])\n",
        "\n",
        "# Weights (filters) for the convolutional layer\n",
        "# Shape: (out_channels=2, in_channels=1, filter_h=3, filter_w=3)\n",
        "w = np.array(\n",
        "    [\n",
        "        [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],\n",
        "        [[0.0, 0.0, 0.0], [0.0, -1.0, 1.0], [0.0, 0.0, 0.0]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Biases (set to zeros for this test)\n",
        "b = np.zeros(2)\n",
        "\n",
        "# Creating an instance of the Conv2d layer\n",
        "conv_layer = Conv2d(in_channels=1, out_channels=2, filter_size=(3, 3))\n",
        "\n",
        "# Manually setting the weights and biases to the specified values\n",
        "conv_layer.W = w.reshape(2, 1, 3, 3)\n",
        "conv_layer.b = b\n",
        "\n",
        "# Forward Propagation\n",
        "print(\"\\n--- Forward Propagation ---\")\n",
        "output_a = conv_layer.forward(x)\n",
        "print(f\"Output (A):\\n{output_a}\")\n",
        "\n",
        "# Expected output from the problem description\n",
        "expected_output_a = np.array([[[-4, -4], [-4, -4]], [[1, 1], [1, 1]]])\n",
        "print(f\"\\nExpected Output:\\n{expected_output_a}\")\n",
        "\n",
        "# Checking if the calculated output matches the expected output\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(output_a, expected_output_a)}\")\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "print(\"\\n--- Backward Propagation ---\")\n",
        "\n",
        "# Error from the subsequent layer (dA)\n",
        "# Shape: (batch_size=2, out_channels=2, H_out=2, W_out=2)\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "da_to_pass = delta.reshape(1, 2, 2, 2)\n",
        "\n",
        "# Passing the gradient to the backward method\n",
        "dx_to_pass = conv_layer.backward(da_to_pass)\n",
        "\n",
        "print(f\"Gradient to previous layer (dX):\\n{dx_to_pass}\")\n",
        "\n",
        "# Manually calculating the expected dX\n",
        "expected_dx = np.array([[[-5, 4, 0, 0], [13, 27, -4, 0], [1, 1, 10, 0], [0, 0, 0, 0]]])\n",
        "print(f\"\\nExpected dX (based on manual calculation):\\n{expected_dx}\")\n",
        "\n",
        "# Checking if the calculated dX matches the manually calculated dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(dx_to_pass, expected_dx)}\")\n",
        "\n",
        "\n",
        "# Problem 4: Testing MaxPool2D\n",
        "print(\"\\n\\n--- Problem 4: Testing MaxPool2D Layer ---\")\n",
        "\n",
        "# Creating a sample input for pooling\n",
        "# Shape: (batch_size=1, channels=1, height=4, width=4)\n",
        "pool_x = np.array([[[[1, 2, 3, 4],\n",
        "                     [5, 6, 7, 8],\n",
        "                     [9, 10, 11, 12],\n",
        "                     [13, 14, 15, 16]]]])\n",
        "\n",
        "# Defining pooling parameters\n",
        "pool_size = (2, 2)\n",
        "stride = (2, 2)\n",
        "\n",
        "# Creating a MaxPool2D instance\n",
        "max_pool_layer = MaxPool2D(pool_size=pool_size, stride=stride)\n",
        "\n",
        "# Forward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Forward Pass ---\")\n",
        "pooled_output = max_pool_layer.forward(pool_x)\n",
        "print(f\"Input for pooling:\\n{pool_x[0, 0]}\")\n",
        "print(f\"\\nPooled Output:\\n{pooled_output[0, 0]}\")\n",
        "\n",
        "# Expected pooled output\n",
        "expected_pooled_output = np.array([[[6, 8], [14, 16]]])\n",
        "print(f\"\\nExpected Pooled Output:\\n{expected_pooled_output[0, 0]}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(pooled_output, expected_pooled_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 1, 2, 2)\n",
        "pooled_grad = np.array([[[[1, 2], [3, 4]]]])\n",
        "grad_to_pass = max_pool_layer.backward(pooled_grad)\n",
        "print(f\"Gradient passed to previous layer (dX):\\n{grad_to_pass[0, 0]}\")\n",
        "\n",
        "# Manually calculating the expected gradient\n",
        "expected_grad = np.array([[[[0, 0, 0, 0],\n",
        "                           [0, 1, 0, 2],\n",
        "                           [0, 0, 0, 0],\n",
        "                           [0, 3, 0, 4]]]])\n",
        "print(f\"\\nExpected dX:\\n{expected_grad[0, 0]}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(grad_to_pass, expected_grad)}\")\n",
        "\n",
        "\n",
        "# Problem 5: Testing AveragePool2D\n",
        "print(\"\\n\\n--- Problem 5: Testing AveragePool2D Layer ---\")\n",
        "\n",
        "# Creating a sample input for average pooling\n",
        "# Shape: (batch_size=1, channels=1, height=4, width=4)\n",
        "avg_pool_x = np.array([[[[1, 2, 3, 4],\n",
        "                        [5, 6, 7, 8],\n",
        "                        [9, 10, 11, 12],\n",
        "                        [13, 14, 15, 16]]]])\n",
        "\n",
        "# Defining pooling parameters\n",
        "avg_pool_size = (2, 2)\n",
        "avg_stride = (2, 2)\n",
        "\n",
        "# Creating an AveragePool2D instance\n",
        "avg_pool_layer = AveragePool2D(pool_size=avg_pool_size, stride=avg_stride)\n",
        "\n",
        "# Forward Pass for Average Pooling\n",
        "print(\"\\n--- AveragePool2D Forward Pass ---\")\n",
        "avg_pooled_output = avg_pool_layer.forward(avg_pool_x)\n",
        "print(f\"Input for pooling:\\n{avg_pool_x[0, 0]}\")\n",
        "print(f\"\\nAverage Pooled Output:\\n{avg_pooled_output[0, 0]}\")\n",
        "\n",
        "# Expected average pooled output\n",
        "expected_avg_output = np.array([[[3.5, 5.5], [11.5, 13.5]]])\n",
        "print(f\"\\nExpected Average Pooled Output:\\n{expected_avg_output[0, 0]}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(avg_pooled_output, expected_avg_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Average Pooling\n",
        "print(\"\\n--- AveragePool2D Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 1, 2, 2)\n",
        "avg_pooled_grad = np.array([[[[1, 2], [3, 4]]]])\n",
        "avg_grad_to_pass = avg_pool_layer.backward(avg_pooled_grad)\n",
        "print(f\"Gradient passed to previous layer (dX):\\n{avg_grad_to_pass[0, 0]}\")\n",
        "\n",
        "# Manually calculating the expected gradient\n",
        "expected_avg_grad = np.array([[[[0.25, 0.25, 0.5, 0.5],\n",
        "                              [0.25, 0.25, 0.5, 0.5],\n",
        "                              [0.75, 0.75, 1.0, 1.0],\n",
        "                              [0.75, 0.75, 1.0, 1.0]]]])\n",
        "print(f\"\\nExpected dX:\\n{expected_avg_grad[0, 0]}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(avg_grad_to_pass, expected_avg_grad)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y16sNDJvqKks"
      },
      "source": [
        "**6. Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-XQunuOqP8S",
        "outputId": "1a5315cb-2be2-4d81-c5b5-547f29051ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\n",
            "\n",
            "--- Forward Propagation ---\n",
            "Output (A):\n",
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Expected Output:\n",
            "[[[-4 -4]\n",
            "  [-4 -4]]\n",
            "\n",
            " [[ 1  1]\n",
            "  [ 1  1]]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Backward Propagation ---\n",
            "Gradient to previous layer (dX):\n",
            "[[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n",
            "\n",
            "Expected dX (based on manual calculation):\n",
            "[[[-5  4  0  0]\n",
            "  [13 27 -4  0]\n",
            "  [ 1  1 10  0]\n",
            "  [ 0  0  0  0]]]\n",
            "\n",
            "Backward pass matches expected dX: False\n",
            "\n",
            "\n",
            "--- Problem 4: Testing MaxPool2D Layer ---\n",
            "\n",
            "--- MaxPool2D Forward Pass ---\n",
            "Input for pooling:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "Pooled Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Expected Pooled Output:\n",
            "[6 8]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- MaxPool2D Backward Pass ---\n",
            "Gradient passed to previous layer (dX):\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Expected dX:\n",
            "[[0 0 0 0]\n",
            " [0 1 0 2]\n",
            " [0 0 0 0]\n",
            " [0 3 0 4]]\n",
            "\n",
            "Backward pass matches expected dX: True\n",
            "\n",
            "\n",
            "--- Problem 5: Testing AveragePool2D Layer ---\n",
            "\n",
            "--- AveragePool2D Forward Pass ---\n",
            "Input for pooling:\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]\n",
            " [13 14 15 16]]\n",
            "\n",
            "Average Pooled Output:\n",
            "[[ 3.5  5.5]\n",
            " [11.5 13.5]]\n",
            "\n",
            "Expected Average Pooled Output:\n",
            "[3.5 5.5]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- AveragePool2D Backward Pass ---\n",
            "Gradient passed to previous layer (dX):\n",
            "[[0.25 0.25 0.5  0.5 ]\n",
            " [0.25 0.25 0.5  0.5 ]\n",
            " [0.75 0.75 1.   1.  ]\n",
            " [0.75 0.75 1.   1.  ]]\n",
            "\n",
            "Expected dX:\n",
            "[[0.25 0.25 0.5  0.5 ]\n",
            " [0.25 0.25 0.5  0.5 ]\n",
            " [0.75 0.75 1.   1.  ]\n",
            " [0.75 0.75 1.   1.  ]]\n",
            "\n",
            "Backward pass matches expected dX: True\n",
            "\n",
            "\n",
            "--- Problem 6: Testing Flatten Layer ---\n",
            "\n",
            "--- Flatten Forward Pass ---\n",
            "Original shape: (1, 2, 2, 2)\n",
            "Flattened output shape: (1, 8)\n",
            "Flattened output:\n",
            "[[1 2 3 4 5 6 7 8]]\n",
            "\n",
            "Expected Flattened Output:\n",
            "[[1 2 3 4 5 6 7 8]]\n",
            "\n",
            "Forward pass matches expected output: True\n",
            "\n",
            "--- Flatten Backward Pass ---\n",
            "Original gradient shape: (1, 8)\n",
            "Reshaped gradient shape: (1, 2, 2, 2)\n",
            "Reshaped gradient to previous layer (dX):\n",
            "[[[[1 2]\n",
            "   [3 4]]\n",
            "\n",
            "  [[5 6]\n",
            "   [7 8]]]]\n",
            "\n",
            "Expected Reshaped Gradient:\n",
            "[[[[1 2]\n",
            "   [3 4]]\n",
            "\n",
            "  [[5 6]\n",
            "   [7 8]]]]\n",
            "\n",
            "Backward pass matches expected dX: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs the forward and backward passes for a convolutional\n",
        "    layer, following the mathematical formulas provided. It assumes a\n",
        "    data format of (batch_size, channels, height, width) (NCHW).\n",
        "    For simplicity, this implementation does not include support for\n",
        "    padding or strides other than 1, as implied by the given formulas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        \"\"\"\n",
        "        Initializes the Conv2d layer with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input array.\n",
        "            out_channels (int): The number of channels in the output array (number of filters).\n",
        "            filter_size (tuple): The height and width of the filter, e.g., (3, 3).\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        # Initializing weights (W) and biases (b).\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "\n",
        "        # Biases are initialized to zeros. Shape: (out_channels,)\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        # Gradients for weights and biases, to be calculated during backward pass.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "        # Storing the input array (X) for use in the backward pass.\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: in_channels, H: input height, W: input width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array (A) after convolution, of shape (N, M, H_out, W_out).\n",
        "                        N: batch size, M: out_channels, H_out, W_out: output dimensions.\n",
        "        \"\"\"\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = X\n",
        "\n",
        "        # Getting input dimensions\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions.\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "\n",
        "        # Initializing the output array with zeros.\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        # Performing the convolution using nested loops.\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        # Extracting the receptive field (region of the input being convolved)\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Performing the element-wise multiplication and summation.\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the convolutional layer.\n",
        "\n",
        "        This method calculates the gradients for the weights (dW) and biases (db)\n",
        "        and the error to be passed to the previous layer (dX).\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the same\n",
        "                             shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer, of the\n",
        "                        same shape as the input of the forward pass.\n",
        "        \"\"\"\n",
        "        # Getting dimensions of input and output arrays\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        # Initializing gradients to zeros\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        # Loop to calculate dW and db using the provided formulas.\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # Extracting the receptive field from the original input\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "\n",
        "                        # Adding the contribution of this specific output element to the gradients\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "\n",
        "                # The sum of all ∂L/∂a_i,j,m for a given output channel gives the bias gradient.\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        # Loop to calculate the error to be passed to the previous layer (dX).\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    # Checking for valid indices\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_output_size(input_size, filter_size, padding, stride):\n",
        "        \"\"\"\n",
        "        Calculates the output size of a 2D convolutional layer based on the\n",
        "        input size, filter size, padding, and stride.\n",
        "\n",
        "        Args:\n",
        "            input_size (tuple): A tuple (input_h, input_w) representing the input size.\n",
        "            filter_size (tuple): A tuple (filter_h, filter_w) for the filter size.\n",
        "            padding (tuple): A tuple (padding_h, padding_w) for the padding.\n",
        "            stride (tuple): A tuple (stride_h, stride_w) for the stride.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple (output_h, output_w) representing the output size.\n",
        "        \"\"\"\n",
        "        input_h, input_w = input_size\n",
        "        filter_h, filter_w = filter_size\n",
        "        padding_h, padding_w = padding\n",
        "        stride_h, stride_w = stride\n",
        "\n",
        "        # Calculating the output height and width using the provided formulas.\n",
        "        output_h = int((input_h + 2 * padding_h - filter_h) / stride_h) + 1\n",
        "        output_w = int((input_w + 2 * padding_w - filter_w) / stride_w) + 1\n",
        "\n",
        "        return output_h, output_w\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    A 2D maximum pooling layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs downsampling by taking the maximum value in a\n",
        "    specific window. It retains the indices of the maximum values for\n",
        "    the backward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        \"\"\"\n",
        "        Initializes the MaxPool2D layer.\n",
        "\n",
        "        Args:\n",
        "            pool_size (tuple): The height and width of the pooling window.\n",
        "            stride (tuple): The stride for the pooling window.\n",
        "        \"\"\"\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "\n",
        "        # Storing the indices of the maximum values for the backward pass\n",
        "        self.max_indices = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the max pooling layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array after pooling.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "\n",
        "        # Initializing output array and a mask to store the max indices\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "        self.max_indices = np.zeros_like(X, dtype=bool)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        # Defining the pooling region\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Finding the maximum value in the region and its index\n",
        "                        max_val = np.max(region)\n",
        "                        max_val_idx = np.argmax(region)\n",
        "\n",
        "                        # Store the maximum value in the output array\n",
        "                        A[n, c, i, j] = max_val\n",
        "\n",
        "                        # Updating the mask with the correct indices\n",
        "                        h_idx = h_start + max_val_idx // self.pool_w\n",
        "                        w_idx = w_start + max_val_idx % self.pool_w\n",
        "\n",
        "                        self.max_indices[n, c, h_idx, w_idx] = True\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the max pooling layer.\n",
        "\n",
        "        The error is passed only to the neuron that had the maximum\n",
        "        activation during the forward pass.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer, of the\n",
        "                             same shape as the output of the forward pass.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer.\n",
        "        \"\"\"\n",
        "        # Initializing the gradient to the previous layer with zeros\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dX[self.max_indices] = dA.ravel()\n",
        "\n",
        "        return dX\n",
        "\n",
        "class AveragePool2D:\n",
        "    \"\"\"\n",
        "    A 2D average pooling layer implemented from scratch using NumPy.\n",
        "\n",
        "    This class performs downsampling by taking the average value in a\n",
        "    specific window.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        \"\"\"\n",
        "        Initializes the AveragePool2D layer.\n",
        "\n",
        "        Args:\n",
        "            pool_size (tuple): The height and width of the pooling window.\n",
        "            stride (tuple): The stride for the pooling window.\n",
        "        \"\"\"\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "\n",
        "        # Storing the input for the backward pass\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the average pooling layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output array after pooling.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "\n",
        "        # Calculating output dimensions\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "\n",
        "        # Initializing output array\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        # Defining the pooling region\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Calculating the average of the region\n",
        "                        avg_val = np.mean(region)\n",
        "\n",
        "                        # Storing the average value in the output array\n",
        "                        A[n, c, i, j] = avg_val\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the average pooling layer.\n",
        "\n",
        "        The error is distributed equally to all neurons in the pooling\n",
        "        region, since the forward pass is a sum divided by the number of elements.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The gradient to be passed to the previous layer.\n",
        "        \"\"\"\n",
        "        # Initializing the gradient to the previous layer with zeros, ensuring it's a float type\n",
        "        dX = np.zeros(self.X.shape, dtype=np.float64)\n",
        "        N, C, H_out, W_out = dA.shape\n",
        "\n",
        "        # Calculating the number of elements in the pooling window\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        # The error is divided evenly across all elements\n",
        "                        distributed_grad = dA[n, c, i, j] / pool_size\n",
        "\n",
        "                        # Defining the region in the original input\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        # Adding the distributed gradient to the corresponding region in dX\n",
        "                        dX[n, c, h_start:h_end, w_start:w_end] += distributed_grad\n",
        "\n",
        "        return dX\n",
        "\n",
        "class Flatten:\n",
        "    \"\"\"\n",
        "    A flattening layer that reshapes a multi-dimensional input into a\n",
        "    one-dimensional vector. This is typically used to transition from\n",
        "    convolutional layers to fully-connected layers.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Flatten layer. It doesn't have any parameters,\n",
        "        but it needs to store the input shape for the backward pass.\n",
        "        \"\"\"\n",
        "        # Stores the original shape of the input array.\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward propagation for the flattening layer.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): The input array of shape (N, C, H, W).\n",
        "                            N: batch size, C: channels, H: height, W: width.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The flattened output array of shape (N, C*H*W).\n",
        "        \"\"\"\n",
        "        # Storing the original shape for the backward pass\n",
        "        self.input_shape = X.shape\n",
        "\n",
        "        # Reshaping the input array to a 2D array\n",
        "        flattened_output = X.reshape(X.shape[0], -1)\n",
        "\n",
        "        return flattened_output\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs the backpropagation for the flattening layer.\n",
        "\n",
        "        It reshapes the gradient from the subsequent layer back into\n",
        "        the original input shape.\n",
        "\n",
        "        Args:\n",
        "            dA (np.ndarray): The gradient from the subsequent layer,\n",
        "                             of shape (N, C*H*W).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The reshaped gradient to be passed to the previous\n",
        "                        layer, of the same shape as the original input.\n",
        "        \"\"\"\n",
        "        # Reshape the incoming gradient back to the original input shape\n",
        "        reshaped_grad = dA.reshape(self.input_shape)\n",
        "\n",
        "        return reshaped_grad\n",
        "\n",
        "\n",
        "# Problem 2: Testing with small arrays\n",
        "print(\"--- Problem 2: Testing Conv2d Forward and Backward Propagation ---\")\n",
        "\n",
        "# Input data for the forward pass\n",
        "# Shape: (batch_size=1, in_channels=1, height=4, width=4)\n",
        "x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])\n",
        "\n",
        "# Weights (filters) for the convolutional layer\n",
        "# Shape: (out_channels=2, in_channels=1, filter_h=3, filter_w=3)\n",
        "w = np.array(\n",
        "    [\n",
        "        [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],\n",
        "        [[0.0, 0.0, 0.0], [0.0, -1.0, 1.0], [0.0, 0.0, 0.0]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Biases (set to zeros for this test)\n",
        "b = np.zeros(2)\n",
        "\n",
        "# Creating an instance of the Conv2d layer\n",
        "conv_layer = Conv2d(in_channels=1, out_channels=2, filter_size=(3, 3))\n",
        "\n",
        "# Manually setting the weights and biases to the specified values\n",
        "conv_layer.W = w.reshape(2, 1, 3, 3)\n",
        "conv_layer.b = b\n",
        "\n",
        "# Forward Propagation\n",
        "print(\"\\n--- Forward Propagation ---\")\n",
        "output_a = conv_layer.forward(x)\n",
        "print(f\"Output (A):\\n{output_a}\")\n",
        "\n",
        "# Expected output from the problem description\n",
        "expected_output_a = np.array([[[-4, -4], [-4, -4]], [[1, 1], [1, 1]]])\n",
        "print(f\"\\nExpected Output:\\n{expected_output_a}\")\n",
        "\n",
        "# Checking if the calculated output matches the expected output\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(output_a, expected_output_a)}\")\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "print(\"\\n--- Backward Propagation ---\")\n",
        "\n",
        "# Error from the subsequent layer (dA)\n",
        "# Shape: (batch_size=2, out_channels=2, H_out=2, W_out=2)\n",
        "delta = np.array([[[-4, -4], [10, 11]], [[1, -7], [1, -11]]])\n",
        "da_to_pass = delta.reshape(1, 2, 2, 2)\n",
        "\n",
        "# Passing the gradient to the backward method\n",
        "dx_to_pass = conv_layer.backward(da_to_pass)\n",
        "\n",
        "print(f\"Gradient to previous layer (dX):\\n{dx_to_pass}\")\n",
        "\n",
        "# Manually calculating the expected dX\n",
        "expected_dx = np.array([[[-5, 4, 0, 0], [13, 27, -4, 0], [1, 1, 10, 0], [0, 0, 0, 0]]])\n",
        "print(f\"\\nExpected dX (based on manual calculation):\\n{expected_dx}\")\n",
        "\n",
        "# Checking if the calculated dX matches the manually calculated dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(dx_to_pass, expected_dx)}\")\n",
        "\n",
        "\n",
        "# Problem 4: Testing MaxPool2D\n",
        "print(\"\\n\\n--- Problem 4: Testing MaxPool2D Layer ---\")\n",
        "\n",
        "# Creating a sample input for pooling\n",
        "# Shape: (batch_size=1, channels=1, height=4, width=4)\n",
        "pool_x = np.array([[[[1, 2, 3, 4],\n",
        "                     [5, 6, 7, 8],\n",
        "                     [9, 10, 11, 12],\n",
        "                     [13, 14, 15, 16]]]])\n",
        "\n",
        "# Defining pooling parameters\n",
        "pool_size = (2, 2)\n",
        "stride = (2, 2)\n",
        "\n",
        "# Creating a MaxPool2D instance\n",
        "max_pool_layer = MaxPool2D(pool_size=pool_size, stride=stride)\n",
        "\n",
        "# Forward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Forward Pass ---\")\n",
        "pooled_output = max_pool_layer.forward(pool_x)\n",
        "print(f\"Input for pooling:\\n{pool_x[0, 0]}\")\n",
        "print(f\"\\nPooled Output:\\n{pooled_output[0, 0]}\")\n",
        "\n",
        "# Expected pooled output\n",
        "expected_pooled_output = np.array([[[6, 8], [14, 16]]])\n",
        "print(f\"\\nExpected Pooled Output:\\n{expected_pooled_output[0, 0]}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(pooled_output, expected_pooled_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Pooling\n",
        "print(\"\\n--- MaxPool2D Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 1, 2, 2)\n",
        "pooled_grad = np.array([[[[1, 2], [3, 4]]]])\n",
        "grad_to_pass = max_pool_layer.backward(pooled_grad)\n",
        "print(f\"Gradient passed to previous layer (dX):\\n{grad_to_pass[0, 0]}\")\n",
        "\n",
        "# Manually calculating the expected gradient\n",
        "expected_grad = np.array([[[[0, 0, 0, 0],\n",
        "                           [0, 1, 0, 2],\n",
        "                           [0, 0, 0, 0],\n",
        "                           [0, 3, 0, 4]]]])\n",
        "print(f\"\\nExpected dX:\\n{expected_grad[0, 0]}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(grad_to_pass, expected_grad)}\")\n",
        "\n",
        "\n",
        "# Problem 5: Testing AveragePool2D\n",
        "print(\"\\n\\n--- Problem 5: Testing AveragePool2D Layer ---\")\n",
        "\n",
        "# Creating a sample input for average pooling\n",
        "# Shape: (batch_size=1, channels=1, height=4, width=4)\n",
        "avg_pool_x = np.array([[[[1, 2, 3, 4],\n",
        "                        [5, 6, 7, 8],\n",
        "                        [9, 10, 11, 12],\n",
        "                        [13, 14, 15, 16]]]])\n",
        "\n",
        "# Defining pooling parameters\n",
        "avg_pool_size = (2, 2)\n",
        "avg_stride = (2, 2)\n",
        "\n",
        "# Creating an AveragePool2D instance\n",
        "avg_pool_layer = AveragePool2D(pool_size=avg_pool_size, stride=avg_stride)\n",
        "\n",
        "# Forward Pass for Average Pooling\n",
        "print(\"\\n--- AveragePool2D Forward Pass ---\")\n",
        "avg_pooled_output = avg_pool_layer.forward(avg_pool_x)\n",
        "print(f\"Input for pooling:\\n{avg_pool_x[0, 0]}\")\n",
        "print(f\"\\nAverage Pooled Output:\\n{avg_pooled_output[0, 0]}\")\n",
        "\n",
        "# Expected average pooled output\n",
        "expected_avg_output = np.array([[[3.5, 5.5], [11.5, 13.5]]])\n",
        "print(f\"\\nExpected Average Pooled Output:\\n{expected_avg_output[0, 0]}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(avg_pooled_output, expected_avg_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Average Pooling\n",
        "print(\"\\n--- AveragePool2D Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 1, 2, 2)\n",
        "avg_pooled_grad = np.array([[[[1, 2], [3, 4]]]])\n",
        "avg_grad_to_pass = avg_pool_layer.backward(avg_pooled_grad)\n",
        "print(f\"Gradient passed to previous layer (dX):\\n{avg_grad_to_pass[0, 0]}\")\n",
        "\n",
        "# Manually calculate the expected gradient\n",
        "expected_avg_grad = np.array([[[[0.25, 0.25, 0.5, 0.5],\n",
        "                              [0.25, 0.25, 0.5, 0.5],\n",
        "                              [0.75, 0.75, 1.0, 1.0],\n",
        "                              [0.75, 0.75, 1.0, 1.0]]]])\n",
        "print(f\"\\nExpected dX:\\n{expected_avg_grad[0, 0]}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(avg_grad_to_pass, expected_avg_grad)}\")\n",
        "\n",
        "\n",
        "# Problem 6: Testing Flatten Layer\n",
        "print(\"\\n\\n--- Problem 6: Testing Flatten Layer ---\")\n",
        "\n",
        "# Creating a sample input for flattening\n",
        "# Shape: (batch_size=1, channels=2, height=2, width=2)\n",
        "flatten_x = np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]])\n",
        "\n",
        "# Creating a Flatten instance\n",
        "flatten_layer = Flatten()\n",
        "\n",
        "# Forward Pass for Flattening\n",
        "print(\"\\n--- Flatten Forward Pass ---\")\n",
        "flattened_output = flatten_layer.forward(flatten_x)\n",
        "print(f\"Original shape: {flatten_x.shape}\")\n",
        "print(f\"Flattened output shape: {flattened_output.shape}\")\n",
        "print(f\"Flattened output:\\n{flattened_output}\")\n",
        "\n",
        "# Expected flattened output\n",
        "expected_flattened_output = np.array([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
        "print(f\"\\nExpected Flattened Output:\\n{expected_flattened_output}\")\n",
        "print(f\"\\nForward pass matches expected output: {np.allclose(flattened_output, expected_flattened_output)}\")\n",
        "\n",
        "\n",
        "# Backward Pass for Flattening\n",
        "print(\"\\n--- Flatten Backward Pass ---\")\n",
        "# Creating a sample gradient to pass back\n",
        "# Shape: (1, 8) - same as flattened output\n",
        "flattened_grad = np.array([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
        "reshaped_grad = flatten_layer.backward(flattened_grad)\n",
        "print(f\"Original gradient shape: {flattened_grad.shape}\")\n",
        "print(f\"Reshaped gradient shape: {reshaped_grad.shape}\")\n",
        "print(f\"Reshaped gradient to previous layer (dX):\\n{reshaped_grad}\")\n",
        "\n",
        "# The expected reshaped gradient is the same as the original input\n",
        "expected_reshaped_grad = flatten_x\n",
        "print(f\"\\nExpected Reshaped Gradient:\\n{expected_reshaped_grad}\")\n",
        "\n",
        "# Checking if the calculated dX matches the expected dX\n",
        "print(f\"\\nBackward pass matches expected dX: {np.allclose(reshaped_grad, expected_reshaped_grad)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GpGRPk2tvVd"
      },
      "source": [
        "**7. Learning and estimation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ6xoqsKt0ZQ",
        "outputId": "ac1dc00b-606f-4af5-b451-d32276827173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing MNIST data...\n",
            "Data loaded successfully.\n",
            "Starting training...\n",
            "Epoch 1/3, Average Loss: 2.3036, Validation Accuracy: 10.28%\n"
          ]
        }
      ],
      "source": [
        "!pip install python-mnist\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from mnist import MNIST\n",
        "\n",
        "# Layer Implementations (from previous problems)\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dX = dA.copy()\n",
        "        dX[self.X <= 0] = 0\n",
        "        return dX\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    A 2D maximum pooling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "        self.max_indices = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "        self.max_indices = np.zeros_like(X, dtype=bool)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "                        max_val = np.max(region)\n",
        "                        max_val_idx = np.argmax(region)\n",
        "                        A[n, c, i, j] = max_val\n",
        "\n",
        "                        h_idx = h_start + max_val_idx // self.pool_w\n",
        "                        w_idx = w_start + max_val_idx % self.pool_w\n",
        "                        self.max_indices[n, c, h_idx, w_idx] = True\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dX[self.max_indices] = dA.ravel()\n",
        "        return dX\n",
        "\n",
        "\n",
        "class Flatten:\n",
        "    \"\"\"\n",
        "    A flattening layer that reshapes a multi-dimensional input into a one-dimensional vector.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_shape = X.shape\n",
        "        flattened_output = X.reshape(X.shape[0], -1)\n",
        "        return flattened_output\n",
        "\n",
        "    def backward(self, dA):\n",
        "        reshaped_grad = dA.reshape(self.input_shape)\n",
        "        return reshaped_grad\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    \"\"\"\n",
        "    A fully-connected (dense) layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.W = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros(output_size)\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.b\n",
        "\n",
        "    def backward(self, dA):\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        self.db = np.sum(dA, axis=0)\n",
        "        dX = np.dot(dA, self.W.T)\n",
        "        return dX\n",
        "\n",
        "\n",
        "# Loss Function\n",
        "class CrossEntropyLoss:\n",
        "    \"\"\"\n",
        "    Cross-Entropy Loss function with Softmax.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.y_true = None\n",
        "        self.logits = None\n",
        "        self.softmax_output = None\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        self.y_true = y_true\n",
        "        self.logits = logits\n",
        "        # Softmax for numerical stability\n",
        "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        self.softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculating loss\n",
        "        loss = -np.sum(y_true * np.log(self.softmax_output + 1e-9)) / logits.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        return (self.softmax_output - self.y_true)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, layers):\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'W') and hasattr(layer, 'dW'):\n",
        "                layer.W -= self.learning_rate * layer.dW\n",
        "                layer.b -= self.learning_rate * layer.db\n",
        "\n",
        "\n",
        "# Model and Training Loop\n",
        "class LeNet:\n",
        "    \"\"\"\n",
        "    A modern implementation of the LeNet-5 architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.layers = [\n",
        "            Conv2d(in_channels=1, out_channels=6, filter_size=(5, 5)),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=(2, 2)),\n",
        "            Conv2d(in_channels=6, out_channels=16, filter_size=(5, 5)),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=(2, 2)),\n",
        "            Flatten(),\n",
        "            Dense(input_size=16 * 4 * 4, output_size=120),\n",
        "            ReLU(),\n",
        "            Dense(input_size=120, output_size=84),\n",
        "            ReLU(),\n",
        "            Dense(input_size=84, output_size=10)\n",
        "        ]\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dA):\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA)\n",
        "        return dA\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    A class to handle the training and evaluation loop.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, loss_function, epochs=3, batch_size=64):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.log_loss = []\n",
        "        self.log_acc = []\n",
        "\n",
        "    def fit(self, X_train, y_train_one_hot, X_val, y_val):\n",
        "        num_batches = len(X_train) // self.batch_size\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # Shuffling data for each epoch\n",
        "            indices = list(range(len(X_train)))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                # Getting a random batch\n",
        "                batch_indices = indices[i * self.batch_size:(i + 1) * self.batch_size]\n",
        "                X_batch = X_train[batch_indices]\n",
        "                y_batch_one_hot = y_train_one_hot[batch_indices]\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.model.forward(X_batch)\n",
        "                loss = self.loss_function.forward(output, y_batch_one_hot)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                dA = self.loss_function.backward()\n",
        "                self.model.backward(dA)\n",
        "\n",
        "                # Updating weights\n",
        "                self.optimizer.update(self.model.layers)\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            self.log_loss.append(avg_loss)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_preds = self.predict(X_val)\n",
        "            val_accuracy = np.mean(val_preds == y_val)\n",
        "            self.log_acc.append(val_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Average Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "        print(\"\\nTraining complete.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.model.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "\n",
        "# Data Preparation and Download\n",
        "def download_mnist_data():\n",
        "    \"\"\"\n",
        "    Downloads the MNIST dataset files if they don't exist.\n",
        "    \"\"\"\n",
        "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
        "    files = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "             \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            url = base_url + file_name\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f\"Downloaded {file_name}.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {file_name}: {e}\")\n",
        "                raise FileNotFoundError(f\"Could not download {file_name}\")\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads MNIST data, ensuring files are downloaded first.\n",
        "    \"\"\"\n",
        "    download_mnist_data()\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    files = [\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\",\n",
        "             \"t10k-images-idx3-ubyte\", \"t10k-labels-idx1-ubyte\"]\n",
        "\n",
        "    for file_name in files:\n",
        "        zipped_path = os.path.join(data_dir, file_name + \".gz\")\n",
        "        unzipped_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(unzipped_path):\n",
        "            print(f\"Unzipping {file_name}.gz...\")\n",
        "            with gzip.open(zipped_path, 'rb') as f_in:\n",
        "                with open(unzipped_path, 'wb') as f_out:\n",
        "                    f_out.write(f_in.read())\n",
        "            print(f\"Unzipped {file_name}.gz.\")\n",
        "\n",
        "    mndata = MNIST(data_dir)\n",
        "    images, labels = mndata.load_training()\n",
        "    test_images, test_labels = mndata.load_testing()\n",
        "\n",
        "    X_train = np.array(images).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_train = np.array(labels)\n",
        "    X_test = np.array(test_images).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    y_train_one_hot = np.zeros((y_train.size, 10))\n",
        "    y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "    return X_train, y_train_one_hot, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# Main Training Script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    print(\"Loading and preparing MNIST data...\")\n",
        "    X_train, y_train_one_hot, y_train, X_test, y_test = load_data()\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Instantiate model, loss, and optimizer\n",
        "    model = LeNet()\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "    # Instantiate and run the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        loss_function=criterion,\n",
        "        epochs=3,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    trainer.fit(X_train, y_train_one_hot, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlx9t-IJ2LT"
      },
      "source": [
        "**8. LeNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btavqiL8J9fr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from mnist import MNIST\n",
        "\n",
        "# Layer Implementations (from previous problems)\n",
        "\n",
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer implemented from scratch using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, filter_size):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.filter_h, self.filter_w = filter_size\n",
        "\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.filter_h, self.filter_w) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "        N_out_h = H - self.filter_h + 1\n",
        "        N_out_w = W - self.filter_w + 1\n",
        "        A = np.zeros((N, self.out_channels, N_out_h, N_out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(N_out_h):\n",
        "                    for j in range(N_out_w):\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "                        A[n, m, i, j] = np.sum(receptive_field * self.W[m]) + self.b[m]\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        N, C_in, H_in, W_in = self.X.shape\n",
        "        N_out, M_out, H_out, W_out = dA.shape\n",
        "\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.db = np.zeros(self.b.shape)\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        for n in range(N):\n",
        "            for m in range(M_out):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        receptive_field = self.X[n, :, i:i + self.filter_h, j:j + self.filter_w]\n",
        "                        self.dW[m] += dA[n, m, i, j] * receptive_field\n",
        "                self.db[m] = np.sum(dA[n, m])\n",
        "\n",
        "        for n in range(N):\n",
        "            for k in range(C_in):\n",
        "                for i in range(H_in):\n",
        "                    for j in range(W_in):\n",
        "                        sum_val = 0\n",
        "                        for m in range(M_out):\n",
        "                            for s in range(self.filter_h):\n",
        "                                for t in range(self.filter_w):\n",
        "                                    if 0 <= (i - s) < H_out and 0 <= (j - t) < W_out:\n",
        "                                        sum_val += dA[n, m, i - s, j - t] * self.W[m, k, s, t]\n",
        "                        dX[n, k, i, j] = sum_val\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dX = dA.copy()\n",
        "        dX[self.X <= 0] = 0\n",
        "        return dX\n",
        "\n",
        "\n",
        "class MaxPool2D:\n",
        "    \"\"\"\n",
        "    A 2D maximum pooling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_h, self.pool_w = pool_size\n",
        "        self.stride_h, self.stride_w = stride\n",
        "        self.max_indices = None\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "        output_h = int((H - self.pool_h) / self.stride_h) + 1\n",
        "        output_w = int((W - self.pool_w) / self.stride_w) + 1\n",
        "        A = np.zeros((N, C, output_h, output_w))\n",
        "        self.max_indices = np.zeros_like(X, dtype=bool)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(output_h):\n",
        "                    for j in range(output_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        h_end = h_start + self.pool_h\n",
        "                        w_end = w_start + self.pool_w\n",
        "\n",
        "                        region = self.X[n, c, h_start:h_end, w_start:w_end]\n",
        "                        max_val = np.max(region)\n",
        "                        max_val_idx = np.argmax(region)\n",
        "                        A[n, c, i, j] = max_val\n",
        "\n",
        "                        h_idx = h_start + max_val_idx // self.pool_w\n",
        "                        w_idx = w_start + max_val_idx % self.pool_w\n",
        "                        self.max_indices[n, c, h_idx, w_idx] = True\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dX[self.max_indices] = dA.ravel()\n",
        "        return dX\n",
        "\n",
        "\n",
        "class Flatten:\n",
        "    \"\"\"\n",
        "    A flattening layer that reshapes a multi-dimensional input into a one-dimensional vector.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_shape = X.shape\n",
        "        flattened_output = X.reshape(X.shape[0], -1)\n",
        "        return flattened_output\n",
        "\n",
        "    def backward(self, dA):\n",
        "        reshaped_grad = dA.reshape(self.input_shape)\n",
        "        return reshaped_grad\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    \"\"\"\n",
        "    A fully-connected (dense) layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.W = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.b = np.zeros(output_size)\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.b\n",
        "\n",
        "    def backward(self, dA):\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        self.db = np.sum(dA, axis=0)\n",
        "        dX = np.dot(dA, self.W.T)\n",
        "        return dX\n",
        "\n",
        "\n",
        "# Loss Function\n",
        "class CrossEntropyLoss:\n",
        "    \"\"\"\n",
        "    Cross-Entropy Loss function with Softmax.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.y_true = None\n",
        "        self.logits = None\n",
        "        self.softmax_output = None\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        self.y_true = y_true\n",
        "        self.logits = logits\n",
        "        # Softmax for numerical stability\n",
        "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        self.softmax_output = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculating loss\n",
        "        loss = -np.sum(y_true * np.log(self.softmax_output + 1e-9)) / logits.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # The backward pass for Softmax and Cross-Entropy combined is\n",
        "        # the softmax output minus the one-hot encoded true labels.\n",
        "        return (self.softmax_output - self.y_true)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, layers):\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'W') and hasattr(layer, 'dW'):\n",
        "                layer.W -= self.learning_rate * layer.dW\n",
        "                layer.b -= self.learning_rate * layer.db\n",
        "\n",
        "\n",
        "# Model and Training Loop\n",
        "class LeNet:\n",
        "    \"\"\"\n",
        "    A modern implementation of the LeNet-5 architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.layers = [\n",
        "            Conv2d(in_channels=1, out_channels=6, filter_size=(5, 5)),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=(2, 2)),\n",
        "            Conv2d(in_channels=6, out_channels=16, filter_size=(5, 5)),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=(2, 2), stride=(2, 2)),\n",
        "            Flatten(),\n",
        "            Dense(input_size=16 * 4 * 4, output_size=120),\n",
        "            ReLU(),\n",
        "            Dense(input_size=120, output_size=84),\n",
        "            ReLU(),\n",
        "            Dense(input_size=84, output_size=10)\n",
        "        ]\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dA):\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA)\n",
        "        return dA\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    A class to handle the training and evaluation loop.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, loss_function, epochs=3, batch_size=64):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.log_loss = []\n",
        "        self.log_acc = []\n",
        "\n",
        "    def fit(self, X_train, y_train_one_hot, X_val, y_val):\n",
        "        num_batches = len(X_train) // self.batch_size\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # Shuffle data for each epoch\n",
        "            indices = list(range(len(X_train)))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                # Get a random batch\n",
        "                batch_indices = indices[i * self.batch_size:(i + 1) * self.batch_size]\n",
        "                X_batch = X_train[batch_indices]\n",
        "                y_batch_one_hot = y_train_one_hot[batch_indices]\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.model.forward(X_batch)\n",
        "                loss = self.loss_function.forward(output, y_batch_one_hot)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                dA = self.loss_function.backward()\n",
        "                self.model.backward(dA)\n",
        "\n",
        "                # Update weights\n",
        "                self.optimizer.update(self.model.layers)\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            self.log_loss.append(avg_loss)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_preds = self.predict(X_val)\n",
        "            val_accuracy = np.mean(val_preds == y_val)\n",
        "            self.log_acc.append(val_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Average Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "        print(\"\\nTraining complete.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.model.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "\n",
        "# Data Preparation and Download\n",
        "def download_mnist_data():\n",
        "    \"\"\"\n",
        "    Downloads the MNIST dataset files if they don't exist.\n",
        "    \"\"\"\n",
        "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
        "    files = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "             \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            url = base_url + file_name\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f\"Downloaded {file_name}.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {file_name}: {e}\")\n",
        "                raise FileNotFoundError(f\"Could not download {file_name}\")\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads MNIST data, ensuring files are downloaded first.\n",
        "    \"\"\"\n",
        "    download_mnist_data()\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    files = [\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\",\n",
        "             \"t10k-images-idx3-ubyte\", \"t10k-labels-idx1-ubyte\"]\n",
        "\n",
        "    for file_name in files:\n",
        "        zipped_path = os.path.join(data_dir, file_name + \".gz\")\n",
        "        unzipped_path = os.path.join(data_dir, file_name)\n",
        "        if not os.path.exists(unzipped_path):\n",
        "            print(f\"Unzipping {file_name}.gz...\")\n",
        "            with gzip.open(zipped_path, 'rb') as f_in:\n",
        "                with open(unzipped_path, 'wb') as f_out:\n",
        "                    f_out.write(f_in.read())\n",
        "            print(f\"Unzipped {file_name}.gz.\")\n",
        "\n",
        "    mndata = MNIST(data_dir)\n",
        "    images, labels = mndata.load_training()\n",
        "    test_images, test_labels = mndata.load_testing()\n",
        "\n",
        "    X_train = np.array(images).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_train = np.array(labels)\n",
        "    X_test = np.array(test_images).reshape(-1, 1, 28, 28) / 255.0\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    y_train_one_hot = np.zeros((y_train.size, 10))\n",
        "    y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "    return X_train, y_train_one_hot, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# Main Training Script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    print(\"Loading and preparing MNIST data...\")\n",
        "    X_train, y_train_one_hot, y_train, X_test, y_test = load_data()\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Instantiate model, loss, and optimizer\n",
        "    model = LeNet()\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "    # Instantiate and run the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        loss_function=criterion,\n",
        "        epochs=3,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    trainer.fit(X_train, y_train_one_hot, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWBlnxodNS3G"
      },
      "source": [
        "**9. Survey of famous image recognition models**\n",
        "\n",
        "AlexNet (2012)\n",
        "\n",
        "AlexNet was a groundbreaking model that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Its victory demonstrated that deep convolutional neural networks (CNNs) were a powerful tool for large-scale image recognition. It was a significant jump from the previous best models and ignited the deep learning revolution in computer vision.\n",
        "\n",
        "Here are some of its key features:\n",
        "\n",
        "Deeper Architecture: It was much deeper than LeNet, with five convolutional layers followed by three fully connected layers.\n",
        "\n",
        "ReLU Activation: It used the Rectified Linear Unit (ReLU) activation function, which was found to train much faster than the sigmoid or tanh functions used in earlier networks.\n",
        "\n",
        "Dropout: To prevent overfitting, the network used dropout layers, which randomly \"turned off\" neurons during training.\n",
        "\n",
        "GPU Utilization: The network was so large that it was split across two GPUs during training, highlighting the need for parallel processing in deep learning.\n",
        "\n",
        "VGG16 (2014)\n",
        "\n",
        "Developed by the Visual Geometry Group (VGG) at the University of Oxford, VGG16 was the runner-up in the ILSVRC in 2014. While it didn't win the competition, its simple and uniform architecture made it incredibly popular. The 16 in its name refers to the number of weight layers.\n",
        "\n",
        "Key features:\n",
        "\n",
        "Simplicity and Uniformity: Instead of large, varying filter sizes, VGG16's key innovation was using a stack of small 3x3 convolutional filters throughout the entire network. The idea was that stacking multiple small filters could achieve the same receptive field as a single larger filter, but with more non-linearity and fewer parameters.\n",
        "\n",
        "Deeper and More Layers: VGG16 pushed the depth of CNNs even further, with 13 convolutional layers and 3 fully connected layers.\n",
        "\n",
        "Transfer Learning: Due to its straightforward architecture and excellent performance, VGG16 quickly became a standard for transfer learning, where a pre-trained model is used as a feature extractor for other image-related tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJir5TsxOkwC"
      },
      "source": [
        "**10. Calculation of output size and number of parameters**\n",
        "\n",
        "1. Calculation for Layer 1\n",
        "\n",
        "Input Size: 144 x 144, 3 channels\n",
        "\n",
        "Filter Size: 3 x 3, 6 channels\n",
        "\n",
        "Stride: 1\n",
        "\n",
        "Padding: None\n",
        "\n",
        "Output Size Calculation:\n",
        "The formula for the output size without padding is:\n",
        "\n",
        "Output=(\n",
        "Stride\n",
        "Input−Filter\n",
        "​\n",
        " )+1\n",
        "Height: (\n",
        "1\n",
        "144−3\n",
        "​\n",
        " )+1=141+1=142\n",
        "\n",
        "Width: (\n",
        "1\n",
        "144−3\n",
        "​\n",
        " )+1=141+1=142\n",
        "\n",
        "Channels: The number of output channels is equal to the number of filters, which is 6.\n",
        "\n",
        "Therefore, the output size is 142 x 142, 6 channels.\n",
        "\n",
        "Number of Parameters Calculation:\n",
        "The formula for the number of parameters (including bias) is:\n",
        "\n",
        "Parameters=(Filter width×Filter height×Input channels+1)×Output channels\n",
        "(3×3×3+1)×6=(27+1)×6=28×6=168\n",
        "\n",
        "Therefore, the number of parameters is 168.\n",
        "\n",
        "---\n",
        "\n",
        "2. Calculation for Layer 2\n",
        "\n",
        "Input Size: 60 x 60, 24 channels\n",
        "\n",
        "Filter Size: 3 x 3, 48 channels\n",
        "\n",
        "Stride: 1\n",
        "\n",
        "Padding: None\n",
        "\n",
        "Output Size Calculation:\n",
        "\n",
        "Height: (\n",
        "1\n",
        "60−3\n",
        "​\n",
        " )+1=57+1=58\n",
        "\n",
        "Width: (\n",
        "1\n",
        "60−3\n",
        "​\n",
        " )+1=57+1=58\n",
        "\n",
        "Channels: The number of output channels is 48.\n",
        "\n",
        "Therefore, the output size is 58 x 58, 48 channels.\n",
        "\n",
        "Number of Parameters Calculation:\n",
        "\n",
        "(3×3×24+1)×48=(216+1)×48=217×48=10,416\n",
        "\n",
        "Therefore, the number of parameters is 10,416.\n",
        "\n",
        "---\n",
        "\n",
        "3. Calculation for Layer 3\n",
        "\n",
        "Input Size: 20 x 20, 10 channels\n",
        "\n",
        "Filter Size: 3 x 3, 20 channels\n",
        "\n",
        "Stride: 2\n",
        "\n",
        "Padding: None\n",
        "\n",
        "Output Size Calculation:\n",
        "\n",
        "Height: (\n",
        "2\n",
        "20−3\n",
        "​\n",
        " )+1=\n",
        "2\n",
        "17\n",
        "​\n",
        " +1=8.5+1. In this case, the result is not an integer. The framework will often round down, effectively ignoring the last row or column of pixels that don't fit the filter and stride.\n",
        "\n",
        "New Height: (⌊\n",
        "2\n",
        "20−3\n",
        "​\n",
        " ⌋)+1=⌊8.5⌋+1=8+1=9\n",
        "\n",
        "New Width: (⌊\n",
        "2\n",
        "20−3\n",
        "​\n",
        " ⌋)+1=⌊8.5⌋+1=8+1=9\n",
        "\n",
        "Channels: The number of output channels is 20.\n",
        "\n",
        "Therefore, the output size is 9 x 9, 20 channels. This is an example of why it's not ideal to use these settings. The model will not be able to process the entire input image.\n",
        "\n",
        "Number of Parameters Calculation:\n",
        "\n",
        "(3×3×10+1)×20=(90+1)×20=91×20=1,820\n",
        "\n",
        "Therefore, the number of parameters is 1,820."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osfT1ZHGRpq8"
      },
      "source": [
        "**11. Survey on filter size**\n",
        "\n",
        "Why 3x3 Filters are Commonly Used\n",
        "\n",
        "The use of multiple small filters, like 3x3, instead of a single large filter, like 7x7, has become a standard practice in modern CNN architectures. This is primarily for two reasons:\n",
        "\n",
        "Deeper Networks and More Non-linearity: A single 7x7 convolutional layer can be replaced by three consecutive 3x3 layers. Each 3x3 layer includes a non-linear activation function (like ReLU). By stacking these layers, the network can learn more complex, abstract features. The three 3x3 layers together have a receptive field size equivalent to one 7x7 layer, but they introduce more non-linearity, which significantly improves the model's ability to learn and classify complex patterns.\n",
        "\n",
        "Reduced Parameters and Computation: This is the most significant advantage. A single 7x7 filter has 49 parameters. In contrast, three 3x3 filters have 3×3×3=27 parameters. While this is a simplified example, the parameter count scales with the number of input and output channels. Using multiple smaller filters drastically reduces the total number of parameters, which decreases memory requirements and speeds up both training and inference.\n",
        "\n",
        "---\n",
        "\n",
        "The Role of a 1x1 Filter\n",
        "\n",
        "A 1x1 filter is also known as a \"pointwise convolution\" because it doesn't operate spatially like a typical filter. Its main purpose is to manipulate the channel dimension of the data.\n",
        "\n",
        "When a 1x1 filter is applied, it performs a weighted sum across all the input channels for each individual pixel location. This allows it to:\n",
        "\n",
        "Reduce Channel Dimensionality: By using fewer 1x1 filters than the number of input channels, you can compress the feature map. For instance, if you have 256 channels, you could use a 1x1 convolution with 64 filters to reduce the channels to 64, thereby significantly cutting down the number of parameters for subsequent layers.\n",
        "\n",
        "Increase Channel Dimensionality: Conversely, you can also use 1x1 filters to expand the number of channels, projecting the feature map into a higher-dimensional space.\n",
        "\n",
        "Introduce Non-linearity: Just like other convolutional layers, a 1x1 filter is typically followed by a non-linear activation function, adding more expressive power to the network.\n",
        "\n",
        "---\n",
        "Data Flow Through the LeNet-5 Model\n",
        "\n",
        "Based on the LeNet-5 code in the document, here's a quick look at how the data flows through the convolutional and pooling layers:\n",
        "\n",
        "Input: The initial image tensor has a shape of (Batch Size, 1, 28, 28).\n",
        "\n",
        "Conv2d: The first convolutional layer applies 6 filters of size 5x5, which reduces the spatial dimensions. The output tensor is now (Batch Size, 6, 24, 24).\n",
        "\n",
        "MaxPool2D: The first max-pooling layer with a 2x2 filter and stride of 2 halves the height and width. The tensor becomes (Batch Size, 6, 12, 12).\n",
        "\n",
        "Conv2d: The second convolutional layer with 16 filters of size 5x5 again reduces the spatial dimensions. The tensor is now (Batch Size, 16, 8, 8).\n",
        "\n",
        "MaxPool2D: The second max-pooling layer halves the height and width once more. The tensor becomes (Batch Size, 16, 4, 4).\n",
        "\n",
        "Flatten: This layer converts the multi-dimensional tensor into a single vector. The dimensions are multiplied together, resulting in a shape of (Batch Size, 256), which is then fed into the fully-connected layers.\n",
        "\n",
        "Each step systematically transforms the input data, extracting increasingly abstract features until it's ready for the final classification layers.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ64tN/ZJjMzCDRTWeHi0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}